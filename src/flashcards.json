[
  {
    "attempts": [],
    "back": "Taylor series is the infinite series:<br><br>\n$$\nf(x) = \\sum_{n=0}^{\\inf}{\\frac{f^{(n)}(a)}{n!}(x - a)^{n}}\n$$\n<br><br>\nTaylor series \"expansion\" of a function typically implies truncation, meaning we approximate the function $f(x)$ by a finite number of terms. <br><br>\n$$\nf(x) \\approx f(a) + f^{\\prime}(a)(x-a) + \\dots + \\frac{f^{(N)}(a)}{N!} (x-a)^N.\n$$\n<br><br>",
    "front": "Taylor series of $f(x)$ at a point $x = a$ <br><br>\nvs. <br><br>\nTaylor series expansion of $f(x)$ at / around a point $x = a$<br>",
    "importance": 2,
    "tags": [
      "18.065",
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<b><u>Definition</u></b> <br>\nThe Euler method is a first-order explicit numerical method that uses the current point to estimate the next point in a step wise fashion.\n<br><br>\n\n<b><u>Practical Purpose</u></b> <br>\nIt is used to approximate solutions to ODEs.\n",
    "front": "Euler method <br><br>\nDefine <br><br>\nPractical purpose",
    "id": 1741038606887,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/trapezoid_rule.webp\"></div>",
    "front": "Trapezoidal Rule <br><br>\n\nPractical use <br><br>\n\nDefinition",
    "id": 1741038908417,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Approach to discretely approximating integrals.\n\n<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/midpoint_rule.jpg\"></div>",
    "front": "Midpoint Rule <br><br>\n\nPractical use <br><br>\n\nDefinition",
    "id": 1741041716871,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "A set of vectors that perfectly describes the space. <br><br>\n\nTheir combinations give one and only one way to produce every vector in the space.",
    "front": "Basis",
    "id": 1740885744839,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/four_fundamental_subspaces.jpeg\"></div>\n<br>\nDimensions: <br>\n$Col(A) \\in \\mathbb{R}^{r}$ <br>\n$Null(A) \\in \\mathbb{R}^{n - r}$ <br>\n$Col(A^T) \\in \\mathbb{R}^{r}$ <br>\n$Null(A) \\in \\mathbb{R}^{m - r}$ <br>",
    "front": "Four fundamental subspaces.",
    "id": 1740884070112,
    "tags": [
      "18.065"
    ],
    "understanding": 2
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Card for mom",
    "id": 1740894978704,
    "tags": [
      "14.13"
    ],
    "understanding": 3
  },
  {
    "attempts": [],
    "back": "- denoted $Col(A) = C(A)$ <br>\n- the column space is a subspace of $\\mathbb{R}^m$ ($A \\in \\mathbb{R}^{n \\times m}$)<br>\n- consists of all linear combinations of $A$'s columns <br>\n- this is captured by the space spanned by $b$ in $Ax = b, \\, \\forall x$",
    "front": "Column space of $A$",
    "id": 1740886684323,
    "tags": [
      "18.065"
    ],
    "understanding": 3
  },
  {
    "attempts": [],
    "back": "- denoted $Col(A^T) = C(A)$, <br>\n- the column space is a subspace of $\\mathbb{R}^n$ (recall $A \\in \\mathbb{R}^{n \\times m}$)<br>\n- consists of all linear combinations of the columns of $A^T$'s <br>\n- this is captured by the space spanned by $b$ in $A^Ty = b, \\, \\forall y$",
    "front": "Row space of $A$ <br><br>",
    "id": 1740887337718,
    "tags": [
      "18.065",
      "hello"
    ],
    "understanding": 2
  },
  {
    "attempts": [],
    "back": "- contains all the solutions $x$ to $Ax = 0$, including $x = 0$.",
    "front": "Null space of $A$",
    "id": 1740887782700,
    "tags": [
      "18.065"
    ],
    "understanding": 2
  },
  {
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741074654248
      }
    ],
    "back": "<u><b>Definition</b></u> <br>\n<p> \nFor a matrix $A \\in \\mathbb{R}^{m \\times n}$, a vector $x \\in \\mathbb{R}^{n} \\setminus \\{0\\}$, and a scalar $\\lambda \\in \\mathbb{R}$, the eigenvalues $\\lambda$ and eigenvectors $x$ satisfy:  <br>\n$$\nAx = \\lambda x\n$$\n</p> \n<br>\n\n<u><b>Procedure</b></u> <br>\n<ol>\n    <li>Rearrange the equation to:\n        $$\n        (A - \\lambda I)x = 0\n        $$\n    </li>\n    <li>For a nonzero $x$, the system has a nontrivial solution only if:\n        $$\n        \\det(A - \\lambda I) = 0\n        $$\n    </li>\n    <li>Solve the characteristic equation $\\det(A - \\lambda I) = 0$ for $\\lambda$ to find the eigenvalues.</li>\n    <li>For each $\\lambda$, substitute into:\n        $$\n        (A - \\lambda I)x = 0\n        $$\n        and solve the resulting system for the eigenvector $x$.</li>\n</ol>",
    "front": "<b>Eigenvalues and Eigenvectors</b> \n<ul>\n  <li><b>Define:</b> What are eigenvalues and eigenvectors?</li>\n  <li><b>Procedure:</b> How do we compute eigenvalues and eigenvectors?</li>\n</ul>",
    "id": 1740890671935,
    "tags": [
      "18.065",
      "16.32"
    ],
    "understanding": 2
  },
  {
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741075650753
      }
    ],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nFor a square matrix $A$, the <b>characteristic polynomial</b> is defined as:\n$$\np(\\lambda) = \\det(A - \\lambda I)\n$$\n</p>\n<br>\n<u><b>Derivation</b></u> <br>\n<ol>\n  <li>Start with the eigenvalue equation:\n    $$\n    Ax = \\lambda x\n    $$\n    for a nonzero vector $x$.\n  </li>\n  <li>Rearrange the equation to:\n    $$\n    (A - \\lambda I)x = 0\n    $$\n  </li>\n  <li>Since $x \\neq 0$, a nontrivial solution exists only if the matrix $(A - \\lambda I)$ is singular, hence:\n    $$\n    \\det(A - \\lambda I) = 0\n    $$\n  </li>\n  <li>This determinant equation is the <b>characteristic polynomial</b> of $A$.\n  </li>\n</ol>",
    "front": "<b>Characteristic Polynomial</b> \n<ul>\n  <li><b>Define:</b> What is the characteristic polynomial?</li>\n  <li><b>Derivation:</b> How is it derived from the eigenvalue equation $Ax = \\lambda x$?</li>\n</ul>",
    "id": 1741075245856,
    "tags": [
      "18.065"
    ],
    "understanding": 2
  },
  {
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nGauss elimination is a systematic method for solving systems of linear equations. It involves applying elementary row operations to an augmented matrix to transform it into row echelon form (or reduced row echelon form), from which the solutions can be obtained through back substitution.\n</p>\n<br>\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Write the augmented matrix for the system of equations.</li>\n  <li>Apply elementary row operations:\n    <ul>\n      <li>Swap rows.</li>\n      <li>Multiply a row by a nonzero scalar.</li>\n      <li>Add or subtract a multiple of one row from another.</li>\n    </ul>\n  </li>\n  <li>Reduce the matrix to row echelon form (upper triangular form).</li>\n  <li>(Optional) Further reduce to reduced row echelon form for a unique solution.</li>\n  <li>Perform back substitution to solve for the variables.</li>\n</ol>",
    "front": "<b>Gauss Elimination</b> \n<ul>\n  <li><b>Define:</b> What is Gauss elimination?</li>\n  <li><b>Procedure:</b> How is Gauss elimination performed?</li>\n</ul>",
    "id": 1740890812452,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<ul>\n  <li>Denoted $rref(A)$</li>\n  <li>Rules:\n    <ol>\n      <li>Each leading 1 (pivot) is the only nonzero entry in its column.</li>\n      <li>The leading 1 in each row appears to the right of the leading 1 in the row above.</li>\n      <li>Any rows of all zeros appear in the bottom rows of the matrix.</li>\n    </ol>\n  </li>\n  <li>The first $r$ pivot columns form an identity-like structure.</li>\n  <li>The remaining $n - r$ columns, denoted as $F$, contain the free variables.</li>\n</ul>",
    "front": "Reduced row echelon form",
    "id": 1740887871355,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "To solve $Ax = b$ is to <u>express b as a linear combination of the columns of $A$.</u>",
    "front": "To solve $Ax = b$ is to ____.",
    "id": 1740886790619,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "The equations $Ax = b$ are solvable iff <u>${b}$ is in the column space of $A$</u>",
    "front": "The equations $Ax = b$ are solvable iff ____.",
    "id": 1740887017912,
    "tags": [
      "18.065"
    ],
    "understanding": 2
  },
  {
    "attempts": [],
    "back": "Inner product / dot product as implied by the multiplication of a transposed column vector $1 \\times n$ and a column vector $n \\times 1$.<br><br>\n\nThe dot product is given by: <br><br>\n$$\n\\mathbf{a}^\\top \\mathbf{b} =\n\\begin{bmatrix} a_1 & a_2 & \\dots & a_n \\end{bmatrix}\n\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{bmatrix} =\n\\sum_{i=1}^{n} a_i b_i\n$$",
    "front": "Let  $a$  and  $b$  be column vectors in $\\mathbb{R}^n$. <br><br>\n\nWhat is $a^Tb$ ?\n",
    "id": 1740895695348,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nA square matrix $A$ is <b>invertible</b> (or nonsingular) if there exists a matrix $B$ such that:\n$$\nAB = BA = I\n$$\nwhere $I$ is the identity matrix.\n</p>\n<br>\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Ensure that $A$ is square.</li>\n  <li>Check if the determinant $\\det(A)$ is nonzero. If $\\det(A) \\neq 0$, then $A$ is invertible.</li>\n  <li>If invertible, compute the inverse $A^{-1}$ using methods like Gaussian elimination or the adjugate formula.</li>\n</ol>",
    "front": "<b>Invertible Matrix</b> \n<ul>\n  <li><b>Define:</b> What is an invertible matrix?</li>\n  <li><b>Practical Use:</b> Why is invertibility important?</li>\n  <li><b>Procedure:</b> How do we determine if a matrix is invertible?</li>\n</ul>",
    "id": 1740887674549,
    "tags": [
      "18.065"
    ],
    "understanding": 2
  },
  {
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nA square matrix $A$ is called <b>singular</b> if it is not invertible. This is equivalent to:\n$$\n\\det(A) = 0\n$$\n</p>\n<br>\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Ensure that $A$ is square.</li>\n  <li>Calculate the determinant $\\det(A)$. If $\\det(A) = 0$, then $A$ is singular.</li>\n  <li>Interpretation: A singular matrix has linearly dependent columns (or rows) and does not possess an inverse.</li>\n</ol>",
    "front": "<b>Singular Matrix</b> \n<ul>\n  <li><b>Define:</b> What is a singular matrix?</li>\n  <li><b>Practical Use:</b> What does it imply when a matrix is singular?</li>\n  <li><b>Procedure:</b> How do we determine if a matrix is singular?</li>\n</ul>",
    "id": 1740887684118,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nA square matrix $A$ is <b>nonsingular</b> if it is invertible, meaning there exists a matrix $B$ such that:\n$$\nAB = BA = I\n$$\nEquivalently, $A$ is nonsingular if:\n$$\n\\det(A) \\neq 0\n$$\n</p>\n<br>\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Confirm that $A$ is a square matrix.</li>\n  <li>Compute $\\det(A)$. If $\\det(A) \\neq 0$, then $A$ is nonsingular.</li>\n  <li>If nonsingular, an inverse $A^{-1}$ exists and can be computed.</li>\n</ol>",
    "front": "<b>Nonsingular Matrix</b> \n<ul>\n  <li><b>Define:</b> What is a nonsingular matrix?</li>\n  <li><b>Practical Use:</b> Why is nonsingularity important?</li>\n  <li><b>Procedure:</b> How do we verify if a matrix is nonsingular?</li>\n</ul>",
    "id": 1741074715967,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nA square matrix $Q$ is called <b>orthogonal</b> if its columns (and rows) form an orthonormal set. This is equivalent to:\n$$\nQ^T Q = QQ^T = I\n$$\nwhere $I$ is the identity matrix.\n</p>\n<br>\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Check that $Q$ is a square matrix.</li>\n  <li>Compute $Q^T Q$. If $Q^T Q = I$, then $Q$ is orthogonal.</li>\n  <li>Alternatively, verify that the columns of $Q$ are orthonormal (i.e., each column has unit length and is orthogonal to the others).</li>\n</ol>",
    "front": "<b>Orthogonal Matrix</b> \n<ul>\n  <li><b>Define:</b> What is an orthogonal matrix?</li>\n  <li><b>Practical Use:</b> Where are orthogonal matrices used?</li>\n  <li><b>Procedure:</b> How do we check if a matrix is orthogonal?</li>\n</ul>",
    "id": 1740887687921,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nA <b>Householder matrix</b> $H$ is a symmetric and orthogonal matrix used to perform reflections. It is typically defined as:\n$$\nH = I - 2\\frac{vv^T}{v^T v}\n$$\nwhere $v$ is a nonzero vector in $\\mathbb{R}^n$.\n</p>\n<br>\n\n<u><b>Practical Use</b></u> <br>\n<p>\nHouseholder matrices are primarily used in numerical linear algebra, especially for QR factorization and for reducing matrices to tridiagonal or Hessenberg forms. TODO: add pset + lecture 02/11 02/13 stuff stuff\n</p>\n<br>\n\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Select a nonzero vector $v \\in \\mathbb{R}^n$.</li>\n  <li>Compute the Householder matrix using:\n      $$\n      H = I - 2\\frac{vv^T}{v^T v}\n      $$\n  </li>\n  <li>Use $H$ to reflect a given vector or to zero out subdiagonal elements during matrix factorization.</li>\n</ol>",
    "front": "<b>Householder Matrix</b> \n<ul>\n  <li><b>Define:</b> What is a Householder matrix?</li>\n  <li><b>Practical Use:</b> What are Householder matrices used for?</li>\n  <li><b>Procedure:</b> How is a Householder matrix constructed?</li>\n</ul>",
    "id": 1741075039903,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/five_factorizations.jpeg\"></div>",
    "front": "The five factorizations of a matrix.",
    "id": 1740884244261,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Procedure for QR factorization",
    "id": 1740890854848,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Definition: factorization of $A$ into upper $U$ and lower $L$ triangular matrices.\n\n<br><br>\nExample a  $3 \\times 3$  matrix: <br><br>\n$$\nA =\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n$$\n\n<br><br>\nLU decomposition gives: <br><br>\n$$\nL =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\nl_{21} & 1 & 0 \\\\\nl_{31} & l_{32} & 1\n\\end{bmatrix},\n\\quad\nU =\n\\begin{bmatrix}\nu_{11} & u_{12} & u_{13} \\\\\n0 & u_{22} & u_{23} \\\\\n0 & 0 & u_{33}\n\\end{bmatrix}\n$$\n\n<br><br>\nApplication: solving linear systems.\n<br><br>\n\nProcedure:\n<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/Screenshot_2025-03-02_at_1.34.36_AM.png\"></div>",
    "front": "$LU$ decomposition <br><br>\nDefine <br><br>\nApplications <br><br>\nProcedure",
    "id": 1740890882881,
    "tags": [
      "18.065"
    ],
    "understanding": 3
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Procedure for singular value decomposition",
    "id": 1740890913235,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Prove that for any matrix $A \\in R^{m\u00d7n}$, that $A^{T}A$ and $A$ have\nthe same null space.",
    "id": 1740890964079,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Symmetric matrix",
    "id": 1740890063445,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Propose a function $x = linear\\_solver(U, b)$, where $U \\in R^{n \\times n}$ is an upper triangular matrix, $b \\in R^n$ is a vector, and $x \\in R^n$ is the solution to $Ux = b$. <br><br>\n\nThen apply your procedure to",
    "id": 1740891657888,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nThe <b>Power Method</b> (or <b>Power Iteration</b>) is an iterative numerical algorithm used to approximate the dominant eigenvalue and its corresponding eigenvector of a matrix $A$. It works best when:\n$$\n|\\lambda_1| > |\\lambda_2| > \\dots > |\\lambda_n|\n$$\nand the eigenvectors form a basis for $\\mathbb{R}^n$.\n</p>\n<br>\n\n<u><b>Derivation</b></u> <br>\n<ul>\n  <li>\n    Any nonzero vector $v$ can be expressed as a linear combination of the eigenvectors of $A$: <br>\n    $$\n    v = c_1 v_1 + c_2 v_2 + \\dots + c_n v_n.\n    $$\n  </li>\n  <li>\n    Multiplying by $A$ gives: <br>\n    $$\n    A v = c_1 \\lambda_1 v_1 + c_2 \\lambda_2 v_2 + \\dots + c_n \\lambda_n v_n.\n    $$\n  </li>\n  <li>\n    Iterating $k$ times, we have: <br>\n    $$\n    A^k v = c_1 \\lambda_1^k v_1 + c_2 \\lambda_2^k v_2 + \\dots + c_n \\lambda_n^k v_n.\n    $$\n  </li>\n  <li>\n    If $c_1 \\neq 0$, as $k \\to \\infty$, the term $c_1 \\lambda_1^k v_1$ dominates because $|\\lambda_1|$ is strictly larger than the other eigenvalues.\n  </li>\n  <li>\n    Thus, the direction of $A^k v$ converges to $v_1$, the dominant eigenvector.\n  </li>\n</ul>\n<br>\n\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Choose an initial random vector $x_0$ (ensuring $x_0 \\neq 0$).</li>\n  <li>For $k = 0, 1, 2, \\dots$, compute:\n    $$\n    x_{k+1} = \\frac{A x_k}{\\|A x_k\\|_2}.\n    $$\n  </li>\n  <li>Continue iterating until the sequence $\\{x_k\\}$ converges. Then, approximate the dominant eigenvector and eigenvalue by setting:\n    $$\n    v_1 \\approx x_k \\quad \\text{and} \\quad \\lambda_1 \\approx \\frac{x_k^T A x_k}{x_k^T x_k}.\n    $$\n  </li>\n  <li>\n    <b>Deflation via Householder Transformation:</b> <br>\n    Construct a Householder matrix $H$ that transforms the dominant eigenvector $v_1$ to align with the first coordinate axis:\n    $$\n    H v_1 = \\pm \\|v_1\\|\\, e_1,\n    $$\n    where $e_1 = [1,0,\\dots,0]^T$.<br>\n    Then, form the similar matrix\n    $$\n    A' = H A H^T.\n    $$\n    In $A'$, the first row and column correspond to $\\lambda_1$, while the lower-right $(n-1)\\times(n-1)$ block (call it $B$) contains the remaining eigenvalues of $A$.\n  </li>\n  <li>\n    Apply the power method (or another suitable algorithm) to the deflated matrix $B$ to approximate the next dominant eigenpair, which corresponds to $(\\lambda_2, v_2)$ for the original matrix $A$. Finally, transform the computed eigenvector back using $H^T$.\n  </li>\n</ol>",
    "front": "<b>Power Method / Power Iteration</b> \n<ul>\n  <li><b>Define:</b> What is the power method and what does it compute?</li>\n  <li><b>Derivation:</b> What is the underlying principle behind its convergence?</li>\n  <li><b>Procedure:</b> What are the steps involved in the algorithm?</li>\n</ul>",
    "id": 1740891867068,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741072483001
      }
    ],
    "back": "<b><u>Definition</u></b> <br>\n<p>\nTwo square matrices $A$ and $B$ that are related by <br>\n$$\nB = X^{-1}AX\n$$\n<br>\nwhere $X$ is a square nonsingular matrix are said to be similar.\n</p>\n<br>\n<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/IMG_3E1D74CC9C45-1.jpeg\"></div> <br>\n\n<b><u>Properties</u></b> <br>\n<ul>\n  <li>\n    If $(\\lambda, v) \\text{ is an eigenpair of } B, \\text{ then } (\\lambda, Xv) \\text{ is an eigenpair of } A$.  \n    <ul>\n      <li><u>Proof</u> <br>\n        If \\( Bv = \\lambda v \\), then  \n        <br>\n        $$\n        A(Xv) = X B X^{-1} X v = X B v = \\lambda (Xv)\n        $$\n      </li>\n    </ul>\n  </li>\n</ul>\n\n<br><br>\n<ul>\n  <li>Identical characteristic polynomials</li>\n  <li>Equal determinants, traces, and ranks</li>\n  <li>The same minimal polynomial and Jordan canonical form</li>\n  <li>Consistent diagonalizability properties</li>\n</ul>",
    "front": "<b>Similar matrix</b> <br><br>\nDefine <br><br>\nProperties",
    "id": 1740892885417,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741074086548
      }
    ],
    "back": "<b><u>Definition</u></b> <br>\n<p>\nDiagonalization is the process of finding a diagonal matrix $D$ such that a square matrix $A$ is similar to $D$, meaning:  <br>\n$$\nA = X D X^{-1}\n$$\n</p>\n\nwhere:\n<ul>\n  <li>$D$ is a diagonal matrix whose entries are the eigenvalues of $A$.</li>\n  <li>$X$ is the matrix whose columns are the eigenvectors of $A$.</li>\n  <li>$A$ is diagonalizable if it has $n$ linearly independent eigenvectors.</li>\n</ul>\n\n<br>\n\n<b><u>Practical Use</u></b> <br>\n<ul>\n  <li><b>Computing Matrix Powers Efficiently:</b> If we need $A^k$, we can compute it as:\n    $$\n    A^k = X D^k X^{-1}\n    $$\n    where raising $D$ to a power is simple since it is diagonal.\n  </li>\n</ul>\n\n<br>\n\n<b><u>Procedure</u></b> <br>\n<ol>\n  <li><b>Find Eigenvalues:</b> Solve the characteristic equation:\n    $$\n    \\det(A - \\lambda I) = 0\n    $$\n    to find the eigenvalues $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$.\n  </li>\n  <li><b>Find Eigenvectors:</b> For each eigenvalue $\\lambda_i$, solve:\n    $$\n    (A - \\lambda_i I)v_i = 0\n    $$\n    to find the corresponding eigenvector $v_i$.\n  </li>\n  <li><b>Construct Matrices:</b>\n    <ul>\n      <li>$X$ is the matrix whose columns are the eigenvectors $v_1, v_2, \\dots, v_n$.</li>\n      <li>$D$ is the diagonal matrix with eigenvalues $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$ on the diagonal.</li>\n    </ul>\n  </li>\n  <li><b>Compute $X^{-1}$:</b> Since $X$ is invertible (if $A$ is diagonalizable), compute its inverse $X^{-1}$.</li>\n  <li><b>Verify the Similarity Relation:</b> Ensure that:\n    $$\n    A = X D X^{-1}\n    $$\n  </li>\n</ol>",
    "front": "<b>Diagonalization</b> \n<ul>\n  <li><b>Define:</b> What is diagonalization?</li>\n  <li><b>Practical Use:</b> Why is diagonalization useful?</li>\n  <li><b>Procedure:</b> How do we diagonalize a matrix?</li>\n</ul>",
    "id": 1741066673709,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "The Taylor series for a vector-valued function \\( F(x) \\) expands it around \\( x \\): <br><br>\n$$\nF(x + \\Delta x) \\approx F(x) + g(x)^T \\Delta x + \\frac{1}{2} \\Delta x^T H(x) \\Delta x + \\dots\n$$\n<br><br>\n\nwhere: <br>\n\n- \\( g(x) \\) is the gradient of \\( F \\): <br><br>\n$$\ng = \\begin{bmatrix} \\frac{\\partial F}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial F}{\\partial x_n} \\end{bmatrix}.\n$$\n\n<br><br>\n- \\( H(x) \\) is the Hessian matrix of second derivatives: <br><br>\n$$\nH =\n\\begin{bmatrix}\n\\frac{\\partial^2 F}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 F}{\\partial x_1 \\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 F}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 F}{\\partial x_n^2}\n\\end{bmatrix}.\n$$\n<br><br>\nTruncating the series gives a quadratic approximation of \\( F(x) \\) around \\( x \\).",
    "front": "Taylor series for a vector-valued function \\( F(x) \\)",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Positive definiteness of a real $n \\times n$ symmetric matrix $A$ is defined as <br><br>\n$$\nx^T A x > 0, \\quad \\forall x \\neq 0, \\; x \\in \\mathbb{R}^n\n$$",
    "front": "Positive definite (PD) <br><br>\nDefine<br><br>\nHow to test for it",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "meeting some necessary conditions does not mean the thing is true but its required that the condition be true when the thing is true. <br><br>\n\nsufficient condition when meant means the thing of interest is true.",
    "front": "Necessary vs. sufficient conditions",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "A positive definite (PD) Hessian guarantees a local minimum, while a positive semi-definite (PSD) Hessian is only necessary\u2014it does not confirm a minimum on its own.<br><br>\n\n\\[\n\\begin{array}{|c|c|c|c|}\n\\hline\n\\textbf{Condition} & \\textbf{Hessian Type} & \\textbf{What It Means} & \\textbf{Conclusion} \\\\\n\\hline\n\\text{Necessary} & \\text{Positive Semi-Definite (PSD)} & \\text{No negative curvature, but could be flat in some directions} & \\text{Local min possible, but not guaranteed} \\\\\n\\hline\n\\text{Sufficient} & \\text{Positive Definite (PD)} & \\text{Strictly upward curvature in all directions} & \\text{Guarantees a local min} \\\\\n\\hline\n\\end{array}\n\\]",
    "front": "Hessian necessary and sufficient conditions for a local minimum.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "$$\ng(x^*) = 0\n$$\n<br><br>\ngradient is zero.",
    "front": "Stationary point necessary condition.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "If $f: \\mathbb{R}^n \\to \\mathbb{R}$ is a differentiable function, the gradient of $f(x)$ is defined as:\n<br><br>\n\n$$\n\\nabla f(x) = \n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\\n\\frac{\\partial f}{\\partial x_2} \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial x_n}\n\\end{bmatrix}.\n$$\n\n<br><br>\nEach component $\\frac{\\partial f}{\\partial x_i}$ represents the partial derivative of $f$ with respect to $x_i$.\n<br><br>\n\nThe gradient is a vector that points in the direction of the steepest increase of $f(x)$.",
    "front": "Define the gradient of $f(x)$.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "A functional is a mapping from a space of functions to the real numbers. <br><br>\n\nMathematically, a functional $J$ takes a function $y(x)$ as input and returns a real number:\n<br><br>\n\n$$\nJ[y] = \\int_{a}^{b} F(x, y, y') \\, dx.\n$$\n\n<br><br>\nFunctionals are commonly used in calculus of variations and physics to express quantities such as energy or action.",
    "front": "What is a <b>functional</b>?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "The first variation of a functional is the first-order term in the Taylor expansion of the increment, providing a linear approximation of the change in the functional. <br><br>\n\nThe first variation of a functional $J[x(t)]$ with respect to a perturbation $\\delta x(t)$ is defined as:\n<br><br>\n\n$$\n\\delta J(x(t), \\delta x(t)) = \\left. \\frac{d}{d\\epsilon} J(x(t) + \\epsilon \\delta x(t)) \\right|_{\\epsilon=0}.\n$$\n<br><br>\n\n<b>TODO: fix beyond here to make more sense</b>\n<br><br>\n\nFor a functional $J[y]$, we express its perturbation as:\n<br><br>\n\n$$\nJ[y + \\epsilon \\delta y] = \\int_a^b F(x, y + \\epsilon \\delta y, y' + \\epsilon \\delta y') \\, dx.\n$$\n<br><br>\n\nExpanding $F( \\cdot )$ in a Taylor series around $\\epsilon = 0$ gives:\n<br><br>\n$$\nF(x, y + \\epsilon \\delta y, y' + \\epsilon \\delta y') = F(x, y, y') + \\epsilon \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) + O(\\epsilon^2).\n$$\n<br>\n\nSubstituting this into the integral:\n<br><br>\n$$\nJ[y + \\epsilon \\delta y] = J[y] + \\epsilon \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx + O(\\epsilon^2).\n$$\n<br><br>\n\nTaking the limit as $\\epsilon \\to 0$, we obtain the first variation:\n<br><br>\n$$\n\\delta J = \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx.\n$$\n<br>",
    "front": "What is the definition of an <b>increment</b>?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "The first variation of functional is a linear approximation of the increment, i.e., first-order Taylor expansion of the increment.<br><br>\n\nThe **first variation** of a functional $J[x(t)]$ with respect to a perturbation $\\delta x(t)$ is defined as:\n<br><br>\n\n$$\n\\delta J(x(t), \\delta x(t)) = \\left. \\frac{d}{d\\epsilon} J(x(t) + \\epsilon \\delta x(t)) \\right|_{\\epsilon=0}.\n$$\n<br><br>\n\nFor a functional $J[y]$ and a small perturbation $\\delta y$, consider:\n<br><br>\n\n$$\nJ[y + \\epsilon \\delta y] = \\int_a^b F(x, y + \\epsilon \\delta y, y' + \\epsilon \\delta y') \\, dx.\n$$\n<br><br>\n\nExpanding the integrand $F( \\cdot )$ in a Taylor series around $\\epsilon = 0$, we obtain:\n<br><br>\n$$\nF(\\cdot) = F(x, y, y') + \\epsilon \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) + O(\\epsilon^2).\n$$\n<br><br>\n\nSubstituting this expansion back into the integral gives:\n<br><br>\n$$\nJ[y + \\epsilon \\delta y] = \\int_a^b \\Bigg[ F(x, y, y') + \\epsilon \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) + O(\\epsilon^2) \\Bigg] dx.\n$$\n<br>\n\nThis can be rewritten as:\n<br><br>\n$$\nJ[y + \\epsilon \\delta y] = J[y] + \\epsilon \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx + O(\\epsilon^2).\n$$\n<br><br>\n\nThe first variation $\\delta J$ is defined as the first-order term:\n<br><br>\n$$\n\\delta J = \\lim_{\\epsilon \\to 0} \\frac{J[y + \\epsilon \\delta y] - J[y]}{\\epsilon} = \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx.\n$$\n<br>",
    "front": "What is the definition of the <b>first variation</b>?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "The Euler-Lagrange equation states that for a function $g(x, \\dot{x}, t)$:\n<br><br>\n\n$$\n\\frac{d}{dt} \\left( \\frac{\\partial g}{\\partial \\dot{x}} \\right) - \\frac{\\partial g}{\\partial x} = 0\n\n\\quad \\quad \\quad \\quad \\text{or} \\quad \\quad \\quad \\quad\n\n\\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{x}} \\right) - \\frac{\\partial L}{\\partial x} = 0\n$$\n\n<br><br>\nRewriting this:\n\n<br><br>\n$$\n\\frac{d}{dt} (g_{\\dot{x}}) - g_x = 0\n\n\\quad \\quad \\quad \\quad \\text{or} \\quad \\quad \\quad \\quad\n\n\\frac{d}{dt} (L_{\\dot{x}}) - L_x = 0\n$$\n\n<br><br>\nwhich is the fundamental condition for extremizing a functional in the calculus of variations.",
    "front": "Define and derive the <b>Euler-Lagrange equation</b>.\n<br><br>",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "$$\nH(x, u, \\lambda, t) = f(x, u, t) \\cdot \\lambda + L(x, u, t).\n$$\n<br><br>\nwhere: <br>\n- x  is the state variable. <br>\n- u  is the control variable. <br>\n- \\lambda  is the costate (adjoint) variable (analogous to momentum in classical mechanics). <br>\n- L(x, u, t)  is the running cost.<br>\n- f(x, u, t)  describes the system dynamics.\n\n<br><br><br><br>\n$$\nH = \\dot{x} \\frac{\\partial L}{\\partial \\dot{x}} - L \n  \\quad \\text{or} \\quad \nH = \\dot{x} \\frac{\\partial g}{\\partial \\dot{x}} - g\n$$\n<br><br>\nRewriting as:<br><br>\n$$\nH = \\dot{x} L_{\\dot{x}} - L \n  \\quad \\text{or} \\quad \nH = \\dot{x} g_{\\dot{x}} - g\n$$\n\n<br><br>\n\nNote that:<br><br>\n$$\ng_t + \\frac{dH}{dt} = 0.\n$$\n\n<br><br>\nThe crucial property of the Hamiltonian is that:<br>\n- If  $L$  does not explicitly depend on time (i.e.,  $L_t = 0$ ), then  $H$  is conserved.<br>\n- This leads directly to the Beltrami identity, which states that:\n$$\n\\dot{x} g_{\\dot{x}} - g = \\text{constant}.\n$$",
    "front": "Hamiltonian",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "$$\n\\min_{x \\in \\mathbb{R}^{n}}{f(x)}\n$$\n<br><br>\nsubject to: <br>\n$$\ng_{i}(x) \\leq 0, \\quad i \\in \\mathcal{I}, \\quad \\mathcal{I} = 1, \\dots, m \\quad \\text{(inequality constraints)}\n$$\n<br>\n$$\nh_{i}(x) = 0, \\quad j \\in \\mathcal{E}, \\quad \\mathcal{E} = 1, \\dots, p \\quad \\; \\text{(equality constraints)}\n$$\n<br><br>\nwhere:\n<ul>\n  <li>$$f(x)$$ is the objective function to be minimized.</li>\n  <li>$$g_i(x) \\leq 0$$ represents inequality constraints that must be satisfied.</li>\n  <li>$$h_j(x) = 0$$ represents equality constraints that must be exactly satisfied.</li>\n  <li>$$x \\in \\mathbb{R}^n$$ is the decision variable (a vector in \\( n \\)-dimensional space).</li>\n</ul>",
    "front": "Standard form of a constrained optimization problem.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<ol>\n\n  <li>Stationarity <br>\n    $$\n    \\nabla f(x^*) + \\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*) + \\sum_{j=1}^{p} \\mu_j \\nabla h_j(x^*) = 0\n    $$\n  </li>\n  <br>\n\n  <li>Primal feasibility <br>\n    $$\n    g_i(x^*) \\leq 0, \\quad \\forall i \\in \\{1, \\dots, m\\}\n    $$\n    <br>\n    $$\n    h_j(x^*) = 0, \\quad \\forall j \\in \\{1, \\dots, p\\}\n    $$\n  </li>\n  <br>\n\n  <li>Dual feasibility <br>\n    $$\n    \\lambda_i \\geq 0, \\quad \\forall i \\in \\{1, \\dots, m\\}\n    $$\n  </li>\n  <br>\n\n  <li>Complementary slackness <br>\n    $$\n    \\lambda_i g_i(x^*) = 0, \\quad \\forall i \\in \\{1, \\dots, m\\}\n    $$\n  </li>\n  <br>\n\n</ol>",
    "front": "KKT Conditions.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "When the gradients of the active constraints are linearly dependent\u2014that is, when the Linear Independence Constraint Qualification (LICQ) fails. In this case, the constraint gradients do not span enough directions to cancel out the gradient of the objective, meaning the stationarity condition cannot be met.",
    "front": "Under what circumstances can we fail to choose Lagrange multipliers that satisfy the KKT conditions without contradiction?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "At an optimal point $x^*$, the idea is that no small, feasible perturbation $d$ should be able to decrease the objective function. In other words, for every feasible direction $d$, we must have <br><br>\n\n$$\n\\nabla f(x^{*})^T d \\geq 0.\n$$\n\n<br><br>\nNow, when a constraint $g_i(x) \\leq 0$ is active (i.e. $g_i(x^*) = 0)$, any small movement d that keeps us feasible must satisfy <br><br>\n$$\n\\nabla g_i(x^{*})^T d \\leq 0.\n$$\n<br><br>\nThis means the feasible directions are limited by the gradients of the active constraints. For there to be no feasible descent direction\u2014that is, for the objective not to decrease along any $d$\u2014the entire gradient $\\nabla f(x^*)$ must be \u201cneutralized\u201d by the constraint forces. This requirement is captured by the stationarity condition: <br><br>\n$$\n\\nabla f(x^*) + \\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*) = 0.\n$$\n<br><br>\nHere, the multipliers $\\lambda_i$ adjust the contribution of each constraint\u2019s gradient so that they together cancel out $\\nabla f(x^*)$. This ensures that any movement allowed by the constraints does not provide a descent direction, preserving the optimality of $x^*$.\n<br>",
    "front": "Why must the stationarity condition hold at an optimal solution in constrained optimization?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "At an optimal point $x^*$, the gradients of the active constraints, $\\nabla g_i(x^*)$ for all $i$ such that $g_i(x^*) = 0$, define the normal (or \u201cblocking\u201d) directions at $x^*$. They essentially tell us which directions are forbidden for movement if we want to remain feasible. <br><br>\n\nFor $x^*$ to be optimal, the gradient $\\nabla f(x^*)$ must not point in any direction that would allow us to decrease $f(x)$ while staying within the feasible region. This is only possible if $\\nabla f(x^*)$ lies entirely in the span of the constraint gradients. In mathematical terms, this means that there exist multipliers $\\lambda_i$ such that <br><br>\n\n$$\n\\nabla f(x^*) = -\\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*).\n$$\n\n<br><br>\nIf any component of $\\nabla f(x^*)$ were to lie outside of the span of the $\\nabla g_i(x^*)$, then there would be a feasible direction in which $f(x)$ could decrease, contradicting the optimality of $x^*$. <br><br>\n\nThus, the condition guarantees that all possible descent directions are \u201cblocked\u201d by the active constraints, ensuring that $x^*$ is indeed optimal.",
    "front": "What is the geometric intuition behind the KKT stationarity condition?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "The LICQ condition states that at a feasible point $x^*$, the gradients of all active constraints must be linearly independent. <br><br>\n\nMathematically, if the set of active constraints at $x^*$ is given by:\n<br><br>\n$$\n\\mathcal{A} = \\{ i \\mid g_i(x^*) = 0 \\},\n$$\n<br><br>\nthen the gradients of these constraints,\n<br><br>\n$$\n\\{ \\nabla g_i(x^*) \\}_{i \\in \\mathcal{A}},\n$$\n<br><br>\nmust be linearly independent. This means that if there exists a set of scalars $\\lambda_i$ such that:\n<br><br>\n$$\n\\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*) = 0,\n$$\n<br><br>\nthen it must follow that $\\lambda_i = 0$ for all $i \\in \\mathcal{A}$. <br><br>\n\nIf LICQ holds, then the active constraint gradients span a well-defined space, ensuring that the KKT conditions hold without contradiction. <br><br>\n\nIf LICQ fails, the stationarity condition may not be satisfied, meaning no valid Lagrange multipliers exist.",
    "front": "Linear Independence Constraint Qualification (LICQ) condition.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO",
    "front": "Collocation points",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Interpolation points.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "General way to express most optimal control problems: <br><br>\n\n\\[\n\\begin{array}{ll}\n\\textbf{Minimize} & J = \\phi\\Bigl(x(t_0),\\, x(t_f),\\, t_0,\\, t_f,\\, q,\\, s\\Bigr) \\\\\n\\\\\n\\textbf{with respect to} & x(t),\\; u(t),\\; t_0,\\; t_f,\\; s,\\; q \\\\\n\\\\\n\\textbf{subject to} & \\dot{x}(t) = f\\Bigl(x(t),\\, u(t),\\, t,\\, s\\Bigr), \\quad t \\in [t_0, t_f], \\\\\n\\\\\n& q = \\int_{t_0}^{t_f} g\\Bigl(x(t),\\, u(t),\\, t,\\, s\\Bigr) dt, \\\\\n\\\\\n& c\\Bigl(x(t),\\, u(t),\\, t,\\, s\\Bigr) \\le 0, \\quad t \\in [t_0, t_f], \\\\\n\\\\\n& \\psi\\Bigl(x(t_0),\\, x(t_f),\\, t_0,\\, t_f,\\, s\\Bigr) = 0, \\\\\n\\\\\n& \\text{and appropriate bounds on } x(t),\\, u(t),\\, s,\\, t_0,\\, t_f.\n\\end{array}\n\\]",
    "front": "Bolza optimization problem formulation.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content\n\n<br><br>\nThe following functions are transcendental:<br>\n<img src=\"/assets/Screenshot_2025-02-28_at_1.31.02_AM.png\">",
    "front": "Transcendental equation",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content <br><br>\n\nWhat is it and what is the equation for it?",
    "front": "Catenary curve.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content <br><br>\n\nWhat is it and what is the equation for it?",
    "front": "Catenoid.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  }
]