[
  {
    "attempts": [],
    "back": "Taylor series is the infinite series:<br><br>\n$$\nf(x) = \\sum_{n=0}^{\\inf}{\\frac{f^{(n)}(a)}{n!}(x - a)^{n}}\n$$\n<br><br>\nTaylor series \"expansion\" of a function typically implies truncation, meaning we approximate the function $f(x)$ by a finite number of terms. <br><br>\n$$\nf(x) \\approx f(a) + f^{\\prime}(a)(x-a) + \\dots + \\frac{f^{(N)}(a)}{N!} (x-a)^N.\n$$\n<br><br>",
    "front": "<b>Taylor Series vs. Taylor Expansion</b>  \n<ul>  \n  <li><b>Define:</b> What is the Taylor series of \\( f(x) \\) at \\( x = a \\)?</li>  \n  <li><b>Define:</b> What is the Taylor series expansion of \\( f(x) \\) at/around \\( x = a \\)?</li>  \n  <li><b>Difference:</b> How do the Taylor series and Taylor expansion differ in meaning and usage?</li>  \n</ul>  ",
    "importance": 2,
    "tags": [
      "18.065",
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Logarithmic Integral Identity <br>\n$$\n\\int \\frac{du}{u} = \\ln |u| + C.\n$$\n",
    "front": "What identity is this? <br>\n$$\n\\int \\frac{du}{u} = \\, \\text{?}\n$$",
    "id": 1741153604364,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Power Rule for Integration <br>\n$$\n\\int u^n \\, du = \\frac{u^{n+1}}{n+1} + C, \\quad n \\neq -1.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int u^n \\, du = \\, \\text{?} \\quad (n \\neq -1)\n$$",
    "id": 1741153795280,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Exponential Integral Identity <br>\n$$\n\\int e^u \\, du = e^u + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int e^u \\, du = \\, \\text{?}\n$$",
    "id": 1741153863506,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Exponential Integral for Base \\( a \\) <br>\n$$\n\\int a^u \\, du = \\frac{a^u}{\\ln a} + C, \\quad a > 0, a \\neq 1.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int a^u \\, du = \\, \\text{?}\n$$",
    "id": 1741153887555,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Sine Integral Identity <br>\n$$\n\\int \\sin u \\, du = -\\cos u + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\sin u \\, du = \\, \\text{?}\n$$",
    "id": 1741153930393,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Cosine Integral Identity <br>\n$$\n\\int \\cos u \\, du = \\sin u + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\cos u \\, du = \\, \\text{?}\n$$",
    "id": 1741153943600,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Tangent Integral Identity <br>\n$$\n\\int \\tan u \\, du = \\ln |\\sec u| + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\tan u \\, du = \\, \\text{?}\n$$",
    "id": 1741153956036,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Secant Integral Identity <br>\n$$\n\\int \\sec u \\, du = \\ln |\\sec u + \\tan u| + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\sec u \\, du = \\, \\text{?}\n$$",
    "id": 1741153992908,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Cosecant Integral Identity <br>\n$$\n\\int \\csc u \\, du = \\ln |\\csc u - \\cot u| + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\csc u \\, du = \\, \\text{?}\n$$",
    "id": 1741154002664,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Inverse Sine Integral Identity <br>\n$$\n\\int \\frac{du}{\\sqrt{1 - u^2}} = \\sin^{-1} u + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\frac{du}{\\sqrt{1 - u^2}} = \\, \\text{?}\n$$",
    "id": 1741154022762,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Inverse Tangent Integral Identity <br>\n$$\n\\int \\frac{du}{1 + u^2} = \\tan^{-1} u + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\frac{du}{1 + u^2} = \\, \\text{?}\n$$",
    "id": 1741154033446,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Hyperbolic Integral Identity <br>\n$$\n\\int \\frac{du}{\\sqrt{u^2 + C_1}} = \\ln | u + \\sqrt{u^2 + C_1} | + C_2.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\frac{du}{\\sqrt{u^2 + C}} = \\, \\text{?}\n$$",
    "id": 1741154053536,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Integral of \\( \\frac{1}{A + u^2} \\) <br>\n$$\n\\int \\frac{du}{A + u^2} = \\frac{1}{\\sqrt{A}} \\tan^{-1} \\left(\\frac{u}{\\sqrt{A}}\\right) + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\frac{du}{A + u^2} = \\, \\text{?}\n$$",
    "id": 1741154072452,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Integration by Parts Formula <br>\n$$\n\\int u v' \\, du = uv - \\int u' v \\, du.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int u v' \\, du = \\, \\text{?}\n$$",
    "id": 1741154102843,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "back": "<b><u>Definition</u></b> <br>\n$$\n\\sinh x = \\frac{e^x - e^{-x}}{2}.\n$$",
    "front": "<b>Hyperbolic Sine</b>\n<ul>\n    <li><b>Define:</b> What is the definition of hyperbolic sine?</li>\n</ul>",
    "id": 1741155965429,
    "tags": [
      "16.32"
    ],
    "understanding": 1,
    "attempts": []
  },
  {
    "back": "<b><u>Definition</u></b> <br>\n$$\n\\cosh x = \\frac{e^x + e^{-x}}{2}.\n$$",
    "front": "<b>Hyperbolic Cosine</b>\n<ul>\n    <li><b>Define:</b> What is the definition of hyperbolic cosine?</li>\n</ul>",
    "id": 1741155975921,
    "tags": [
      "16.32"
    ],
    "understanding": 1,
    "attempts": []
  },
  {
    "back": "<b><u>Separation of Variables Technique</u></b> <br>\nSeparation of variables is a method for solving first-order ordinary differential equations (ODEs) of the form: <br>  \n$$\n\\frac{dy}{dx} = f(x) g(y).\n$$  \n<br>  \nThe method involves rewriting the equation so that all terms involving \\( y \\) are on one side and all terms involving \\( x \\) are on the other: <br>  \n$$\n\\frac{dy}{g(y)} = f(x) dx.\n$$  \n<br>  \nThen, both sides are integrated separately: <br>  \n$$\n\\int \\frac{dy}{g(y)} = \\int f(x) dx.\n$$  \n<br>  \nSolving these integrals gives the general solution to the differential equation.",
    "front": "What is the separation of variables technique in differential equations?",
    "id": 1741156069533,
    "tags": [
      "16.32"
    ],
    "understanding": 1,
    "attempts": []
  },
  {
    "attempts": [],
    "back": "<b><u>Definition</u></b><br>  \nCompleting the square is an algebraic technique used to transform a quadratic expression into a perfect square trinomial plus a constant. This method is useful for solving quadratic equations, analyzing parabolas, and deriving the quadratic formula.<br><br>  \n\n<b><u>Procedure</u></b><br>  \nGiven a quadratic expression of the form: $ax^2 + bx + c$\n<ol>  \n  <li>Factor out \\( a \\) if it is not 1: <br>\n      \\[\n      a(x^2 + \\frac{b}{a}x) + c\n      \\]  \n  </li>  \n  <li>Add and subtract \\( \\left(\\frac{b}{2a}\\right)^2 \\) inside the parentheses to form a perfect square: <br>\n      \\[\n      a\\left(x^2 + \\frac{b}{a}x + \\left(\\frac{b}{2a}\\right)^2 - \\left(\\frac{b}{2a}\\right)^2\\right) + c\n      \\]  \n  </li>  \n  <li>Rewrite the perfect square trinomial as a squared binomial: <br>\n      \\[\n      a\\left( \\left(x + \\frac{b}{2a} \\right)^2 - \\left(\\frac{b}{2a}\\right)^2 \\right) + c\n      \\]  \n  </li>  \n  <li>Distribute \\( a \\) and simplify: <br>\n      \\[\n      a\\left(x + \\frac{b}{2a}\\right)^2 - a\\left(\\frac{b}{2a}\\right)^2 + c\n      \\]  \n  </li>  \n  <li>Final form: <br>\n      \\[\n      a\\left(x + \\frac{b}{2a}\\right)^2 + \\left(c - \\frac{b^2}{4a}\\right)\n      \\]  \n  </li>  \n</ol>\n <br>\n\nThis method is particularly useful for solving quadratic equations, deriving the quadratic formula, and completing square-based integrals in calculus.",
    "front": "<b>Complete the Square</b>  \n<ul>  \n  <li><b>Define:</b> What does it mean to complete the square in an algebraic expression?</li>  \n  <li><b>Procedure:</b> How do you complete the square for a quadratic expression of the form \\( ax^2 + bx + c \\)?</li>  \n</ul>  ",
    "id": 1741151957761,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<b><u>Definition</u></b><br>  \nThe Beltrami identity is a conserved quantity that arises in the calculus of variations when the Lagrangian does not explicitly depend on the independent variable. It provides a first integral of the Euler-Lagrange equation, simplifying the problem of finding extremals.<br><br>  \n\n<b><u>Derivation & Usage</u></b><br>  \nIf a functional is given by \\( J = \\int L(y, y', x) \\,dx \\), and \\( L \\), the Lagrangian, does not explicitly depend on \\( x \\), the independent variable, then the Beltrami identity states that:<br>  \n\\[\nL - y' \\frac{\\partial L}{\\partial y'} = C\n\\]\nwhere \\( C \\) is a constant.<br><br>  \nThis identity is particularly useful in reducing the order of the Euler-Lagrange equation, making it easier to solve variational problems where the Lagrangian lacks explicit dependence on the independent variable.",
    "front": "<b>Beltrami Identity</b>  \n<ul>  \n  <li><b>Define:</b> What is the Beltrami identity in the context of the calculus of variations?</li>  \n  <li><b>Derivation & Usage:</b> How is the Beltrami identity derived, and when is it used to simplify the Euler-Lagrange equation?</li>  \n</ul>  ",
    "id": 1741151016852,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "<b>Hessian of a Vector Function</b> \n<ul>\n  <li><b>Define:</b> What is the Hessian of a vector function?</li>\n  <li><b>Procedure:</b> How is the Hessian matrix computed, and what does it represent in terms of curvature?</li>\n</ul>",
    "id": 1741150839007,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO <br><br>\n\nquadrature in the context of numerical optimal control <br><br>\n\nquadrature used to approximate integrals in optimal control problems",
    "front": "<b>Quadrature</b> \n<ul>\n  <li><b>Define:</b> What is quadrature in the context of numerical optimal control?</li>\n  <li><b>Procedure:</b> How is used?</li>\n</ul>",
    "id": 1741148713751,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Explain how to convert this optimal control problem into a standard calculus of variations problem where you seek the function $x(t)$  minimizing a cost functional $J(x)$ subject to boundary conditions.",
    "id": 1741150252242,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<b><u>Definition</u></b> <br>\nThe Euler method is a first-order explicit numerical method that uses the current point to estimate the next point in a step wise fashion.\n<br><br>\n\n<b><u>Practical Purpose</u></b> <br>\nIt is used to approximate solutions to ODEs.\n",
    "front": "Euler method <br><br>\nDefine <br><br>\nPractical purpose",
    "id": 1741038606887,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/trapezoid_rule.webp\"></div>",
    "front": "Trapezoidal Rule <br><br>\n\nPractical use <br><br>\n\nDefinition",
    "id": 1741038908417,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Approach to discretely approximating integrals.\n\n<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/midpoint_rule.jpg\"></div>",
    "front": "Midpoint Rule <br><br>\n\nPractical use <br><br>\n\nDefinition",
    "id": 1741041716871,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "A set of vectors that perfectly describes the space. <br><br>\n\nTheir combinations give one and only one way to produce every vector in the space.",
    "front": "Basis",
    "id": 1740885744839,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/four_fundamental_subspaces.jpeg\"></div>\n<br>\nDimensions: <br>\n$Col(A) \\in \\mathbb{R}^{r}$ <br>\n$Null(A) \\in \\mathbb{R}^{n - r}$ <br>\n$Col(A^T) \\in \\mathbb{R}^{r}$ <br>\n$Null(A) \\in \\mathbb{R}^{m - r}$ <br>",
    "front": "Four Fundamental Subspaces",
    "id": 1740884070112,
    "tags": [
      "18.065"
    ],
    "understanding": 2
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Card for mom",
    "id": 1740894978704,
    "tags": [
      "14.13"
    ],
    "understanding": 3
  },
  {
    "attempts": [],
    "back": "- denoted $Col(A) = C(A)$ <br>\n- the column space is a subspace of $\\mathbb{R}^m$ ($A \\in \\mathbb{R}^{n \\times m}$)<br>\n- consists of all linear combinations of $A$'s columns <br>\n- this is captured by the space spanned by $b$ in $Ax = b, \\, \\forall x$",
    "front": "Column space of $A$",
    "id": 1740886684323,
    "tags": [
      "18.065"
    ],
    "understanding": 2
  },
  {
    "attempts": [],
    "back": "- denoted $Col(A^T) = C(A)$, <br>\n- the column space is a subspace of $\\mathbb{R}^n$ (recall $A \\in \\mathbb{R}^{n \\times m}$)<br>\n- consists of all linear combinations of the columns of $A^T$'s <br>\n- this is captured by the space spanned by $b$ in $A^Ty = b, \\, \\forall y$",
    "front": "Row space of $A$ <br><br>",
    "id": 1740887337718,
    "tags": [
      "18.065",
      "hello"
    ],
    "understanding": 2
  },
  {
    "attempts": [],
    "back": "<b><u>Definition</u></b> <br>  \nLet \\( A \\in \\mathbb{R}^{m \\times n} \\) be a matrix. The null space of \\( A \\) is defined as: <br>  \n\\[\n\\text{Null}(A) = \\{ x \\in \\mathbb{R}^n \\mid Ax = 0 \\}\n\\]\nwhich is a subspace of \\( \\mathbb{R}^n \\). <br><br>  \n\nSimilarly, the null space of the transpose \\( A^T \\) is: <br>  \n\\[\n\\text{Null}(A^T) = \\{ y \\in \\mathbb{R}^m \\mid A^T y = 0 \\}\n\\]\nwhich is a subspace of \\( \\mathbb{R}^m \\). <br><br>  \n\n<b><u>Dimension Relationships</u></b> <br>  \nThe dimensions of the null space and the row/column spaces are related by: <br>  \n\\[\n\\dim(\\text{Row}(A)) + \\dim(\\text{Null}(A)) = n,\n\\]\n\\[\n\\dim(\\text{Col}(A)) + \\dim(\\text{Null}(A^T)) = m.\n\\]  ",
    "front": "<b>Null Space</b>\n<ul>\n    <li><b>Define:</b> What is the null space of a matrix?</li>\n    <li><b>Properties:</b> What are the key dimension relationships involving the null space?</li>\n</ul>",
    "id": 1740887782700,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "id": 1741167244470,
    "front": "<b>Orthogonal Complement</b>  \n<ul>  \n    <li><b>Define:</b> What is the orthogonal complement of a subspace in \\( \\mathbb{R}^n \\)?</li>  \n    <li><b>Procedure:</b> How do you determine the orthogonal complement of a given subspace?</li>  \n    <li><b>Theorem:</b> How are the null space and row space related via the orthogonal complement?</li>  \n</ul>  ",
    "back": "<b><u>Definition</u></b> <br>  \nThe orthogonal complement of a subspace \\( S \\) in \\( \\mathbb{R}^n \\), denoted \\( S^\\perp \\), is the set of all vectors in \\( \\mathbb{R}^n \\) that are orthogonal to every vector in \\( S \\):  \n$$  \nS^\\perp = \\{ x \\in \\mathbb{R}^n \\mid x \\perp y, \\; \\forall y \\in S \\}.  \n$$\n<br>\nA vector \\( x \\) is orthogonal to \\( y \\) if and only if their inner product is zero:<br>  \n$$  \nx \\perp y \\iff \\langle x, y \\rangle = x^T y = 0.  \n$$  \n<br><br>\n\n<b><u>Procedure</u></b> <br>  \n1. Find a basis for the subspace \\( S \\). <br>  \n2. Solve for all vectors \\( x \\) that satisfy \\( x^T y = 0 \\) for every basis vector \\( y \\) in \\( S \\). <br>  \n3. The set of all such \\( x \\) forms the orthogonal complement \\( S^\\perp \\). <br>  \n4. If \\( \\dim(S) = k \\) in \\( \\mathbb{R}^n \\), then \\( \\dim(S^\\perp) = n - k \\). <br>  \n<br>  \n\n<b><u>Theorem</u></b> <br>  \nThe null space and row space of a matrix are related through the orthogonal complement:  <br>\n$$  \n\\text{Null}(A) = (\\text{Row}(A))^\\perp.  \n$$\n<br>\nSimilarly, the null space of the transpose is the orthogonal complement of the column space: <br>\n$$  \n\\text{Null}(A^T) = (\\text{Col}(A))^\\perp.  \n$$\n<br>\nThis means that the row space and column space define the constraints that determine the null spaces of \\( A \\) and \\( A^T \\), respectively.  ",
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741074654248
      }
    ],
    "back": "<u><b>Definition</b></u> <br>\n<p> \nFor a matrix $A \\in \\mathbb{R}^{m \\times n}$, a vector $x \\in \\mathbb{R}^{n} \\setminus \\{0\\}$, and a scalar $\\lambda \\in \\mathbb{R}$, the eigenvalues $\\lambda$ and eigenvectors $x$ satisfy:  <br>\n$$\nAx = \\lambda x\n$$\n</p> \n<br>\n\n<u><b>Procedure</b></u> <br>\n<ol>\n    <li>Rearrange the equation to:\n        $$\n        (A - \\lambda I)x = 0\n        $$\n    </li>\n    <li>For a nonzero $x$, the system has a nontrivial solution only if:\n        $$\n        \\det(A - \\lambda I) = 0\n        $$\n    </li>\n    <li>Solve the characteristic equation $\\det(A - \\lambda I) = 0$ for $\\lambda$ to find the eigenvalues.</li>\n    <li>For each $\\lambda$, substitute into:\n        $$\n        (A - \\lambda I)x = 0\n        $$\n        and solve the resulting system for the eigenvector $x$.</li>\n</ol>",
    "front": "<b>Eigenvalues and Eigenvectors</b> \n<ul>\n  <li><b>Define:</b> What are eigenvalues and eigenvectors?</li>\n  <li><b>Procedure:</b> How do we compute eigenvalues and eigenvectors?</li>\n</ul>",
    "id": 1740890671935,
    "tags": [
      "18.065",
      "16.32"
    ],
    "understanding": 2
  },
  {
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741075650753
      }
    ],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nFor a square matrix $A$, the <b>characteristic polynomial</b> is defined as:\n$$\np(\\lambda) = \\det(A - \\lambda I)\n$$\n</p>\n<br>\n<u><b>Derivation</b></u> <br>\n<ol>\n  <li>Start with the eigenvalue equation:\n    $$\n    Ax = \\lambda x\n    $$\n    for a nonzero vector $x$.\n  </li>\n  <li>Rearrange the equation to:\n    $$\n    (A - \\lambda I)x = 0\n    $$\n  </li>\n  <li>Since $x \\neq 0$, a nontrivial solution exists only if the matrix $(A - \\lambda I)$ is singular, hence:\n    $$\n    \\det(A - \\lambda I) = 0\n    $$\n  </li>\n  <li>This determinant equation is the <b>characteristic polynomial</b> of $A$.\n  </li>\n</ol>",
    "front": "<b>Characteristic Polynomial</b> \n<ul>\n  <li><b>Define:</b> What is the characteristic polynomial?</li>\n  <li><b>Derivation:</b> How is it derived from the eigenvalue equation $Ax = \\lambda x$?</li>\n</ul>",
    "id": 1741075245856,
    "tags": [
      "18.065"
    ],
    "understanding": 2
  },
  {
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nGauss elimination is a systematic method for solving systems of linear equations. It involves applying elementary row operations to an augmented matrix to transform it into row echelon form (or reduced row echelon form), from which the solutions can be obtained through back substitution.\n</p>\n<br>\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Write the augmented matrix for the system of equations.</li>\n  <li>Apply elementary row operations:\n    <ul>\n      <li>Swap rows.</li>\n      <li>Multiply a row by a nonzero scalar.</li>\n      <li>Add or subtract a multiple of one row from another.</li>\n    </ul>\n  </li>\n  <li>Reduce the matrix to row echelon form (upper triangular form).</li>\n  <li>(Optional) Further reduce to reduced row echelon form for a unique solution.</li>\n  <li>Perform back substitution to solve for the variables.</li>\n</ol>",
    "front": "<b>Gauss Elimination</b> \n<ul>\n  <li><b>Define:</b> What is Gauss elimination?</li>\n  <li><b>Procedure:</b> How is Gauss elimination performed?</li>\n</ul>",
    "id": 1740890812452,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<ul>\n  <li>Denoted $rref(A)$</li>\n  <li>Rules:\n    <ol>\n      <li>Each leading 1 (pivot) is the only nonzero entry in its column.</li>\n      <li>The leading 1 in each row appears to the right of the leading 1 in the row above.</li>\n      <li>Any rows of all zeros appear in the bottom rows of the matrix.</li>\n    </ol>\n  </li>\n  <li>The first $r$ pivot columns form an identity-like structure.</li>\n  <li>The remaining $n - r$ columns, denoted as $F$, contain the free variables.</li>\n</ul>",
    "front": "Reduced row echelon form",
    "id": 1740887871355,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "To solve $Ax = b$ is to <u>express b as a linear combination of the columns of $A$.</u>",
    "front": "To solve $Ax = b$ is to ____.",
    "id": 1740886790619,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "The equations $Ax = b$ are solvable iff <u>${b}$ is in the column space of $A$</u>",
    "front": "The equations $Ax = b$ are solvable iff ____.",
    "id": 1740887017912,
    "tags": [
      "18.065"
    ],
    "understanding": 2
  },
  {
    "attempts": [],
    "back": "Inner product / dot product as implied by the multiplication of a transposed column vector $1 \\times n$ and a column vector $n \\times 1$.<br><br>\n\nThe dot product is given by: <br><br>\n$$\n\\mathbf{a}^\\top \\mathbf{b} =\n\\begin{bmatrix} a_1 & a_2 & \\dots & a_n \\end{bmatrix}\n\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{bmatrix} =\n\\sum_{i=1}^{n} a_i b_i\n$$",
    "front": "Let  $a$  and  $b$  be column vectors in $\\mathbb{R}^n$. <br><br>\n\nWhat is $a^Tb$ ?\n",
    "id": 1740895695348,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nA square matrix $A$ is <b>invertible</b> (or nonsingular) if there exists a matrix $B$ such that:\n$$\nAB = BA = I\n$$\nwhere $I$ is the identity matrix.\n</p>\n<br>\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Ensure that $A$ is square.</li>\n  <li>Check if the determinant $\\det(A)$ is nonzero. If $\\det(A) \\neq 0$, then $A$ is invertible.</li>\n  <li>If invertible, compute the inverse $A^{-1}$ using methods like Gaussian elimination or the adjugate formula.</li>\n</ol>",
    "front": "<b>Invertible Matrix</b> \n<ul>\n  <li><b>Define:</b> What is an invertible matrix?</li>\n  <li><b>Practical Use:</b> Why is invertibility important?</li>\n  <li><b>Procedure:</b> How do we determine if a matrix is invertible?</li>\n</ul>",
    "id": 1740887674549,
    "tags": [
      "18.065"
    ],
    "understanding": 2
  },
  {
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nA square matrix $A$ is called <b>singular</b> if it is not invertible. This is equivalent to:\n$$\n\\det(A) = 0\n$$\n</p>\n<br>\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Ensure that $A$ is square.</li>\n  <li>Calculate the determinant $\\det(A)$. If $\\det(A) = 0$, then $A$ is singular.</li>\n  <li>Interpretation: A singular matrix has linearly dependent columns (or rows) and does not possess an inverse.</li>\n</ol>",
    "front": "<b>Singular Matrix</b> \n<ul>\n  <li><b>Define:</b> What is a singular matrix?</li>\n  <li><b>Practical Use:</b> What does it imply when a matrix is singular?</li>\n  <li><b>Procedure:</b> How do we determine if a matrix is singular?</li>\n</ul>",
    "id": 1740887684118,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nA square matrix $A$ is <b>nonsingular</b> if it is invertible, meaning there exists a matrix $B$ such that:\n$$\nAB = BA = I\n$$\nEquivalently, $A$ is nonsingular if:\n$$\n\\det(A) \\neq 0\n$$\n</p>\n<br>\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Confirm that $A$ is a square matrix.</li>\n  <li>Compute $\\det(A)$. If $\\det(A) \\neq 0$, then $A$ is nonsingular.</li>\n  <li>If nonsingular, an inverse $A^{-1}$ exists and can be computed.</li>\n</ol>",
    "front": "<b>Nonsingular Matrix</b> \n<ul>\n  <li><b>Define:</b> What is a nonsingular matrix?</li>\n  <li><b>Practical Use:</b> Why is nonsingularity important?</li>\n  <li><b>Procedure:</b> How do we verify if a matrix is nonsingular?</li>\n</ul>",
    "id": 1741074715967,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nA square matrix $Q$ is called <b>orthogonal</b> if its columns (and rows) form an orthonormal set. This is equivalent to:\n$$\nQ^T Q = QQ^T = I\n$$\nwhere $I$ is the identity matrix.\n</p>\n<br>\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Check that $Q$ is a square matrix.</li>\n  <li>Compute $Q^T Q$. If $Q^T Q = I$, then $Q$ is orthogonal.</li>\n  <li>Alternatively, verify that the columns of $Q$ are orthonormal (i.e., each column has unit length and is orthogonal to the others).</li>\n</ol>",
    "front": "<b>Orthogonal Matrix</b> \n<ul>\n  <li><b>Define:</b> What is an orthogonal matrix?</li>\n  <li><b>Practical Use:</b> Where are orthogonal matrices used?</li>\n  <li><b>Procedure:</b> How do we check if a matrix is orthogonal?</li>\n</ul>",
    "id": 1740887687921,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nA <b>Householder matrix</b> $H$ is a symmetric and orthogonal matrix used to perform reflections. It is typically defined as:\n$$\nH = I - 2\\frac{vv^T}{v^T v}\n$$\nwhere $v$ is a nonzero vector in $\\mathbb{R}^n$.\n</p>\n<br>\n\n<u><b>Practical Use</b></u> <br>\n<p>\nHouseholder matrices are primarily used in numerical linear algebra, especially for QR factorization and for reducing matrices to tridiagonal or Hessenberg forms. TODO: add pset + lecture 02/11 02/13 stuff stuff\n</p>\n<br>\n\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Select a nonzero vector $v \\in \\mathbb{R}^n$.</li>\n  <li>Compute the Householder matrix using:\n      $$\n      H = I - 2\\frac{vv^T}{v^T v}\n      $$\n  </li>\n  <li>Use $H$ to reflect a given vector or to zero out subdiagonal elements during matrix factorization.</li>\n</ol>",
    "front": "<b>Householder Matrix</b> \n<ul>\n  <li><b>Define:</b> What is a Householder matrix?</li>\n  <li><b>Practical Use:</b> What are Householder matrices used for?</li>\n  <li><b>Procedure:</b> How is a Householder matrix constructed?</li>\n</ul>",
    "id": 1741075039903,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/five_factorizations.jpeg\"></div>",
    "front": "The five factorizations of a matrix.",
    "id": 1740884244261,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "id": 1741168096548,
    "front": "Orthonormality Condition",
    "back": "TODO: New back content",
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Procedure for QR factorization",
    "id": 1740890854848,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "id": 1741167809635,
    "front": "<b>CR Factorization</b>  \n<ul>  \n  <li><b>Define:</b> What is CR factorization?</li>  \n  <li><b>Applications:</b> Where is CR factorization used?</li>  \n  <li><b>Procedure:</b> How is CR factorization performed?</li>  \n</ul>  ",
    "back": "<b><u>Definition</u></b> <br>  \nCR factorization is a matrix decomposition technique where a given matrix \\( A \\) is factored into the product of two matrices:  <br>\n$$  \nA = CR\n$$  \n<br>\nwhere \\( C \\) is a matrix with orthonormal columns (often computed via Gram-Schmidt or QR-like processes), and \\( R \\) is an upper triangular matrix.\n<br><br>  \n\n<b><u>Applications</u></b> <br>  \n<ul>  \n  <li>Used in numerical linear algebra for solving least squares problems.</li>  \n  <li>Helps in reducing computational complexity in iterative methods.</li>  \n  <li>Applied in signal processing and machine learning for matrix approximations.</li>  \n</ul>  \n<br>  \n\n<b><u>Procedure</u></b> <br>  \n<ol>  \n  <li>Start with a given matrix \\( A \\) (typically an \\( m \\times n \\) matrix).</li>  \n  <li>Compute the matrix \\( C \\) whose columns form an orthonormal basis for the column space of \\( A \\).</li>  \n  <li>Compute the matrix \\( R \\) as the product \\( C^T A \\), which results in an upper triangular matrix.</li>  \n  <li>The decomposition satisfies \\( A = CR \\), where \\( C^T C = I \\) (orthonormality condition).</li>  \n</ol>  ",
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "id": 1741168186959,
    "front": "<b>Factor matrix \\( A \\) into \\( CR \\)</b> <br>  \n\\[\nA =\n\\begin{bmatrix}\n1 & 2 \\\\\n0 & 3\n\\end{bmatrix}\n\\]",
    "back": "<b><u>CR Factorization Result</u></b> <br>\nFor the matrix: <br>\n\\[\nA =\n\\begin{bmatrix}\n1 & 2 \\\\\n0 & 3\n\\end{bmatrix},\n\\] <br>\nwe can choose: <br>\n\\[\nC =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} \\quad \\text{(the identity matrix, which is orthonormal)},\n\\]\nand\n\\[\nR =\n\\begin{bmatrix}\n1 & 2 \\\\\n0 & 3\n\\end{bmatrix} \\quad \\text{(an upper triangular matrix with integer elements)}.\n\\] <br><br>\nThus, \\(A = CR\\).",
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "id": 1741165893792,
    "front": "<b>Permutation Matrix</b>",
    "back": "TODO: New back content",
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<b><u>Definition</u></b> <br>  \nLU decomposition is the factorization of a square matrix \\( A \\) into the product of a lower triangular matrix \\( L \\) and an upper triangular matrix \\( U \\), such that:  <br>\n\\[\nA = LU.\n\\] \n\n<br><br>  \n\n<b><u>Example</u></b> <br>  \nFor a \\( 3 \\times 3 \\) matrix:  <br>\n\\[\nA =\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\\]\n<br>\nLU decomposition gives:  <br>\n\\[\nL =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\nl_{21} & 1 & 0 \\\\\nl_{31} & l_{32} & 1\n\\end{bmatrix},\n\\quad\nU =\n\\begin{bmatrix}\nu_{11} & u_{12} & u_{13} \\\\\n0 & u_{22} & u_{23} \\\\\n0 & 0 & u_{33}\n\\end{bmatrix}\n\\]  \n\n<br><br>\n\n<b><u>Applications</u></b> <br>  \n<ol>  \n  <li>Solving linear systems efficiently using forward and backward substitution.</li>  \n  <li>Computing matrix determinants as \\( \\det(A) = \\det(L) \\det(U) \\).</li>  \n  <li>Matrix inversion by solving multiple systems efficiently.</li>  \n  <li>Numerical methods, such as optimization and differential equations.</li>  \n</ol>  \n\n<br>\n\n<b><u>Procedure</u></b> <br>  \nLU decomposition is performed through Gaussian elimination:  \n<ol>  \n  <li>Convert matrix \\( A \\) into an upper triangular matrix \\( U \\) using row operations.</li>  \n  <li>Keep track of the multipliers used in each step to form the lower triangular matrix \\( L \\).</li>  \n  <li>If partial pivoting is required, an additional permutation matrix \\( P \\) may be introduced, leading to \\( PA = LU \\).</li>  \n</ol>  \n\n<br>  \n<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/Screenshot_2025-03-02_at_1.34.36_AM.png\"></div> ",
    "front": "<b>LU Decomposition</b>  \n<ul>  \n  <li><b>Define:</b> What is LU decomposition?</li>  \n  <li><b>Applications:</b> Where is LU decomposition used?</li>  \n  <li><b>Procedure:</b> How is LU decomposition performed?</li>  \n</ul>  ",
    "id": 1740890882881,
    "tags": [
      "18.065"
    ],
    "understanding": 3
  },
  {
    "id": 1741164002995,
    "front": "<b>Factor matrix \\( A \\) into \\( LU \\)</b> <br>  \n\\[\nA =\n\\begin{bmatrix}\n4 & 3 \\\\\n6 & 3\n\\end{bmatrix}\n\\]\n",
    "back": "<b><u>LU Decomposition Result</u></b> <br>\nFor the matrix: <br>\n\\[\nA =\n\\begin{bmatrix}\n4 & 3 \\\\\n6 & 3\n\\end{bmatrix},\n\\] <br>\nthe LU decomposition (without permutation) is: <br>\n\\[\nL =\n\\begin{bmatrix}\n1 & 0 \\\\\n\\frac{3}{2} & 1\n\\end{bmatrix},\n\\quad\nU =\n\\begin{bmatrix}\n4 & 3 \\\\\n0 & -\\frac{3}{2}\n\\end{bmatrix}.\n\\] <br><br>",
    "tags": [
      "18.065"
    ],
    "understanding": 1,
    "attempts": [
      {
        "timestamp": 1741165546433,
        "correct": true
      }
    ]
  },
  {
    "id": 1741164054782,
    "front": "<b>Factor matrix \\( A \\) into \\( LU \\)</b> <br>  \n\\[\nA =\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 10\n\\end{bmatrix}\n\\]",
    "back": "<b><u>LU Decomposition Result</u></b> <br>\nFor the matrix: <br>\n\\[\nA =\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 10\n\\end{bmatrix},\n\\] <br>\nthe LU decomposition (without permutation) is: <br>\n\\[\nL =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n4 & 1 & 0 \\\\\n7 & 2 & 1\n\\end{bmatrix},\n\\quad\nU =\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & 0 & 1\n\\end{bmatrix}.\n\\] <br><br>",
    "tags": [
      "18.065"
    ],
    "understanding": 1,
    "attempts": [
      {
        "timestamp": 1741165545220,
        "correct": true
      }
    ]
  },
  {
    "id": 1741165926652,
    "front": "<b>Factor matrix \\( A \\) into \\( LU \\)</b> <br>  \n\\[\nA = \\begin{bmatrix}\n1 & 3 & 2 \\\\\n2 & 6 & 5 \\\\\n3 & 8 & 6\n\\end{bmatrix}\n\\]",
    "back": "<b><u>LU Decomposition Result</u></b> <br>\nFor the given matrix, a permutation is required during Gaussian elimination. We first form a permutation matrix P that swaps rows 2 and 3 to avoid a zero pivot in the second elimination step. <br>\n\nPermutation Matrix:  <br>\n\\[\nP =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix}\n\\]\n\n<br>\nTransformed Matrix: <br>\n\\[\nPA =\n\\begin{bmatrix}\n1 & 3 & 2 \\\\\n3 & 8 & 6 \\\\\n2 & 6 & 5\n\\end{bmatrix}\n\\]\n\n<br>\nThe LU decomposition of PA (i.e., PA = LU) is: <br>\n\\[\nL =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n3 & 1 & 0 \\\\\n2 & 0 & 1\n\\end{bmatrix},\n\\quad\nU =\n\\begin{bmatrix}\n1 & 3 & 2 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n \\]",
    "tags": [
      "18.065"
    ],
    "understanding": 1,
    "attempts": [
      {
        "timestamp": 1741166780847,
        "correct": true
      }
    ]
  },
  {
    "id": 1741163983569,
    "front": "TODO: New front content",
    "back": "TODO: New back content",
    "tags": [],
    "understanding": 1
  },
  {
    "id": 1741163982143,
    "front": "TODO: New front content",
    "back": "TODO: New back content",
    "tags": [],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Procedure for singular value decomposition",
    "id": 1740890913235,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Prove that for any matrix $A \\in R^{m\u00d7n}$, that $A^{T}A$ and $A$ have\nthe same null space.",
    "id": 1740890964079,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Symmetric matrix",
    "id": 1740890063445,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Propose a function $x = linear\\_solver(U, b)$, where $U \\in R^{n \\times n}$ is an upper triangular matrix, $b \\in R^n$ is a vector, and $x \\in R^n$ is the solution to $Ux = b$. <br><br>\n\nThen apply your procedure to",
    "id": 1740891657888,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Deflation via Householder Transformation",
    "id": 1741078759558,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741072483001
      }
    ],
    "back": "<b><u>Definition</u></b> <br>\n<p>\nTwo square matrices $A$ and $B$ that are related by <br>\n$$\nB = X^{-1}AX\n$$\n<br>\nwhere $X$ is a square nonsingular matrix are said to be similar.\n</p>\n<br>\n<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/IMG_3E1D74CC9C45-1.jpeg\"></div> <br>\n\n<b><u>Properties</u></b> <br>\n<ul>\n  <li>\n    If $(\\lambda, v) \\text{ is an eigenpair of } B, \\text{ then } (\\lambda, Xv) \\text{ is an eigenpair of } A$.  \n    <ul>\n      <li><u>Proof</u> <br>\n        If \\( Bv = \\lambda v \\), then  \n        <br>\n        $$\n        A(Xv) = X B X^{-1} X v = X B v = \\lambda (Xv)\n        $$\n      </li>\n    </ul>\n  </li>\n</ul>\n\n<br><br>\n<ul>\n  <li>Identical characteristic polynomials</li>\n  <li>Equal determinants, traces, and ranks</li>\n  <li>The same minimal polynomial and Jordan canonical form</li>\n  <li>Consistent diagonalizability properties</li>\n</ul>",
    "front": "<b>Similar matrix</b> <br><br>\nDefine <br><br>\nProperties",
    "id": 1740892885417,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741074086548
      }
    ],
    "back": "<b><u>Definition</u></b> <br>\n<p>\nDiagonalization is the process of finding a diagonal matrix $D$ such that a square matrix $A$ is similar to $D$, meaning:  <br>\n$$\nA = X D X^{-1}\n$$\n</p>\n\nwhere:\n<ul>\n  <li>$D$ is a diagonal matrix whose entries are the eigenvalues of $A$.</li>\n  <li>$X$ is the matrix whose columns are the eigenvectors of $A$.</li>\n  <li>$A$ is diagonalizable if it has $n$ linearly independent eigenvectors.</li>\n</ul>\n\n<br>\n\n<b><u>Practical Use</u></b> <br>\n<ul>\n  <li><b>Computing Matrix Powers Efficiently:</b> If we need $A^k$, we can compute it as:\n    $$\n    A^k = X D^k X^{-1}\n    $$\n    where raising $D$ to a power is simple since it is diagonal.\n  </li>\n</ul>\n\n<br>\n\n<b><u>Procedure</u></b> <br>\n<ol>\n  <li><b>Find Eigenvalues:</b> Solve the characteristic equation:\n    $$\n    \\det(A - \\lambda I) = 0\n    $$\n    to find the eigenvalues $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$.\n  </li>\n  <li><b>Find Eigenvectors:</b> For each eigenvalue $\\lambda_i$, solve:\n    $$\n    (A - \\lambda_i I)v_i = 0\n    $$\n    to find the corresponding eigenvector $v_i$.\n  </li>\n  <li><b>Construct Matrices:</b>\n    <ul>\n      <li>$X$ is the matrix whose columns are the eigenvectors $v_1, v_2, \\dots, v_n$.</li>\n      <li>$D$ is the diagonal matrix with eigenvalues $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$ on the diagonal.</li>\n    </ul>\n  </li>\n  <li><b>Compute $X^{-1}$:</b> Since $X$ is invertible (if $A$ is diagonalizable), compute its inverse $X^{-1}$.</li>\n  <li><b>Verify the Similarity Relation:</b> Ensure that:\n    $$\n    A = X D X^{-1}\n    $$\n  </li>\n</ol>",
    "front": "<b>Diagonalization</b> \n<ul>\n  <li><b>Define:</b> What is diagonalization?</li>\n  <li><b>Practical Use:</b> Why is diagonalization useful?</li>\n  <li><b>Procedure:</b> How do we diagonalize a matrix?</li>\n</ul>",
    "id": 1741066673709,
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "id": 1741166998328,
    "front": "<b>The Power Method</b>\n<ul>\n    <li><b>Define:</b> What is the power method?</li>\n    <li><b>Procedure:</b> How does the power method algorithm approximate the dominant eigenvalue and its corresponding eigenvector?</li>\n</ul>",
    "back": "<b><u>Definition</u></b> <br>\nThe power method is an iterative algorithm used to approximate the dominant eigenvalue (the eigenvalue with the greatest absolute value) and its corresponding eigenvector of a matrix. <br><br>\n\n<b><u>Procedure</u></b> <br>\n1. Choose an initial nonzero vector \\( x_0 \\). <br>\n2. For \\( k = 0, 1, 2, \\dots \\), compute \\( x_{k+1} = A x_k \\). <br>\n3. Normalize \\( x_{k+1} \\) to avoid numerical overflow. <br>\n4. Repeat until convergence; \\( x_k \\) approaches the eigenvector associated with the dominant eigenvalue. <br>\n5. Estimate the dominant eigenvalue by \\( \\lambda \\approx \\frac{x_k^T A x_k}{x_k^T x_k} \\).",
    "tags": [
      "18.065"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "The Taylor series for a vector-valued function \\( F(x) \\) expands it around \\( x \\): <br><br>\n$$\nF(x + \\Delta x) \\approx F(x) + g(x)^T \\Delta x + \\frac{1}{2} \\Delta x^T H(x) \\Delta x + \\dots\n$$\n<br><br>\n\nwhere: <br>\n\n- \\( g(x) \\) is the gradient of \\( F \\): <br><br>\n$$\ng = \\begin{bmatrix} \\frac{\\partial F}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial F}{\\partial x_n} \\end{bmatrix}.\n$$\n\n<br><br>\n- \\( H(x) \\) is the Hessian matrix of second derivatives: <br><br>\n$$\nH =\n\\begin{bmatrix}\n\\frac{\\partial^2 F}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 F}{\\partial x_1 \\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 F}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 F}{\\partial x_n^2}\n\\end{bmatrix}.\n$$\n<br><br>\nTruncating the series gives a quadratic approximation of \\( F(x) \\) around \\( x \\).",
    "front": "Taylor series for a vector-valued function \\( F(x) \\)",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "Positive definiteness of a real $n \\times n$ symmetric matrix $A$ is defined as <br><br>\n$$\nx^T A x > 0, \\quad \\forall x \\neq 0, \\; x \\in \\mathbb{R}^n\n$$",
    "front": "Positive definite (PD) <br><br>\nDefine<br><br>\nHow to test for it",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "meeting some necessary conditions does not mean the thing is true but its required that the condition be true when the thing is true. <br><br>\n\nsufficient condition when meant means the thing of interest is true.",
    "front": "Necessary vs. sufficient conditions",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "A positive definite (PD) Hessian guarantees a local minimum, while a positive semi-definite (PSD) Hessian is only necessary\u2014it does not confirm a minimum on its own.<br><br>\n\n\\[\n\\begin{array}{|c|c|c|c|}\n\\hline\n\\textbf{Condition} & \\textbf{Hessian Type} & \\textbf{What It Means} & \\textbf{Conclusion} \\\\\n\\hline\n\\text{Necessary} & \\text{Positive Semi-Definite (PSD)} & \\text{No negative curvature, but could be flat in some directions} & \\text{Local min possible, but not guaranteed} \\\\\n\\hline\n\\text{Sufficient} & \\text{Positive Definite (PD)} & \\text{Strictly upward curvature in all directions} & \\text{Guarantees a local min} \\\\\n\\hline\n\\end{array}\n\\]",
    "front": "Hessian necessary and sufficient conditions for a local minimum.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "$$\ng(x^*) = 0\n$$\n<br><br>\ngradient is zero.",
    "front": "Stationary point necessary condition.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "If $f: \\mathbb{R}^n \\to \\mathbb{R}$ is a differentiable function, the gradient of $f(x)$ is defined as:\n<br><br>\n\n$$\n\\nabla f(x) = \n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\\n\\frac{\\partial f}{\\partial x_2} \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial x_n}\n\\end{bmatrix}.\n$$\n\n<br><br>\nEach component $\\frac{\\partial f}{\\partial x_i}$ represents the partial derivative of $f$ with respect to $x_i$.\n<br><br>\n\nThe gradient is a vector that points in the direction of the steepest increase of $f(x)$.",
    "front": "<b>Gradient of a Differentiable Vector Function</b> \n<ul>\n  <li><b>Define:</b> What is the gradient of a differentiable vector function?</li>\n  <li><b>Procedure:</b> How is the gradient of a vector function computed and interpreted geometrically?</li>\n</ul>",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "A functional is a mapping from a space of functions to the real numbers. <br><br>\n\nMathematically, a functional $J$ takes a function $y(x)$ as input and returns a real number:\n<br><br>\n\n$$\nJ[y] = \\int_{a}^{b} F(x, y, y') \\, dx.\n$$\n\n<br><br>\nFunctionals are commonly used in calculus of variations and physics to express quantities such as energy or action.",
    "front": "What is a <b>functional</b>?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "The first variation of a functional is the first-order term in the Taylor expansion of the increment, providing a linear approximation of the change in the functional. <br><br>\n\nThe first variation of a functional $J[x(t)]$ with respect to a perturbation $\\delta x(t)$ is defined as:\n<br><br>\n\n$$\n\\delta J(x(t), \\delta x(t)) = \\left. \\frac{d}{d\\epsilon} J(x(t) + \\epsilon \\delta x(t)) \\right|_{\\epsilon=0}.\n$$\n<br><br>\n\n<b>TODO: fix beyond here to make more sense</b>\n<br><br>\n\nFor a functional $J[y]$, we express its perturbation as:\n<br><br>\n\n$$\nJ[y + \\epsilon \\delta y] = \\int_a^b F(x, y + \\epsilon \\delta y, y' + \\epsilon \\delta y') \\, dx.\n$$\n<br><br>\n\nExpanding $F( \\cdot )$ in a Taylor series around $\\epsilon = 0$ gives:\n<br><br>\n$$\nF(x, y + \\epsilon \\delta y, y' + \\epsilon \\delta y') = F(x, y, y') + \\epsilon \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) + O(\\epsilon^2).\n$$\n<br>\n\nSubstituting this into the integral:\n<br><br>\n$$\nJ[y + \\epsilon \\delta y] = J[y] + \\epsilon \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx + O(\\epsilon^2).\n$$\n<br><br>\n\nTaking the limit as $\\epsilon \\to 0$, we obtain the first variation:\n<br><br>\n$$\n\\delta J = \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx.\n$$\n<br>",
    "front": "<b>Increment in Calculus of Variations</b> \n<ul>\n  <li><b>Define:</b> What is the increment of a functional?</li>\n  <li><b>Procedure:</b> How is the increment of a functional expressed and used when deriving the first variation?</li>\n</ul>",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "The first variation of functional is a linear approximation of the increment, i.e., first-order Taylor expansion of the increment.<br><br>\n\nThe **first variation** of a functional $J[x(t)]$ with respect to a perturbation $\\delta x(t)$ is defined as:\n<br><br>\n\n$$\n\\delta J(x(t), \\delta x(t)) = \\left. \\frac{d}{d\\epsilon} J(x(t) + \\epsilon \\delta x(t)) \\right|_{\\epsilon=0}.\n$$\n<br><br>\n\nFor a functional $J[y]$ and a small perturbation $\\delta y$, consider:\n<br><br>\n\n$$\nJ[y + \\epsilon \\delta y] = \\int_a^b F(x, y + \\epsilon \\delta y, y' + \\epsilon \\delta y') \\, dx.\n$$\n<br><br>\n\nExpanding the integrand $F( \\cdot )$ in a Taylor series around $\\epsilon = 0$, we obtain:\n<br><br>\n$$\nF(\\cdot) = F(x, y, y') + \\epsilon \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) + O(\\epsilon^2).\n$$\n<br><br>\n\nSubstituting this expansion back into the integral gives:\n<br><br>\n$$\nJ[y + \\epsilon \\delta y] = \\int_a^b \\Bigg[ F(x, y, y') + \\epsilon \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) + O(\\epsilon^2) \\Bigg] dx.\n$$\n<br>\n\nThis can be rewritten as:\n<br><br>\n$$\nJ[y + \\epsilon \\delta y] = J[y] + \\epsilon \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx + O(\\epsilon^2).\n$$\n<br><br>\n\nThe first variation $\\delta J$ is defined as the first-order term:\n<br><br>\n$$\n\\delta J = \\lim_{\\epsilon \\to 0} \\frac{J[y + \\epsilon \\delta y] - J[y]}{\\epsilon} = \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx.\n$$\n<br>",
    "front": "<b>First Variation</b> \n<ul>\n  <li><b>Define:</b> What is the first variation in the context of calculus of variations?</li>\n  <li><b>Procedure:</b> How is the first variation computed and used to determine extremals of functionals?</li>\n</ul>",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "The Euler-Lagrange equation states that for a function $g(x, \\dot{x}, t)$:\n<br><br>\n\n$$\n\\frac{d}{dt} \\left( \\frac{\\partial g}{\\partial \\dot{x}} \\right) - \\frac{\\partial g}{\\partial x} = 0\n\n\\quad \\quad \\quad \\quad \\text{or} \\quad \\quad \\quad \\quad\n\n\\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{x}} \\right) - \\frac{\\partial L}{\\partial x} = 0\n$$\n\n<br><br>\nRewriting this:\n\n<br><br>\n$$\n\\frac{d}{dt} (g_{\\dot{x}}) - g_x = 0\n\n\\quad \\quad \\quad \\quad \\text{or} \\quad \\quad \\quad \\quad\n\n\\frac{d}{dt} (L_{\\dot{x}}) - L_x = 0\n$$\n\n<br><br>\nwhich is the fundamental condition for extremizing a functional in the calculus of variations.",
    "front": "<b>Euler-Lagrange Equation</b> \n<ul>\n  <li><b>Define:</b> What is the Euler-Lagrange equation?</li>\n  <li><b>Procedure:</b> How is the Euler-Lagrange equation derived and applied in optimization problems?</li>\n</ul>",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "$$\nH(x, u, \\lambda, t) = f(x, u, t) \\cdot \\lambda + L(x, u, t).\n$$\n<br><br>\nwhere: <br>\n- x  is the state variable. <br>\n- u  is the control variable. <br>\n- \\lambda  is the costate (adjoint) variable (analogous to momentum in classical mechanics). <br>\n- L(x, u, t)  is the running cost.<br>\n- f(x, u, t)  describes the system dynamics.\n\n<br><br><br><br>\n$$\nH = \\dot{x} \\frac{\\partial L}{\\partial \\dot{x}} - L \n  \\quad \\text{or} \\quad \nH = \\dot{x} \\frac{\\partial g}{\\partial \\dot{x}} - g\n$$\n<br><br>\nRewriting as:<br><br>\n$$\nH = \\dot{x} L_{\\dot{x}} - L \n  \\quad \\text{or} \\quad \nH = \\dot{x} g_{\\dot{x}} - g\n$$\n\n<br><br>\n\nNote that:<br><br>\n$$\ng_t + \\frac{dH}{dt} = 0.\n$$\n\n<br><br>\nThe crucial property of the Hamiltonian is that:<br>\n- If  $L$  does not explicitly depend on time (i.e.,  $L_t = 0$ ), then  $H$  is conserved.<br>\n- This leads directly to the Beltrami identity, which states that:\n$$\n\\dot{x} g_{\\dot{x}} - g = \\text{constant}.\n$$",
    "front": "Hamiltonian",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "$$\n\\min_{x \\in \\mathbb{R}^{n}}{f(x)}\n$$\n<br><br>\nsubject to: <br>\n$$\ng_{i}(x) \\leq 0, \\quad i \\in \\mathcal{I}, \\quad \\mathcal{I} = 1, \\dots, m \\quad \\text{(inequality constraints)}\n$$\n<br>\n$$\nh_{i}(x) = 0, \\quad j \\in \\mathcal{E}, \\quad \\mathcal{E} = 1, \\dots, p \\quad \\; \\text{(equality constraints)}\n$$\n<br><br>\nwhere:\n<ul>\n  <li>$$f(x)$$ is the objective function to be minimized.</li>\n  <li>$$g_i(x) \\leq 0$$ represents inequality constraints that must be satisfied.</li>\n  <li>$$h_j(x) = 0$$ represents equality constraints that must be exactly satisfied.</li>\n  <li>$$x \\in \\mathbb{R}^n$$ is the decision variable (a vector in \\( n \\)-dimensional space).</li>\n</ul>",
    "front": "Standard form of a constrained optimization problem.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "<ol>\n\n  <li>Stationarity <br>\n    $$\n    \\nabla f(x^*) + \\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*) + \\sum_{j=1}^{p} \\mu_j \\nabla h_j(x^*) = 0\n    $$\n  </li>\n  <br>\n\n  <li>Primal feasibility <br>\n    $$\n    g_i(x^*) \\leq 0, \\quad \\forall i \\in \\{1, \\dots, m\\}\n    $$\n    <br>\n    $$\n    h_j(x^*) = 0, \\quad \\forall j \\in \\{1, \\dots, p\\}\n    $$\n  </li>\n  <br>\n\n  <li>Dual feasibility <br>\n    $$\n    \\lambda_i \\geq 0, \\quad \\forall i \\in \\{1, \\dots, m\\}\n    $$\n  </li>\n  <br>\n\n  <li>Complementary slackness <br>\n    $$\n    \\lambda_i g_i(x^*) = 0, \\quad \\forall i \\in \\{1, \\dots, m\\}\n    $$\n  </li>\n  <br>\n\n</ol>",
    "front": "KKT Conditions.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "When the gradients of the active constraints are linearly dependent\u2014that is, when the Linear Independence Constraint Qualification (LICQ) fails. In this case, the constraint gradients do not span enough directions to cancel out the gradient of the objective, meaning the stationarity condition cannot be met.",
    "front": "Under what circumstances can we fail to choose Lagrange multipliers that satisfy the KKT conditions without contradiction?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "At an optimal point $x^*$, the idea is that no small, feasible perturbation $d$ should be able to decrease the objective function. In other words, for every feasible direction $d$, we must have <br><br>\n\n$$\n\\nabla f(x^{*})^T d \\geq 0.\n$$\n\n<br><br>\nNow, when a constraint $g_i(x) \\leq 0$ is active (i.e. $g_i(x^*) = 0)$, any small movement d that keeps us feasible must satisfy <br><br>\n$$\n\\nabla g_i(x^{*})^T d \\leq 0.\n$$\n<br><br>\nThis means the feasible directions are limited by the gradients of the active constraints. For there to be no feasible descent direction\u2014that is, for the objective not to decrease along any $d$\u2014the entire gradient $\\nabla f(x^*)$ must be \u201cneutralized\u201d by the constraint forces. This requirement is captured by the stationarity condition: <br><br>\n$$\n\\nabla f(x^*) + \\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*) = 0.\n$$\n<br><br>\nHere, the multipliers $\\lambda_i$ adjust the contribution of each constraint\u2019s gradient so that they together cancel out $\\nabla f(x^*)$. This ensures that any movement allowed by the constraints does not provide a descent direction, preserving the optimality of $x^*$.\n<br>",
    "front": "Why must the stationarity condition hold at an optimal solution in constrained optimization?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "At an optimal point $x^*$, the gradients of the active constraints, $\\nabla g_i(x^*)$ for all $i$ such that $g_i(x^*) = 0$, define the normal (or \u201cblocking\u201d) directions at $x^*$. They essentially tell us which directions are forbidden for movement if we want to remain feasible. <br><br>\n\nFor $x^*$ to be optimal, the gradient $\\nabla f(x^*)$ must not point in any direction that would allow us to decrease $f(x)$ while staying within the feasible region. This is only possible if $\\nabla f(x^*)$ lies entirely in the span of the constraint gradients. In mathematical terms, this means that there exist multipliers $\\lambda_i$ such that <br><br>\n\n$$\n\\nabla f(x^*) = -\\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*).\n$$\n\n<br><br>\nIf any component of $\\nabla f(x^*)$ were to lie outside of the span of the $\\nabla g_i(x^*)$, then there would be a feasible direction in which $f(x)$ could decrease, contradicting the optimality of $x^*$. <br><br>\n\nThus, the condition guarantees that all possible descent directions are \u201cblocked\u201d by the active constraints, ensuring that $x^*$ is indeed optimal.",
    "front": "What is the geometric intuition behind the KKT stationarity condition?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "The LICQ condition states that at a feasible point $x^*$, the gradients of all active constraints must be linearly independent. <br><br>\n\nMathematically, if the set of active constraints at $x^*$ is given by:\n<br><br>\n$$\n\\mathcal{A} = \\{ i \\mid g_i(x^*) = 0 \\},\n$$\n<br><br>\nthen the gradients of these constraints,\n<br><br>\n$$\n\\{ \\nabla g_i(x^*) \\}_{i \\in \\mathcal{A}},\n$$\n<br><br>\nmust be linearly independent. This means that if there exists a set of scalars $\\lambda_i$ such that:\n<br><br>\n$$\n\\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*) = 0,\n$$\n<br><br>\nthen it must follow that $\\lambda_i = 0$ for all $i \\in \\mathcal{A}$. <br><br>\n\nIf LICQ holds, then the active constraint gradients span a well-defined space, ensuring that the KKT conditions hold without contradiction. <br><br>\n\nIf LICQ fails, the stationarity condition may not be satisfied, meaning no valid Lagrange multipliers exist.",
    "front": "Linear Independence Constraint Qualification (LICQ) condition.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO",
    "front": "Collocation points",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Interpolation points.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "General way to express most optimal control problems: <br><br>\n\n\\[\n\\begin{array}{ll}\n\\textbf{Minimize} & J = \\phi\\Bigl(x(t_0),\\, x(t_f),\\, t_0,\\, t_f,\\, q,\\, s\\Bigr) \\\\\n\\\\\n\\textbf{with respect to} & x(t),\\; u(t),\\; t_0,\\; t_f,\\; s,\\; q \\\\\n\\\\\n\\textbf{subject to} & \\dot{x}(t) = f\\Bigl(x(t),\\, u(t),\\, t,\\, s\\Bigr), \\quad t \\in [t_0, t_f], \\\\\n\\\\\n& q = \\int_{t_0}^{t_f} g\\Bigl(x(t),\\, u(t),\\, t,\\, s\\Bigr) dt, \\\\\n\\\\\n& c\\Bigl(x(t),\\, u(t),\\, t,\\, s\\Bigr) \\le 0, \\quad t \\in [t_0, t_f], \\\\\n\\\\\n& \\psi\\Bigl(x(t_0),\\, x(t_f),\\, t_0,\\, t_f,\\, s\\Bigr) = 0, \\\\\n\\\\\n& \\text{and appropriate bounds on } x(t),\\, u(t),\\, s,\\, t_0,\\, t_f.\n\\end{array}\n\\]",
    "front": "Bolza optimization problem formulation.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content\n\n<br><br>\nThe following functions are transcendental:<br>\n<img src=\"/assets/Screenshot_2025-02-28_at_1.31.02_AM.png\">",
    "front": "Transcendental equation",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content <br><br>\n\nWhat is it and what is the equation for it?",
    "front": "Catenary curve.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "attempts": [],
    "back": "TODO: New back content <br><br>\n\nWhat is it and what is the equation for it?",
    "front": "Catenoid.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  }
]