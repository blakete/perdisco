[
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1745595072258
      }
    ],
    "back": "TODO: New back content",
    "front": "How to factor a cube polynomial e.g. characteristic polynomial for 3x3 matrix for eigenvalues.",
    "id": 1742438698251,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": false,
        "reason": "i did this wrong",
        "timestamp": 1745595076988
      }
    ],
    "back": "<p>\nTaylor series is the infinite series:<br>\n$$\nf(x) = \\sum_{n=0}^{\\inf}{\\frac{f^{(n)}(a)}{n!}(x - a)^{n}}\n$$\n</p>\n<br>\n\n<p>\nTaylor series \"expansion\" or \"Taylor expansion\" of a function typically implies truncation, meaning we approximate the function $f(x)$ by a finite number of terms. <br>\n$$\nf(x) \\approx f(a) + f^{\\prime}(a)(x-a) + \\dots + \\frac{f^{(N)}(a)}{N!} (x-a)^N.\n$$\n</p>\n<br>",
    "front": "<b>Taylor Series vs. Taylor Expansion</b>  \n<ul>  \n  <li><b>Define:</b> What is the Taylor series of \\( f(x) \\) at \\( x = a \\)?</li>  \n  <li><b>Define:</b> What is the Taylor series expansion of \\( f(x) \\) at/around \\( x = a \\)?</li>  \n  <li><b>Difference:</b> How do the Taylor series and Taylor expansion differ in meaning and usage?</li>  \n</ul>  ",
    "id": 1742426830175,
    "importance": 2,
    "tags": [
      "18.065",
      "midterm1",
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>A:</b></u><br>\n<p>\nA function depends explicitly on a variable if that variable appears directly in its formula (e.g., \\( \\sin(t) \\) in \\( f(x,t) = x^2 + \\sin(t) \\)). It depends implicitly on a variable if that variable affects the function only through another variable (e.g., if \\( x = x(t) \\), then \\( g(x) = x^2 \\) implicitly depends on \\( t \\)).\n</p>",
    "front": "<b>Explicit vs. Implicit Dependence</b>\n<ul>  \n    <li><b>Q:</b> What is the difference between explicit and implicit dependence in a function?</li>\n</ul>",
    "id": 1742437463232,
    "tags": [
      "16.32",
      "18.065",
      "Multivariable Calculus"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>A:</b></u><br>\n<p>\nWrite it as \n\\( L(x,u,t) = x^2 + u^2 + t x + t^2 u + P(t) + 2 \\rho(t) \\), \nwhere \\( x \\) and \\( u \\) are treated as independent variables (thus their dependence on \\( t \\) is implicit) and \\( t \\) appears explicitly.\n</p>",
    "front": "<b>Rewriting a Cost Functional</b>\n<ul>  \n    <li><b>Q:</b> How can we formally rewrite \n    \\( L[x(t), u(t), t] = x(t)^2 + u(t)^2 + t x(t) + t^2 u(t) + P(t) + 2 \\rho(t) \\) \n    to show explicit versus implicit dependencies?</li>\n</ul>",
    "id": 1742437570738,
    "tags": [
      "16.32",
      "18.065",
      "Multivariable Calculus"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>A:</b></u><br>\n<p>\nA partial derivative differentiates only the explicit dependence on a variable while treating other variables as constant. A total derivative accounts for both explicit and implicit dependence, using the chain rule. For example, for \\( V(x,t) \\) with \\( x = x(t) \\):\n$$\n\\frac{dV}{dt} = \\frac{\\partial V}{\\partial t} + \\frac{\\partial V}{\\partial x} \\frac{dx}{dt}.\n$$\n</p>",
    "front": "<b>Partial vs. Total Derivatives</b>\n<ul>  \n    <li><b>Q:</b> What is the difference between a partial derivative and a total derivative?</li>\n</ul>",
    "id": 1742437659254,
    "tags": [
      "16.32",
      "18.065",
      "Multivariable Calculus"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>A:</b></u><br>\n<p>\nThe chain rule combines the derivative of the function with respect to its explicit variable and the derivative of that variable with respect to time. For example, if \\( V(x,t) \\) and \\( x = x(t) \\), then\n$$\n\\frac{dV}{dt} = \\frac{\\partial V}{\\partial t} + \\frac{\\partial V}{\\partial x} \\frac{dx}{dt},\n$$\nwhere \\( \\frac{\\partial V}{\\partial t} \\) captures the explicit \\( t \\)-dependence and \\( \\frac{\\partial V}{\\partial x} \\frac{dx}{dt} \\) captures the implicit \\( t \\)-dependence via \\( x(t) \\).\n</p>",
    "front": "<b>Role of the Chain Rule</b>\n<ul>  \n    <li><b>Q:</b> How does the chain rule capture implicit time dependence in a function?</li>\n</ul>",
    "id": 1742437672894,
    "tags": [
      "16.32",
      "18.065",
      "Multivariable Calculus"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>A:</b></u><br>\n<p>\nBecause the partial derivative \\( \\frac{\\partial V}{\\partial t} \\) is taken by holding \\( x \\) constant, so only the explicit time dependence (through \\( P(t) \\), \\( \\eta(t) \\), and \\( \\rho(t) \\)) is differentiated.\n</p>",
    "front": "<b>Treating Variables as Independent in Partial Derivatives</b>\n<ul>  \n    <li><b>Q:</b> When computing \\( \\frac{\\partial V}{\\partial t} \\) for \n    \\( V(x,t) = x^T P(t)x + 2\\eta(t)^T x + \\rho(t) \\),\n    why is \\( x \\) treated as independent of \\( t \\)?</li>\n</ul>",
    "id": 1742437697135,
    "tags": [
      "16.32",
      "18.065",
      "Multivariable Calculus"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Solution</b></u><br>\n<p>\n$$ \\mathbf{f(x) = a^T x \\quad \\Rightarrow \\quad \\nabla_x f(x) = a.} $$\n</p>",
    "front": "$\\mathbf{f(x) = a^T x \\quad \\Rightarrow \\quad  \\nabla_x f(x) = \\, ?}$",
    "id": 1742435359998,
    "tags": [
      "matrix calculus fundamentals",
      "16.32",
      "18.065"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Solution</b></u><br>\n<p>\n$$ \\mathbf{f(x) = x^T a \\quad \\Rightarrow \\quad \\nabla_x f(x) = a.} $$\n</p>",
    "front": "$\\mathbf{f(x) = x^T a \\quad \\Rightarrow \\quad \\nabla_x f(x) = \\, ?}$",
    "id": 1742435664538,
    "tags": [
      "matrix calculus fundamentals",
      "16.32",
      "18.065"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Solution</b></u><br>\n<p>\n$$ \\mathbf{f(x) = x^T A x \\quad \\Rightarrow \\quad \\nabla_x f(x) = (A + A^T)x.} $$\n</p>\nIf \\( A \\) is symmetric:\n<p>\n$$ \\mathbf{\\nabla_x f(x) = 2Ax.} $$\n</p>",
    "front": "$\\mathbf{f(x) = x^T A x \\quad \\Rightarrow \\quad \\nabla_x f(x) = \\, ?}$",
    "id": 1742435684155,
    "tags": [
      "matrix calculus fundamentals",
      "16.32",
      "18.065"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Solution</b></u><br>\n<p>\n$$ \\mathbf{f(x) = \\|x\\|^2 = x^T x \\quad \\Rightarrow \\quad \\nabla_x f(x) = 2x.} $$\n</p>",
    "front": "$\\mathbf{f(x) = \\|x\\|^2 \\quad \\Rightarrow \\quad \\nabla_x f(x) = \\, ?}$",
    "id": 1742435706550,
    "tags": [
      "matrix calculus fundamentals",
      "16.32",
      "18.065"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Solution</b></u><br>\n<p>\n$$ \\mathbf{f(x, y) = x^T A y \\quad \\Rightarrow \\quad \\nabla_x f(x, y) = Ay.} $$\n</p>",
    "front": "$\\mathbf{f(x, y) = x^T A y \\quad \\Rightarrow \\quad \\nabla_x f(x, y) = \\, ?}$",
    "id": 1742435713675,
    "tags": [
      "matrix calculus fundamentals",
      "16.32",
      "18.065"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Solution</b></u><br>\n<p>\n$$ \\mathbf{f(x, y) = x^T A y \\quad \\Rightarrow \\quad \\nabla_y f(x, y) = A^T x.} $$\n</p>",
    "front": "$\\mathbf{f(x, y) = x^T A y \\quad \\Rightarrow \\quad \\nabla_y f(x, y) = \\, ?}$",
    "id": 1742435724329,
    "tags": [
      "matrix calculus fundamentals",
      "16.32",
      "18.065"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Solution</b></u><br>\n<p>\n$$ \\mathbf{\\operatorname{tr}(ABC) = \\operatorname{tr}(BCA) = \\operatorname{tr}(CAB).} $$\n</p>",
    "front": "$\\mathbf{\\operatorname{tr}(ABC) = \\, ?}$",
    "id": 1742435760681,
    "tags": [
      "matrix calculus fundamentals",
      "16.32",
      "18.065"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Solution</b></u><br>\n<p>\n$$ \\mathbf{\\frac{d}{dx} f(g(x)) = \\frac{\\partial f}{\\partial g} \\frac{dg}{dx}.} $$\n</p>",
    "front": "$\\mathbf{\\frac{d}{dx} f(g(x)) = \\, ?}$",
    "id": 1742435766742,
    "tags": [
      "matrix calculus fundamentals",
      "16.32",
      "18.065"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Solution</b></u><br>\n<p>\n$$ \\mathbf{\\frac{d}{dx} \\left[f(x) g(x)\\right] = \\frac{df(x)}{dx} g(x) + f(x) \\frac{dg(x)}{dx}.} $$\n</p>",
    "front": "$\\mathbf{\\frac{d}{dx} \\left[f(x) g(x)\\right] = \\, ?}$",
    "id": 1742435796643,
    "tags": [
      "matrix calculus fundamentals",
      "16.32",
      "18.065"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Solution</b></u><br>\n<p>\n$$ \\mathbf{f(x) = x^T A x \\quad \\Rightarrow \\quad H = A + A^T.} $$\n</p>\nIf \\( A \\) is symmetric:\n<p>\n$$ \\mathbf{H = 2A.} $$\n</p>",
    "front": "$\\mathbf{f(x) = x^T A x \\quad \\Rightarrow \\quad H = \\, ?}$",
    "id": 1742435812702,
    "tags": [
      "matrix calculus fundamentals",
      "16.32",
      "18.065"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Solution</b></u><br>\n<p>\n$$ \\mathbf{\\nabla_x V(x,t) = 2P(t)x + 2\\eta(t).} $$\n</p>",
    "front": "$\\mathbf{V(x,t) = x^T P(t)x + 2\\eta(t)^T x + \\rho(t) \\quad \\Rightarrow \\quad \\nabla_x V(x,t) = \\, ?}$",
    "id": 1742435818470,
    "tags": [
      "matrix calculus fundamentals",
      "16.32",
      "18.065"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Answer</b></u><br>  \n<p>  \nThe <b>absolute value</b> is the standard norm for scalars in \\( \\mathbb{R} \\), and the modulus is the norm in \\( \\mathbb{C} \\). It qualifies as a norm because it satisfies three key properties:  \n</p>  \n<ol>  \n  <li><b>Non-negativity and Definiteness:</b>  <br>\n  $$ |x| \\geq 0, \\quad |x| = 0 \\text{ if and only if } x = 0. $$  \n  </li>  \n  <li><b>Homogeneity (Absolute Scalability):</b>  <br>\n  $$ |\\alpha x| = |\\alpha| \\cdot |x|, \\quad \\text{for any scalar } \\alpha. $$  \n  </li>  \n  <li><b>Triangle Inequality:</b>  <br>\n  $$ |x + y| \\leq |x| + |y|, \\quad \\text{for any scalars } x, y. $$  \n  </li>  \n</ol>  ",
    "front": "<b>What is the standard norm for scalars? Why is it a norm?</b> ",
    "id": 1741583127018,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741814336690
      },
      {
        "correct": false,
        "timestamp": 1741814338023
      },
      {
        "correct": true,
        "timestamp": 1741814338816
      },
      {
        "correct": true,
        "timestamp": 1741814340140
      },
      {
        "correct": true,
        "timestamp": 1741814343510
      }
    ],
    "back": "<u><b>Definition</b></u><br>\n<p>\nThe projection of \\( v_k \\) onto \\( q_i \\) is given by: <br>\n$$\n\\text{proj}_{q_i}(v_k) = \\frac{\\langle v_k, q_i \\rangle}{\\langle q_i, q_i \\rangle} q_i = \\left( \\frac{\\| v_k^T q_i \\|}{\\| q_i \\|} \\right) \\frac{q_i}{\\| q_i \\|} = \\frac{\\| v_k^T q_i \\|}{\\| q_i \\|^2} q_i\n$$\n<br>\nwhere \\( \\langle v_k, q_i \\rangle \\) represents the inner product of \\( v_k \\) and \\( q_i \\).\n</p>\n",
    "front": "<b>Projection of \\( v_k \\) onto \\( q_i \\)?</b>",
    "id": 1741812268747,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Answer</b></u><br>  \n<p>  \nBy the distributive property of vector multiplication, we expand as follows:  <br>\n$$  \nu_i^\\top (a + b) = u_i^\\top a + u_i^\\top b  \n$$  \n<br>\nwhere \\( u_i^\\top \\) represents the transpose of the \\( i \\)-th unit vector.  \n</p>  ",
    "front": "<b>Distributive Property of Vector Multiplication</b>  \n<ul>  \n    <li><b>Expression:</b> \\( u_i^\\top (a + b) = \\) ?</li>  \n</ul>",
    "id": 1741819054246,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": false,
        "timestamp": 1741840151126
      },
      {
        "correct": true,
        "timestamp": 1741840151440
      },
      {
        "correct": true,
        "timestamp": 1741840152393
      },
      {
        "correct": true,
        "timestamp": 1741840152777
      },
      {
        "correct": true,
        "timestamp": 1741840153337
      }
    ],
    "back": "<u><b>Definition</b></u><br>\n<p>\nA vector norm is a function \\( \\|\\cdot\\|: \\mathbb{R}^{n} \\to \\mathbb{R} \\) that assigns a non-negative length to a vector and satisfies the following three properties for all \\( \\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^{n} \\) and any scalar \\( \\alpha \\in \\mathbb{R} \\):\n</p>\n<ol>\n  <li><b>Non-negativity and Definiteness:</b> \\( \\|\\mathbf{v}\\| \\geq 0 \\) and \\( \\|\\mathbf{v}\\| = 0 \\) if and only if \\( \\mathbf{v} = 0 \\).</li>\n  <li><b>Homogeneity (Scaling Property):</b> \\( \\|\\alpha \\mathbf{v}\\| = |\\alpha| \\|\\mathbf{v}\\| \\).</li>\n  <li><b>Triangle Inequality:</b> \\( \\|\\mathbf{v} + \\mathbf{w}\\| \\leq \\|\\mathbf{v}\\| + \\|\\mathbf{w}\\| \\).</li>\n</ol>",
    "front": "<b>Vector Norm</b>\n<ul>\n  <li><b>Definition:</b> What is a vector norm, and what properties must it satisfy?</li>\n</ul>",
    "id": 1741557565682,
    "tags": [
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "TODO: New front content",
    "id": 1741835588624,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Answer</b></u><br>\n<p>\nThe expression for \\( (A \\cdot A^T)_{ij} \\) in summation notation is:\n$$\n(A \\cdot A^T)_{ij} = \\sum_{k=1}^{n} A_{ik} A_{jk}\n$$\n<br>\nSince \\( A^T_{kj} = A_{jk} \\), this is equivalent to: <br>\n$$\n(A \\cdot A^T)_{ij} = \\sum_{k=1}^{n} A_{ik} A^T_{kj}\n$$\n</p>",
    "front": "<b>Summation Notation of \\( A \\cdot A^T \\)</b>\n<ul>  \n    <li>Write the expression for \\( (A \\cdot A^T)_{ij} \\) using summation notation.</li>\n</ul>",
    "id": 1741578939662,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Definition</b></u><br>\n<p>\nThe <b>Frobenius norm</b> of an \\( m \\times n \\) real matrix \\( A \\) is defined as the square root of the sum of the squares of its elements:\n$$\n\\|A\\|_F = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}^2}\n$$\n<br><br>\nIt is also equivalent to the square root of the trace of \\( A^T A \\):\n$$\n\\|A\\|_F = \\sqrt{\\operatorname{Tr} (A^T A)}\n$$\n</p>\n<br>\n<u><b>Procedure</b></u><br>\n<ol>\n  <li>Square each element of the matrix \\( A \\).</li>\n  <li>Sum all squared values.</li>\n  <li>Take the square root of the result.</li>\n</ol>\n<br>\n<u><b>Practical Use</b></u><br>\n<p>\nThe Frobenius norm is widely used in numerical analysis, machine learning, and optimization to measure matrix magnitude and quantify errors in approximations.\n</p>",
    "front": "<b>Frobenius Matrix Norm</b>\n<ul>  \n    <li><b>Define:</b> What is the Frobenius norm?</li>\n    <li><b>Procedure:</b> How do you compute the Frobenius norm of a matrix?</li>\n    <li><b>Practical Use:</b> Where/when is the Frobenius norm used?</li>\n</ul>",
    "id": 1741576267043,
    "tags": [
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": false,
        "timestamp": 1741840673326
      },
      {
        "correct": true,
        "timestamp": 1741840674263
      },
      {
        "correct": true,
        "timestamp": 1741840689419
      }
    ],
    "back": "<u><b>Definition</b></u><br>\n<p>\nThe <b>Vector L1 Norm</b> (also called the Manhattan norm or taxicab norm) is the sum of the absolute values of the vector components: <br>\n$$\n\\| \\mathbf{x} \\|_1 = \\sum_{i=1}^{n} |x_i|\n$$\n<br>\nwhere \\( x_i \\) represents the components of the vector \\( \\mathbf{x} \\).\n</p>",
    "front": "<b>Vector L1 Norm</b>\n<ul>  \n    <li><b>Define:</b> What is a Vector L1 Norm?</li>\n    <li><b>Procedure:</b> How do you compute the L1 norm of a vector?</li>\n    <li><b>Practical Use:</b> Where/when is the L1 norm used?</li>\n</ul>",
    "id": 1741580520869,
    "tags": [
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741840657936
      },
      {
        "correct": true,
        "timestamp": 1741840708711
      }
    ],
    "back": "<u><b>Definition</b></u><br>\n<p>\nThe <b>Vector L2 Norm</b> (also called the Euclidean norm) is the square root of the sum of the squared components of the vector: <br>\n$$\n\\| \\mathbf{x} \\|_2 = \\sqrt{\\sum_{i=1}^{n} x_i^2}\n$$\n<br>\nwhere \\( x_i \\) represents the components of the vector \\( \\mathbf{x} \\).\n</p>",
    "front": "<b>Vector L2 Norm</b>\n<ul>  \n    <li><b>Define:</b> What is a Vector L2 Norm?</li>\n</ul>",
    "id": 1741580621810,
    "tags": [
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741849808936
      }
    ],
    "back": "Just another name for the L2 vector norm! :)",
    "front": "Spectral vector norm",
    "id": 1741841305902,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741849821874
      }
    ],
    "back": "<u><b>Definition</b></u><br>\n<p>\nThe <b>Vector $\\mathbf{L_{\\infty}}$ Norm</b> (also called the maximum norm or supremum norm) is defined as the maximum absolute value of the components of the vector: <br>\n$$\n\\| \\mathbf{x} \\|_\\infty = \\max_{1 \\leq i \\leq n} |x_i|\n$$\n<br>\nwhere \\( x_i \\) represents the components of the vector \\( \\mathbf{x} \\).\n</p>",
    "front": "<b>Vector $l_{\\infty}$ Norm</b>\n<ul>  \n    <li><b>Define:</b> What is a Vector $l_{\\infty}$ Norm?</li>\n</ul>",
    "id": 1741586047180,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741840979900
      },
      {
        "correct": true,
        "timestamp": 1741849829482
      }
    ],
    "back": "<u><b>Definition</b></u><br>\n<p>\nThe <b>Frobenius Vector Norm</b> is a measure of a vector's magnitude, defined as the square root of the sum of the absolute squares of its components: <br>\n$$\n\\| \\mathbf{x} \\|_F = \\sqrt{\\sum_{i=1}^{n} |x_i|^2}\n$$\n<br>\nwhere \\( x_i \\) are the components of the vector \\( \\mathbf{x} \\).\n</p>",
    "front": "<b>Frobenius Vector Norm</b>\n<ul>  \n    <li><b>Define:</b> What is the Frobenius Vector Norm?</li>\n</ul>",
    "id": 1741840883509,
    "tags": [
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741840285550
      },
      {
        "correct": false,
        "timestamp": 1741840287739
      },
      {
        "correct": true,
        "timestamp": 1741840288617
      },
      {
        "correct": true,
        "timestamp": 1741840599393
      },
      {
        "correct": true,
        "timestamp": 1741840602640
      }
    ],
    "back": "<u><b>Definition</b></u><br>\n<p>\nAn <b>induced matrix norm</b> (or operator norm) is a norm for matrices that is derived from a given vector norm. It is defined as: <br>\n$$\n\\|A\\| = \\max_{x \\neq 0} \\frac{\\|Ax\\|}{\\|x\\|} = \\max_{\\|x\\|=1} \\|Ax\\|\n$$\n<br>\nwhere \\( A \\) is an \\( m \\times n \\) matrix, and \\( \\|\\cdot\\| \\) is a vector norm on \\( \\mathbb{R}^n \\).\n</p>\n<br>\n\n<u><b>Key Properties</b></u><br>\n<ol>\n  <li><b>Non-Negativity and Definiteness:</b>\n    <ul>\n      <li>\\(\\|A\\| \\geq 0\\) for any matrix \\( A \\).</li>\n      <li>\\(\\|A\\| = 0\\) if and only if \\( A \\) is the zero matrix.</li>\n    </ul>\n  </li>\n  <li><b>Homogeneity (Absolute Scalability):</b>\n    <ul>\n      <li>For any scalar \\( \\alpha \\) and matrix \\( A \\),</li>\n      <li>$$ \\|\\alpha A\\| = |\\alpha| \\|A\\|. $$</li>\n    </ul>\n  </li>\n  <li><b>Triangle Inequality (Subadditivity):</b>\n    <ul>\n      <li>For any two matrices \\( A \\) and \\( B \\) of the same size,</li>\n      <li>$$ \\|A + B\\| \\leq \\|A\\| + \\|B\\|. $$</li>\n    </ul>\n  </li>\n  <li><b>Submultiplicativity:</b>\n    <ul>\n      <li>For any two matrices \\( A \\) and \\( B \\) such that the product \\( AB \\) is defined,</li>\n      <li>$$ \\|AB\\| \\leq \\|A\\| \\|B\\|. $$</li>\n    </ul>\n  </li>\n  <li><b>Consistency with the Vector Norm:</b>\n    <ul>\n      <li>For any vector \\( x \\),</li>\n      <li>$$ \\|Ax\\| \\leq \\|A\\| \\|x\\|. $$</li>\n    </ul>\n  </li>\n</ol>\n<br>\n\n<u><b>Procedure</b></u><br>\n<ol>\n  <li>Select a vector norm \\( \\|\\cdot\\| \\) (e.g., \\( \\ell_1 \\), \\( \\ell_2 \\), or \\( \\ell_{\\infty} \\) norm).</li>\n  <li>Compute \\( \\|Ax\\| \\) for all unit vectors \\( x \\) (\\(\\|x\\| = 1\\)).</li>\n  <li>Find the maximum value of \\( \\|Ax\\| \\), which gives the induced matrix norm.</li>\n</ol>\n<br>",
    "front": "<b>Induced Matrix Norm</b>\n<ul>  \n    <li><b>Define:</b> What is an induced matrix norm?</li>\n    <li><b>Procedure:</b> How do you compute the induced matrix norm from a given vector norm?</li>\n</ul>",
    "id": 1741582036446,
    "tags": [
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Definition</b></u><br>  \n<p>  \nThe <b>trace</b> of a square matrix \\( A \\), denoted as \\( \\text{tr}(A) \\), is the sum of its diagonal elements:  <br>\n$$  \n\\text{tr}(A) = \\sum_{i=1}^{n} A_{ii}  \n$$  \n<br>\nwhere \\( A_{ii} \\) represents the diagonal entries of the \\( n \\times n \\) matrix \\( A \\).\n</p>  \n<br>  \n<u><b>Procedure</b></u><br>  \n<ol>  \n  <li>Identify the main diagonal elements of the square matrix.</li>  \n  <li>Sum all the diagonal elements.</li>  \n</ol>  \n<br>  \n<u><b>Practical Use</b></u><br>  \n<p>  \nThe trace of a matrix is used in various mathematical and applied fields, including:  \n<ul>  \n  <li>Linear algebra (e.g., determining similarity transformations).</li>  \n  <li>Quantum mechanics (e.g., density matrices in quantum states).</li>  \n  <li>Machine learning (e.g., computing matrix derivatives).</li>  \n  <li>Control theory (e.g., analyzing system stability).</li>  \n</ul>  \n</p>  ",
    "front": "<b>Trace of a Matrix</b>  \n<ul>  \n    <li><b>Define:</b> What is the trace of a matrix?</li>  \n    <li><b>Procedure:</b> How do you compute the trace of a matrix?</li>  \n    <li><b>Practical Use:</b> Where/when is the trace of a matrix used?</li>  \n</ul>  ",
    "id": 1741577714816,
    "tags": [
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "$$\n\\|A\\|_F = \\left( \\sum_{i=1}^{n} \\sum_{j=1}^{n} a_{i,j}^2 \\right)^{\\frac{1}{2}} = \\left( \\text{trace} \\left( A^T A \\right) \\right)^{\\frac{1}{2}}\n$$",
    "id": 1741577238385,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Conjugate Transpose",
    "id": 1741576658519,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Definition</b></u><br>  \n<p>  \nA <b>Symmetric-Antisymmetric Decomposition</b> is a unique way to express any square matrix \\( A \\) as the sum of a symmetric matrix \\( S \\) and a skew-symmetric (antisymmetric) matrix \\( N \\):  <br>\n$$  \nA = S + N  \n$$  \n<br>\nwhere  <br>\n$$  \nS = \\frac{1}{2} (A + A^T), \\quad N = \\frac{1}{2} (A - A^T)  \n$$ \n<br> \nHere, \\( S^T = S \\) and \\( N^T = -N \\).\n</p>  \n<br>  \n\n<u><b>Procedure</b></u><br>  \n<ol>  \n  <li>Given a square matrix \\( A \\), compute its transpose \\( A^T \\).</li>  \n  <li>Calculate the symmetric part: \\( S = \\frac{1}{2} (A + A^T) \\).</li>  \n  <li>Calculate the antisymmetric part: \\( N = \\frac{1}{2} (A - A^T) \\).</li>  \n  <li>Verify that \\( S \\) is symmetric and \\( N \\) is antisymmetric.</li>  \n</ol>  \n<br>\n\n<u><b>Practical Use</b></u><br>  \n<ul>  \n    <li>Used in quadratic forms, where only the symmetric part matters for positive definiteness.</li>  \n    <li>Fundamental in matrix theory and Lie algebra decomposition.</li>  \n    <li>Important in physics for separating conservative (symmetric) and rotational (antisymmetric) components of tensors.</li>  \n    <li>Helps in solving eigenvalue problems and in principal component analysis (PCA).</li>  \n</ul>  \n<br>  \n",
    "front": "<b>Symmetric-Antisymmetric Decomposition</b>  \n<ul>  \n    <li><b>Define:</b> What is a Symmetric-Antisymmetric Decomposition?</li>\n    <li><b>Procedure:</b> How do you perform Symmetric-Antisymmetric Decomposition?</li>\n    <li><b>Practical Use:</b> Where/when is Symmetric-Antisymmetric Decomposition used?</li>\n</ul>  ",
    "id": 1741551371010,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741555932662
      },
      {
        "correct": false,
        "timestamp": 1741555934785
      },
      {
        "correct": true,
        "timestamp": 1741555935529
      },
      {
        "correct": true,
        "timestamp": 1741841169914
      },
      {
        "correct": true,
        "timestamp": 1745279883284
      }
    ],
    "back": "<u><b>Definition</b></u><br>\n<p>\nA matrix \\( S \\in \\mathbb{R}^{n \\times n} \\) is positive definite (PD) if two conditions hold:\n</p>\n<ol>\n  <li>\\( S^T = S \\), i.e., \\( S \\) is symmetric.</li>\n  <li>\\( v^{T} S v > 0 \\) for all \\( v \\in \\mathbb{R}^{n} \\setminus \\{0\\} \\), meaning the quadratic form is strictly positive for all nonzero vectors.</li>\n</ol>\n<br>\n\n<u><b>Notation</b></u><br>\n<p>\nA positive definite matrix is denoted as \\( S \\succ 0 \\).\n</p>\n<br>\n\n<u><b>Procedure</b></u><br>\n<p>\nTo check if a matrix \\( S \\) is positive definite:\n</p>\n<ol>\n  <li>Verify that \\( S \\) is symmetric (\\( S^T = S \\)).</li>\n  <li>Use the <b>Eigenvalue Test</b>: Compute the eigenvalues \\( \\lambda_i \\) of \\( S \\). If all eigenvalues satisfy \\( \\lambda_i > 0 \\), then \\( S \\) is positive definite.</li>\n</ol>",
    "front": "<b>Positive Definite</b>\n<ul>\n  <li><b>Definition:</b> What is a Positive Definite (PD) matrix?</li>\n  <li><b>Notation:</b> How is a PD matrix denoted?</li>\n  <li><b>Procedure:</b> How do you check if a matrix is PD?</li>\n</ul>",
    "id": 1741553622438,
    "tags": [
      "18.065",
      "midterm1",
      "16.32",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741555931639
      },
      {
        "correct": false,
        "timestamp": 1741555936869
      },
      {
        "correct": true,
        "timestamp": 1741555937053
      },
      {
        "correct": true,
        "timestamp": 1741841167646
      },
      {
        "correct": true,
        "timestamp": 1745279802617
      }
    ],
    "back": "<u><b>Definition</b></u><br>\n<p>\nA matrix \\( S \\in \\mathbb{R}^{n \\times n} \\) is positive semi-definite (PSD) if two conditions hold:\n</p>\n<ol>\n  <li>\\( S^T = S \\), i.e., \\( S \\) is symmetric.</li>\n  <li>\\( v^{T} S v \\geq 0 \\) for all \\( v \\in \\mathbb{R}^{n} \\), meaning the quadratic form is non-negative for all vectors.</li>\n</ol>\n<br>\n\n<u><b>Notation</b></u><br>\n<p>\nA positive semi-definite matrix is denoted as \\( S \\succeq 0 \\).\n</p>\n<br>\n\n<u><b>Procedure</b></u><br>\n<p>\nTo check if a matrix \\( S \\) is positive semi-definite:\n</p>\n<ol>\n  <li>Verify that \\( S \\) is symmetric (\\( S^T = S \\)).</li>\n  <li>Use the <b>Eigenvalue Test</b>: Compute the eigenvalues \\( \\lambda_i \\) of \\( S \\). If all eigenvalues satisfy \\( \\lambda_i \\geq 0 \\), then \\( S \\) is positive semi-definite.</li>\n</ol>",
    "front": "<b>Positive Semi-Definite</b>\n<ul>\n  <li><b>Definition:</b> What is a Positive Semi-Definite (PSD) matrix?</li>\n  <li><b>Notation:</b> How is a PSD matrix denoted?</li>\n  <li><b>Procedure:</b> How do you check if a matrix is PSD?</li>\n</ul>",
    "id": 1741551353638,
    "tags": [
      "18.065",
      "midterm1",
      "16.32",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741843023841
      }
    ],
    "back": "<u><b>Concept</b></u><br>\n<p>\nThe Hessian test is used at a critical point (where the gradient is zero) to determine the type of extremum.\n</p>\n<br>\n\n<u><b>Positive Definite (PD) Case</b></u><br>\n<p>\nIf all eigenvalues of the Hessian matrix are strictly positive, then the function has a <b>strict local minimum</b> at that point. This means the function value increases in every direction away from the point.\n</p>\n<br>\n\n<u><b>Positive Semi-Definite (PSD) Case</b></u><br>\n<p>\nIf all eigenvalues of the Hessian are nonnegative (some may be zero), then the Hessian suggests a local minimum, but the test is <b>inconclusive regarding strictness </b>. The function might be flat in some directions, meaning it is not necessarily a strict minimum.\n</p>\n<br>\n\n<u><b>Summary</b></u><br>\n<p>\nA PD Hessian guarantees a strong local minimum, while a PSD Hessian only suggests a local minimum without guaranteeing strictness.\n</p>",
    "front": "<b>Hessian Test for Local Minima</b>\n<ul>\n  <li><b>Concept:</b> What does the Hessian test determine at a critical point?</li>\n  <li><b>Positive Definite (PD) Case:</b> What does a PD Hessian indicate?</li>\n  <li><b>Positive Semi-Definite (PSD) Case:</b> What does a PSD Hessian indicate?</li>\n</ul>",
    "id": 1741556276234,
    "tags": [
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n\\sin\\left(x+\\frac{\\pi}{2}\\right) = \\cos(x)\n$$",
    "front": "$$\n\\sin\\left(x+\\frac{\\pi}{2}\\right) = \\, \\text{?}\n$$",
    "id": 1741487630440,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n\\cos\\left(x+\\frac{\\pi}{2}\\right) = -\\sin(x)\n$$",
    "front": "$$\n\\cos\\left(x+\\frac{\\pi}{2}\\right) = \\, \\text{?}\n$$",
    "id": 1741487733230,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n\\int_{t_0}^{t_f} \\sin(at+b)\\, dt = \\left[-\\frac{1}{a}\\cos(at+b)\\right]_{t_0}^{t_f}\n$$\n<br><br>\nDo not forget about the sneaky chain rule when doing anti-derivatives!",
    "front": "$$\n\\int_{t_0}^{t_f} \\sin(at+b)\\, dt = \\, \\text{?}\n$$",
    "id": 1741484115943,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n\\int_{t_0}^{t_f} \\cos(at+b)\\, dt = \\left[\\frac{1}{a}\\sin(at+b)\\right]_{t_0}^{t_f}\n$$\n<br><br>\nDo not forget about the sneaky chain rule when doing anti-derivatives!",
    "front": "$$\n\\int_{t_0}^{t_f} \\cos(at+b)\\, dt = \\, \\text{?}\n$$",
    "id": 1741484741272,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$Ax$ is a linear combination of the columns of $A$.\n$$ \nAx = [a_1, a_2, \\dots, a_n] \\cdot \n\\begin{bmatrix} x_1 \\\\  x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \n= x_1a_1 + x_2a_2 + \\dots + x_na_n \n$$",
    "front": "Matrix vector multiplication $Ax$",
    "id": 1741420947202,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741843116415
      },
      {
        "correct": true,
        "timestamp": 1745280125467
      }
    ],
    "back": "<b><u>Definition</u></b><br>  \nThe condition number of a matrix (or function) quantifies how sensitive the output is to small changes in the input. In numerical linear algebra, the condition number of a matrix \\( A \\) is defined as:<br>\n\n\\[\n\\kappa(A) = \\|A\\| \\cdot \\|A^{-1}\\|\n\\]\n\n<br>  \nfor a given matrix norm. It measures how much relative error in the input propagates to the output in solving linear systems.<br><br>  \n\n<b><u>Interpretation & Usage</u></b><br>  \n<ol>  \n  <li>A <b>low condition number</b> (close to 1) indicates a well-conditioned problem, meaning small input changes lead to small output changes.</li>  \n  <li>A <b>high condition number</b> (much greater than 1) suggests an ill-conditioned problem, where small input errors can cause large output errors, making numerical solutions unstable.</li>  \n  <li>In optimization and scientific computing, condition numbers help assess numerical stability and the reliability of computed solutions.</li>  \n</ol> \n<br>\n<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/Screenshot_from_2025-04-21_20-01-03.png\"></div>",
    "front": "<b>Condition Number</b>  \n<ul>  \n  <li><b>Define:</b> What is the condition number of a matrix or function, and what does it measure?</li>  \n  <li><b>Interpretation & Usage:</b> How is the condition number used in numerical analysis, and what does a high or low value indicate about a problem\u2019s stability?</li>  \n</ul>  ",
    "id": 1741379016329,
    "tags": [
      "16.32",
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n\\frac{d}{dx}\\cos(x) = -\\sin(x)\n$$",
    "front": "$$\n\\frac{d}{dx}\\cos(x) = \\, \\text{?}\n$$",
    "id": 1741483979618,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n\\frac{d}{dx}\\tan(x) = \\sec^2(x)\n$$",
    "front": "$$\n\\frac{d}{dx}\\tan(x) = \\, \\text{?}\n$$",
    "id": 1741483988448,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n\\frac{d}{dx}\\cot(x) = -\\csc^2(x)\n$$",
    "front": "$$\n\\frac{d}{dx}\\cot(x) = \\, \\text{?}\n$$",
    "id": 1741484025952,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n\\frac{sin(x)}{cos(x)} = \\, tan(x)\n$$",
    "front": "$$\n\\frac{sin(x)}{cos(x)} = \\, \\text{?}\n$$",
    "id": 1741480472207,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n\\tan(x) = \\frac{\\sin(x)}{\\cos(x)}\n$$",
    "front": "$$\n\\tan(x) = \\, \\text{?}\n$$",
    "id": 1741481425263,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n\\cot(x) = \\frac{\\cos(x)}{\\sin(x)}\n$$",
    "front": "$$\n\\cot(x) = \\, \\text{?}\n$$",
    "id": 1741481442494,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n\\sec(x) = \\frac{1}{\\cos(x)}\n$$",
    "front": "$$\n\\sec(x) = \\, \\text{?}\n$$",
    "id": 1741481457664,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n\\csc(x) = \\frac{1}{\\sin(x)}\n$$",
    "front": "$$\n\\csc(x) = \\, \\text{?}\n$$",
    "id": 1741481473863,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n\\sin^2(x) + \\cos^2(x) = 1\n$$",
    "front": "$$\n\\sin^2(x) + \\cos^2(x) = \\, \\text{?}\n$$",
    "id": 1741481480924,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n1 + \\cot^2(x) = \\csc^2(x)\n$$",
    "front": "$$\n1 + \\cot^2(x) = \\, \\text{?}\n$$",
    "id": 1741481395486,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n1 + tan^2(x) = \\, sec^2(x)\n$$",
    "front": "$$\n1 + tan^2(x) = \\, \\text{?}\n$$",
    "id": 1741480992748,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n\\begin{aligned}\n\\sin(a\\pm b) &= \\sin(a)\\cos(b) \\pm \\cos(a)\\sin(b), \\\\\n\\cos(a\\pm b) &= \\cos(a)\\cos(b) \\mp \\sin(a)\\sin(b), \\\\\n\\tan(a\\pm b) &= \\frac{\\tan(a) \\pm \\tan(b)}{1 \\mp \\tan(a)\\tan(b)}.\n\\end{aligned}\n$$",
    "front": "$$\n\\begin{aligned}\n\\sin(a\\pm b) &= \\, \\text{?} \\\\\n\\cos(a\\pm b) &= \\, \\text{?} \\\\\n\\tan(a\\pm b) &= \\, \\text{?}\n\\end{aligned}\n$$",
    "id": 1741481623251,
    "tags": [
      "16.32",
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Logarithmic Integral Identity <br>\n$$\n\\int \\frac{du}{u} = \\ln |u| + C.\n$$\n",
    "front": "What identity is this? <br>\n$$\n\\int \\frac{du}{u} = \\, \\text{?}\n$$",
    "id": 1741153604364,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Power Rule for Integration <br>\n$$\n\\int u^n \\, du = \\frac{u^{n+1}}{n+1} + C, \\quad n \\neq -1.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int u^n \\, du = \\, \\text{?} \\quad (n \\neq -1)\n$$",
    "id": 1741153795280,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Exponential Integral Identity <br>\n$$\n\\int e^u \\, du = e^u + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int e^u \\, du = \\, \\text{?}\n$$",
    "id": 1741153863506,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Exponential Integral for Base \\( a \\) <br>\n$$\n\\int a^u \\, du = \\frac{a^u}{\\ln a} + C, \\quad a > 0, a \\neq 1.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int a^u \\, du = \\, \\text{?}\n$$",
    "id": 1741153887555,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Sine Integral Identity <br>\n$$\n\\int \\sin u \\, du = -\\cos u + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\sin u \\, du = \\, \\text{?}\n$$",
    "id": 1741153930393,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Cosine Integral Identity <br>\n$$\n\\int \\cos u \\, du = \\sin u + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\cos u \\, du = \\, \\text{?}\n$$",
    "id": 1741153943600,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Tangent Integral Identity <br>\n$$\n\\int \\tan u \\, du = \\ln |\\sec u| + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\tan u \\, du = \\, \\text{?}\n$$",
    "id": 1741153956036,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Secant Integral Identity <br>\n$$\n\\int \\sec u \\, du = \\ln |\\sec u + \\tan u| + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\sec u \\, du = \\, \\text{?}\n$$",
    "id": 1741153992908,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Cosecant Integral Identity <br>\n$$\n\\int \\csc u \\, du = \\ln |\\csc u - \\cot u| + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\csc u \\, du = \\, \\text{?}\n$$",
    "id": 1741154002664,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Inverse Sine Integral Identity <br>\n$$\n\\int \\frac{du}{\\sqrt{1 - u^2}} = \\sin^{-1} u + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\frac{du}{\\sqrt{1 - u^2}} = \\, \\text{?}\n$$",
    "id": 1741154022762,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Inverse Tangent Integral Identity <br>\n$$\n\\int \\frac{du}{1 + u^2} = \\tan^{-1} u + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\frac{du}{1 + u^2} = \\, \\text{?}\n$$",
    "id": 1741154033446,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Hyperbolic Integral Identity <br>\n$$\n\\int \\frac{du}{\\sqrt{u^2 + C_1}} = \\ln | u + \\sqrt{u^2 + C_1} | + C_2.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\frac{du}{\\sqrt{u^2 + C}} = \\, \\text{?}\n$$",
    "id": 1741154053536,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Integral of \\( \\frac{1}{A + u^2} \\) <br>\n$$\n\\int \\frac{du}{A + u^2} = \\frac{1}{\\sqrt{A}} \\tan^{-1} \\left(\\frac{u}{\\sqrt{A}}\\right) + C.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int \\frac{du}{A + u^2} = \\, \\text{?}\n$$",
    "id": 1741154072452,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Integration by Parts Formula <br>\n$$\n\\int u v' \\, du = uv - \\int u' v \\, du.\n$$",
    "front": "What identity is this? <br>\n$$\n\\int u v' \\, du = \\, \\text{?}\n$$",
    "id": 1741154102843,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<b><u>Definition</u></b> <br>\n$$\n\\sinh x = \\frac{e^x - e^{-x}}{2}.\n$$",
    "front": "<b>Hyperbolic Sine</b>\n<ul>\n    <li><b>Define:</b> What is the definition of hyperbolic sine?</li>\n</ul>",
    "id": 1741155965429,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<b><u>Definition</u></b> <br>\n$$\n\\cosh x = \\frac{e^x + e^{-x}}{2}.\n$$",
    "front": "<b>Hyperbolic Cosine</b>\n<ul>\n    <li><b>Define:</b> What is the definition of hyperbolic cosine?</li>\n</ul>",
    "id": 1741155975921,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<b><u>Separation of Variables Technique</u></b> <br>\nSeparation of variables is a method for solving first-order ordinary differential equations (ODEs) of the form: <br>  \n$$\n\\frac{dy}{dx} = f(x) g(y).\n$$  \n<br>  \nThe method involves rewriting the equation so that all terms involving \\( y \\) are on one side and all terms involving \\( x \\) are on the other: <br>  \n$$\n\\frac{dy}{g(y)} = f(x) dx.\n$$  \n<br>  \nThen, both sides are integrated separately: <br>  \n$$\n\\int \\frac{dy}{g(y)} = \\int f(x) dx.\n$$  \n<br>  \nSolving these integrals gives the general solution to the differential equation.",
    "front": "What is the separation of variables technique in differential equations?",
    "id": 1741156069533,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<b><u>Definition</u></b><br>  \nCompleting the square is an algebraic technique used to transform a quadratic expression into a perfect square trinomial plus a constant. This method is useful for solving quadratic equations, analyzing parabolas, and deriving the quadratic formula.<br><br>  \n\n<b><u>Procedure</u></b><br>  \nGiven a quadratic expression of the form: $ax^2 + bx + c$\n<ol>  \n  <li>Factor out \\( a \\) if it is not 1: <br>\n      \\[\n      a(x^2 + \\frac{b}{a}x) + c\n      \\]  \n  </li>  \n  <li>Add and subtract \\( \\left(\\frac{b}{2a}\\right)^2 \\) inside the parentheses to form a perfect square: <br>\n      \\[\n      a\\left(x^2 + \\frac{b}{a}x + \\left(\\frac{b}{2a}\\right)^2 - \\left(\\frac{b}{2a}\\right)^2\\right) + c\n      \\]  \n  </li>  \n  <li>Rewrite the perfect square trinomial as a squared binomial: <br>\n      \\[\n      a\\left( \\left(x + \\frac{b}{2a} \\right)^2 - \\left(\\frac{b}{2a}\\right)^2 \\right) + c\n      \\]  \n  </li>  \n  <li>Distribute \\( a \\) and simplify: <br>\n      \\[\n      a\\left(x + \\frac{b}{2a}\\right)^2 - a\\left(\\frac{b}{2a}\\right)^2 + c\n      \\]  \n  </li>  \n  <li>Final form: <br>\n      \\[\n      a\\left(x + \\frac{b}{2a}\\right)^2 + \\left(c - \\frac{b^2}{4a}\\right)\n      \\]  \n  </li>  \n</ol>\n <br>\n\nThis method is particularly useful for solving quadratic equations, deriving the quadratic formula, and completing square-based integrals in calculus.",
    "front": "<b>Complete the Square</b>  \n<ul>  \n  <li><b>Define:</b> What does it mean to complete the square in an algebraic expression?</li>  \n  <li><b>Procedure:</b> How do you complete the square for a quadratic expression of the form \\( ax^2 + bx + c \\)?</li>  \n</ul>  ",
    "id": 1741151957761,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<b><u>Definition</u></b><br>  \nThe Beltrami identity is a conserved quantity that arises in the calculus of variations when the Lagrangian does not explicitly depend on the independent variable. It provides a first integral of the Euler-Lagrange equation, simplifying the problem of finding extremals.<br><br>  \n\n<b><u>Derivation & Usage</u></b><br>  \nIf a functional is given by \\( J = \\int L(y, y', x) \\,dx \\), and \\( L \\), the Lagrangian, does not explicitly depend on \\( x \\), the independent variable, then the Beltrami identity states that:<br>  \n\\[\nL - y' \\frac{\\partial L}{\\partial y'} = C\n\\]\nwhere \\( C \\) is a constant.<br><br>  \nThis identity is particularly useful in reducing the order of the Euler-Lagrange equation, making it easier to solve variational problems where the Lagrangian lacks explicit dependence on the independent variable.",
    "front": "<b>Beltrami Identity</b>  \n<ul>  \n  <li><b>Define:</b> What is the Beltrami identity in the context of the calculus of variations?</li>  \n  <li><b>Derivation & Usage:</b> How is the Beltrami identity derived, and when is it used to simplify the Euler-Lagrange equation?</li>  \n</ul>  ",
    "id": 1741151016852,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Definition</b></u><br>  \n<p>  \nClairaut\u2019s theorem states that if a function \\( f(x,y) \\) is twice continuously differentiable (\\( C^2 \\)), then the mixed partial derivatives are equal: <br>\n$$  \nf_{xy} = f_{yx}.  \n$$  \n</p>  \n<br>  \n<u><b>Procedure</b></u><br>  \n<ol>  \n  <li>Compute \\( f_x \\) and \\( f_y \\) (first-order partial derivatives).</li>  \n  <li>Find \\( f_{xy} \\) by differentiating \\( f_x \\) with respect to \\( y \\).</li>  \n  <li>Find \\( f_{yx} \\) by differentiating \\( f_y \\) with respect to \\( x \\).</li>  \n  <li>If \\( f_{xy} = f_{yx} \\) and \\( f \\) is \\( C^2 \\), Clairaut\u2019s theorem holds.</li>  \n</ol>  \n<br>  \n<u><b>Practical Use</b></u><br>  \n<p>  \nFor smooth functions (\\( C^2 \\)), the order of partial differentiation does not matter. However, for non-smooth functions, discontinuities in second derivatives can lead to \\( f_{xy} \\neq f_{yx} \\), requiring careful computation.  \n</p>",
    "front": "<b>Clairaut\u2019s Theorem</b>  \n<ul>  \n    <li><b>Define:</b> What is Clairaut\u2019s theorem?</li>  \n    <li><b>Procedure:</b> How do you verify Clairaut\u2019s theorem for a function?</li>  \n    <li><b>Practical Use:</b> When does the order of partial differentiation matter?</li>  \n</ul>",
    "id": 1741554967453,
    "tags": [
      "18.065",
      "midterm1",
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1745280251313
      }
    ],
    "back": "<u><b>Definition</b></u><br>  \n<p>  \nThe <b>Hessian matrix</b> of a twice differentiable function \\( f(x, y) \\) is the square matrix of second-order partial derivatives:  <br>\n$$  \n\\nabla^2 f(x, y) =  \n\\begin{pmatrix}  \nf_{xx} & f_{xy} \\\\  \nf_{yx} & f_{yy}  \n\\end{pmatrix}.  \n$$\n<br>\nIf \\( f \\) is \\( C^2 \\), Clairaut\u2019s theorem ensures \\( f_{xy} = f_{yx} \\).  \n</p>  \n<br>  \n<u><b>Procedure</b></u><br>  \n<ol>  \n  <li>Compute the first-order partial derivatives \\( f_x \\) and \\( f_y \\).</li>  \n  <li>Find the second-order partial derivatives: \\( f_{xx} \\), \\( f_{yy} \\), and mixed derivatives \\( f_{xy} \\), \\( f_{yx} \\).</li>  \n  <li>Construct the Hessian matrix using these derivatives.</li>  \n</ol>  \n<br>  \n<u><b>Practical Use</b></u><br>  \n<p>  \nThe Hessian matrix is used in:  \n<ul>  \n  <li>Determining the concavity and curvature of functions.</li>  \n  <li>Classifying critical points (local minima, maxima, or saddle points).</li>  \n  <li>Optimization problems in machine learning and economics.</li>  \n  <li>Solving differential equations and stability analysis.</li>  \n</ul>  \n</p>  ",
    "front": "<b>Hessian Matrix</b>  \n<ul>  \n    <li><b>Define:</b> What is the Hessian matrix?</li>  \n    <li><b>Procedure:</b> How do you compute the Hessian matrix?</li>  \n    <li><b>Practical Use:</b> Where is the Hessian matrix used?</li>  \n</ul>  ",
    "id": 1741150839007,
    "tags": [
      "18.065",
      "midterm1",
      "16.32",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO <br><br>\n\nquadrature in the context of numerical optimal control <br><br>\n\nquadrature used to approximate integrals in optimal control problems",
    "front": "<b>Quadrature</b> \n<ul>\n  <li><b>Define:</b> What is quadrature in the context of numerical optimal control?</li>\n  <li><b>Procedure:</b> How is used?</li>\n</ul>",
    "id": 1741148713751,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Explain how to convert this optimal control problem into a standard calculus of variations problem where you seek the function $x(t)$  minimizing a cost functional $J(x)$ subject to boundary conditions.",
    "id": 1741150252242,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<b><u>Definition</u></b> <br>\nThe Euler method is a first-order explicit numerical method that uses the current point to estimate the next point in a step wise fashion.\n<br><br>\n\n<b><u>Practical Purpose</u></b> <br>\nIt is used to approximate solutions to ODEs.\n",
    "front": "Euler method <br><br>\nDefine <br><br>\nPractical purpose",
    "id": 1741038606887,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/trapezoid_rule.webp\"></div>",
    "front": "Trapezoidal Rule <br><br>\n\nPractical use <br><br>\n\nDefinition",
    "id": 1741038908417,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Approach to discretely approximating integrals.\n\n<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/midpoint_rule.jpg\"></div>",
    "front": "Midpoint Rule <br><br>\n\nPractical use <br><br>\n\nDefinition",
    "id": 1741041716871,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "A set of vectors that perfectly describes the space. <br><br>\n\nTheir combinations give one and only one way to produce every vector in the space.",
    "front": "Basis",
    "id": 1740885744839,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Procedure</b></u><br>  \n<ol>  \n  <li>Start with the first equation and solve for the first variable.</li>  \n  <li>Substitute the obtained value into the next equation.</li>  \n  <li>Repeat this process down the system until all variables are solved.</li>  \n</ol>  \n<br>  \n<u><b>Pseudocode</b></u>\n<pre>  \nfor i = 1 to n:\n    for j = 1 to i-1:\n        x[i] = (b[i] - sum(L[i, j] * x[j])) / L[i, i]  \n</pre>  \n",
    "front": "<b>Forward Substitution</b>  \n<ul>  \n    <li><b>Procedure:</b> How do you perform Forward Substitution?</li>  \n</ul>  \n",
    "id": 1741419253077,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Backward Substitution",
    "id": 1741821659612,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/four_fundamental_subspaces.jpeg\"></div>\n<br>\nDimensions: <br>\n$Col(A) \\in \\mathbb{R}^{r}$ <br>\n$Null(A) \\in \\mathbb{R}^{n - r}$ <br>\n$Col(A^T) \\in \\mathbb{R}^{r}$ <br>\n$Null(A) \\in \\mathbb{R}^{m - r}$ <br>",
    "front": "<b>Four Fundamental Subspaces</b>",
    "id": 1740884070112,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 2
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Card for mom",
    "id": 1740894978704,
    "tags": [
      "14.13"
    ],
    "understanding": 3
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "- denoted $Col(A) = C(A)$ <br>\n- the column space is a subspace of $\\mathbb{R}^m$ ($A \\in \\mathbb{R}^{n \\times m}$)<br>\n- consists of all linear combinations of $A$'s columns <br>\n- this is captured by the space spanned by $b$ in $Ax = b, \\, \\forall x$",
    "front": "Column space of $A$",
    "id": 1740886684323,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 2
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "- denoted $Col(A^T) = C(A)$, <br>\n- the column space is a subspace of $\\mathbb{R}^n$ (recall $A \\in \\mathbb{R}^{n \\times m}$)<br>\n- consists of all linear combinations of the columns of $A^T$'s <br>\n- this is captured by the space spanned by $b$ in $A^Ty = b, \\, \\forall y$",
    "front": "Row space of $A$ <br><br>",
    "id": 1740887337718,
    "tags": [
      "18.065",
      "midterm1",
      "hello"
    ],
    "understanding": 2
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<b><u>Definition</u></b> <br>  \nLet \\( A \\in \\mathbb{R}^{m \\times n} \\) be a matrix. The null space of \\( A \\) is defined as: <br>  \n\\[\n\\text{Null}(A) = \\{ x \\in \\mathbb{R}^n \\mid Ax = 0 \\}\n\\]\nwhich is a subspace of \\( \\mathbb{R}^n \\). <br><br>  \n\nSimilarly, the null space of the transpose \\( A^T \\) is: <br>  \n\\[\n\\text{Null}(A^T) = \\{ y \\in \\mathbb{R}^m \\mid A^T y = 0 \\}\n\\]\nwhich is a subspace of \\( \\mathbb{R}^m \\). <br><br>  \n\n<b><u>Dimension Relationships</u></b> <br>  \nThe dimensions of the null space and the row/column spaces are related by: <br>  \n\\[\n\\dim(\\text{Row}(A)) + \\dim(\\text{Null}(A)) = n,\n\\]\n\\[\n\\dim(\\text{Col}(A)) + \\dim(\\text{Null}(A^T)) = m.\n\\]  ",
    "front": "<b>Null Space</b>\n<ul>\n    <li><b>Define:</b> What is the null space of a matrix?</li>\n    <li><b>Properties:</b> What are the key dimension relationships involving the null space?</li>\n</ul>",
    "id": 1740887782700,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<b><u>Definition</u></b> <br>  \nThe orthogonal complement of a subspace \\( S \\) in \\( \\mathbb{R}^n \\), denoted \\( S^\\perp \\), is the set of all vectors in \\( \\mathbb{R}^n \\) that are orthogonal to every vector in \\( S \\):  \n$$  \nS^\\perp = \\{ x \\in \\mathbb{R}^n \\mid x \\perp y, \\; \\forall y \\in S \\}.  \n$$\n<br>\nA vector \\( x \\) is orthogonal to \\( y \\) if and only if their inner product is zero:<br>  \n$$  \nx \\perp y \\iff \\langle x, y \\rangle = x^T y = 0.  \n$$  \n<br><br>\n\n<b><u>Procedure</u></b> <br>  \n1. Find a basis for the subspace \\( S \\). <br>  \n2. Solve for all vectors \\( x \\) that satisfy \\( x^T y = 0 \\) for every basis vector \\( y \\) in \\( S \\). <br>  \n3. The set of all such \\( x \\) forms the orthogonal complement \\( S^\\perp \\). <br>  \n4. If \\( \\dim(S) = k \\) in \\( \\mathbb{R}^n \\), then \\( \\dim(S^\\perp) = n - k \\). <br>  \n<br>  \n\n<b><u>Theorem</u></b> <br>  \nThe null space and row space of a matrix are related through the orthogonal complement:  <br>\n$$  \n\\text{Null}(A) = (\\text{Row}(A))^\\perp.  \n$$\n<br>\nSimilarly, the null space of the transpose is the orthogonal complement of the column space: <br>\n$$  \n\\text{Null}(A^T) = (\\text{Col}(A))^\\perp.  \n$$\n<br>\nThis means that the row space and column space define the constraints that determine the null spaces of \\( A \\) and \\( A^T \\), respectively.  ",
    "front": "<b>Orthogonal Complement</b>  \n<ul>  \n    <li><b>Define:</b> What is the orthogonal complement of a subspace in \\( \\mathbb{R}^n \\)?</li>  \n    <li><b>Procedure:</b> How do you determine the orthogonal complement of a given subspace?</li>  \n    <li><b>Theorem:</b> How are the null space and row space related via the orthogonal complement?</li>  \n</ul>  ",
    "id": 1741167244470,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741074654248
      },
      {
        "correct": true,
        "timestamp": 1745280394273
      }
    ],
    "back": "<u><b>Definition</b></u> <br>\n<p> \nFor a matrix $A \\in \\mathbb{R}^{m \\times n}$, a vector $x \\in \\mathbb{R}^{n} \\setminus \\{0\\}$, and a scalar $\\lambda \\in \\mathbb{R}$, the eigenvalues $\\lambda$ and eigenvectors $x$ satisfy:  <br>\n$$\nAx = \\lambda x\n$$\n</p> \n<br>\n\n<u><b>Procedure</b></u> <br>\n<ol>\n    <li>Rearrange the equation to:\n        $$\n        (A - \\lambda I)x = 0\n        $$\n    </li>\n    <li>For a nonzero $x$, the system has a nontrivial solution only if:\n        $$\n        \\det(A - \\lambda I) = 0\n        $$\n    </li>\n    <li>Solve the characteristic equation $\\det(A - \\lambda I) = 0$ for $\\lambda$ to find the eigenvalues.</li>\n    <li>For each $\\lambda$, substitute into:\n        $$\n        (A - \\lambda I)x = 0\n        $$\n        and solve the resulting system for the eigenvector $x$.</li>\n</ol>",
    "front": "<b>Eigenvalues and Eigenvectors</b> \n<ul>\n  <li><b>Define:</b> What are eigenvalues and eigenvectors?</li>\n  <li><b>Procedure:</b> How do we compute eigenvalues and eigenvectors?</li>\n</ul>",
    "id": 1740890671935,
    "tags": [
      "18.065",
      "midterm1",
      "16.32",
      "midterm2"
    ],
    "understanding": 2
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741843127771
      },
      {
        "correct": true,
        "timestamp": 1745280603920
      }
    ],
    "back": "<u><b>Definition</b></u><br>\n<p>\nThe <b>Spectral Theorem</b> states that every real symmetric matrix can be diagonalized by an orthogonal matrix. More precisely, for a real symmetric matrix \\( A \\), there exists an orthogonal matrix \\( Q \\) and a diagonal matrix \\( \\Lambda \\) such that:\n$$\nA = Q \\Lambda Q^T\n$$\nwhere:\n<ul>\n    <li>The columns of \\( Q \\) are the eigenvectors of \\( A \\) and form an orthonormal basis.</li>\n    <li>The diagonal entries of \\( \\Lambda \\) are the eigenvalues of \\( A \\), which are all real.</li>\n</ul>\n</p>",
    "front": "<b>Spectral Theorem</b>\n<ul>  \n    <li><b>Define:</b> What is the Spectral Theorem?</li>\n</ul>",
    "id": 1741841391277,
    "tags": [
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nFor a square matrix $A$, the <b>characteristic polynomial</b> is defined as:\n$$\np(\\lambda) = \\det(A - \\lambda I)\n$$\n</p>\n<br>\n<u><b>Derivation</b></u> <br>\n<ol>\n  <li>Start with the eigenvalue equation:\n    $$\n    Ax = \\lambda x\n    $$\n    for a nonzero vector $x$.\n  </li>\n  <li>Rearrange the equation to:\n    $$\n    (A - \\lambda I)x = 0\n    $$\n  </li>\n  <li>Since $x \\neq 0$, a nontrivial solution exists only if the matrix $(A - \\lambda I)$ is singular, hence:\n    $$\n    \\det(A - \\lambda I) = 0\n    $$\n  </li>\n  <li>This determinant equation is the <b>characteristic polynomial</b> of $A$.\n  </li>\n</ol>",
    "front": "<b>Characteristic Polynomial</b> \n<ul>\n  <li><b>Define:</b> What is the characteristic polynomial?</li>\n  <li><b>Derivation:</b> How is it derived from the eigenvalue equation $Ax = \\lambda x$?</li>\n</ul>",
    "id": 1741075245856,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 2
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": false,
        "reason": "O(n**3)",
        "timestamp": 1741834575261
      },
      {
        "correct": true,
        "timestamp": 1741834578886
      },
      {
        "correct": true,
        "timestamp": 1741834581321
      }
    ],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nGauss elimination is a systematic method for solving systems of linear equations. It involves applying elementary row operations to an augmented matrix to transform it into row echelon form (or reduced row echelon form), from which the solutions can be obtained through back substitution.\n</p>\n<br>\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Write the augmented matrix for the system of equations.</li>\n  <li>Apply elementary row operations:\n    <ul>\n      <li>Swap rows.</li>\n      <li>Multiply a row by a nonzero scalar.</li>\n      <li>Add or subtract a multiple of one row from another.</li>\n    </ul>\n  </li>\n  <li>Reduce the matrix to row echelon form (upper triangular form).</li>\n  <li>(Optional) Further reduce to reduced row echelon form for a unique solution.</li>\n  <li>Perform back substitution to solve for the variables.</li>\n</ol>",
    "front": "<b>Gauss Elimination</b> \n<ul>\n  <li><b>Define:</b> What is Gauss elimination?</li>\n  <li><b>Procedure:</b> How is Gauss elimination performed?</li>\n</ul>",
    "id": 1740890812452,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<ul>\n  <li>Denoted $rref(A)$</li>\n  <li>Rules:\n    <ol>\n      <li>Each leading 1 (pivot) is the only nonzero entry in its column.</li>\n      <li>The leading 1 in each row appears to the right of the leading 1 in the row above.</li>\n      <li>Any rows of all zeros appear in the bottom rows of the matrix.</li>\n    </ol>\n  </li>\n  <li>The first $r$ pivot columns form an identity-like structure.</li>\n  <li>The remaining $n - r$ columns, denoted as $F$, contain the free variables.</li>\n</ul>",
    "front": "Reduced row echelon form",
    "id": 1740887871355,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "To solve $Ax = b$ is to <u>express b as a linear combination of the columns of $A$.</u>",
    "front": "To solve $Ax = b$ is to ____.",
    "id": 1740886790619,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "The equations $Ax = b$ are solvable iff <u>${b}$ is in the column space of $A$</u>",
    "front": "The equations $Ax = b$ are solvable iff ____.",
    "id": 1740887017912,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 2
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Answer</b></u><br>\n<p>\nThe inner product (dot product) of two vectors \\( v \\) and \\( q \\) is:\n$$\n\\langle v, q \\rangle = v^T q\n$$\n<br>\nwhere \\( v^T \\) is the transpose of \\( v \\), making it a row vector.\n</p>\n<p>\n<br>\nIf \\( v \\) and \\( q \\) are column vectors: <br>\n$$\nv =\n\\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\n\\vdots \\\\\nv_n\n\\end{bmatrix}\n, \\quad\nq =\n\\begin{bmatrix}\nq_1 \\\\\nq_2 \\\\\n\\vdots \\\\\nq_n\n\\end{bmatrix}\n$$\n<br><br>\nThen their inner product is computed as: <br>\n$$\n\\langle v, q \\rangle =\n\\begin{bmatrix}\nv_1 & v_2 & \\dots & v_n\n\\end{bmatrix}\n\\begin{bmatrix}\nq_1 \\\\\nq_2 \\\\\n\\vdots \\\\\nq_n\n\\end{bmatrix}\n=\nv_1 q_1 + v_2 q_2 + \\dots + v_n q_n\n$$\n</p>",
    "front": "<b>Inner / Dot Product: </b> \\( \\langle v, q \\rangle = \\; ? \\)",
    "id": 1741812943588,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "Inner product / dot product as implied by the multiplication of a transposed column vector $1 \\times n$ and a column vector $n \\times 1$.<br><br>\n\nThe dot product is given by: <br><br>\n$$\n\\mathbf{a}^\\top \\mathbf{b} =\n\\begin{bmatrix} a_1 & a_2 & \\dots & a_n \\end{bmatrix}\n\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{bmatrix} =\n\\sum_{i=1}^{n} a_i b_i\n$$",
    "front": "Let  $a$  and  $b$  be column vectors in $\\mathbb{R}^n$. <br><br>\n\nWhat is $a^Tb$ ?\n",
    "id": 1740895695348,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741721940878
      }
    ],
    "back": "<u><b>Answer</b></u><br>\n<p>\nIf \\( A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\),  \nthen its determinant is given by: <br>\n$$\n\\det(A) = a d - b c\n$$\n<br>\n<br>\n<ul>\n    <li>\\( A \\) is invertible iff \\( \\det(A) \\neq 0 \\); if \\( \\det(A) = 0 \\), then \\( A \\) has NO inverse.</li>\n    <li>Represents the area scaling factor of the transformation described by \\( A \\).</li>\n</ul>\n</p>",
    "front": "<b>Determinant of a $2 \\times 2$ Matrix</b>\n<p>Let \\( A \\in \\mathbb{R}^{2 \\times 2} \\), then \\( \\det(A) = ? \\)</p>",
    "id": 1741721369131,
    "tags": [
      "18.065",
      "midterm1",
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741722270852
      }
    ],
    "back": "<u><b>Answer</b></u><br>\n<p>\nIf \\( A = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} \\), \nthen its determinant is given by: <br>\n$$\n\\det(A) = a(ei - fh) - b(di - fg) + c(dh - eg)\n$$\n<br>\n<br>\n<ul>\n    <li>\\( A \\) is invertible iff \\( \\det(A) \\neq 0 \\); if \\( \\det(A) = 0 \\), then \\( A \\) has no inverse.</li>\n    <li>Measures the volume scaling factor of the transformation described by \\( A \\); if \\( \\det(A) = 0 \\), the transformation collapses 3D space into a lower-dimensional subspace.</li>\n</ul>\n</p>",
    "front": "<b>Determinant of a $3 \\times 3$ Matrix</b>\n<p>Let \\( A \\in \\mathbb{R}^{3 \\times 3} \\), then \\( \\det(A) = ? \\)</p>",
    "id": 1741721412394,
    "tags": [
      "18.065",
      "midterm1",
      "16.32",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741722887911
      },
      {
        "correct": true,
        "timestamp": 1741723683058
      }
    ],
    "back": "<u><b>Answer</b></u><br>\n<p>\nThe \\( ij \\)th minor of \\( A \\), denoted as \\( M_{ij} \\), is defined as: <br>\n$$\nM_{ij} = \\det(A_{ij})\n$$\n<br>\nwhere \\( A_{ij} \\) is the \\((n-1) \\times (n-1)\\) submatrix obtained by removing the \\( i \\)th row and \\( j \\)th column from \\( A \\). <br>\n<br>\n<ul>\n    <li>Minors are used to compute determinants of larger matrices through cofactor expansion.</li>\n    <li>They are essential in defining cofactors, which are used in matrix inverses and adjugates.</li>\n</ul>\n</p>",
    "front": "<b>Minor of a Matrix</b>\n<p>Suppose \\( A \\) is an \\( n \\times n \\) matrix. What is the \\( ij \\)th minor of \\(A\\), denoted by \\( M_{ij} \\)?</p>",
    "id": 1741722586077,
    "tags": [
      "18.065",
      "midterm1",
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": false,
        "reason": "missed det(M_ij)",
        "timestamp": 1741722492320
      },
      {
        "correct": true,
        "timestamp": 1741723678165
      }
    ],
    "back": "<u><b>Answer</b></u><br>\n<p>\nThe \\( ij \\)th cofactor of \\( A \\), denoted as \\( C_{ij} \\), is defined as: <br>\n$$\nC_{ij} = (-1)^{i+j} \\det(M_{ij})\n$$\n<br>\nwhere \\( M_{ij} \\) is the \\((n-1) \\times (n-1)\\) minor of \\( A \\), obtained by removing the \\( i \\)th row and \\( j \\)th column from \\( A \\). <br>\n<br>\n<ul>\n    <li>Cofactors are used in the computation of determinants via cofactor expansion.</li>\n    <li>They are also used to compute the adjugate of a matrix, which helps in finding matrix inverses.</li>\n</ul>\n</p>",
    "front": "<b>Cofactor of a Matrix</b>\n<p>Suppose \\( A \\) is an \\( n \\times n \\) matrix. What is the \\( ij \\)th cofactor of \\(A\\), denoted by \\( C_{ij} \\)?</p>",
    "id": 1741722346863,
    "tags": [
      "18.065",
      "midterm1",
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741723494708
      },
      {
        "correct": false,
        "reason": "fixed row i where i = 1, ..., n",
        "timestamp": 1741723622137
      },
      {
        "correct": true,
        "timestamp": 1741723626966
      }
    ],
    "back": "<u><b>Answer</b></u><br>\n<p>\nThe determinant of \\( A \\) can be computed using <b>cofactor expansion along the \\( i \\)th row</b>, where \\( i \\) is a <b>fixed</b> row chosen from \\( \\{1,2, \\dots, n\\} \\): <br>\n$$\n\\det(A) = \\sum_{j=1}^{n} a_{ij} C_{ij} = a_{i1} C_{i1} + a_{i2} C_{i2} + \\cdots + a_{in} C_{in}.\n$$\n<br>\n<ul>\n    <li>To compute \\( \\det(A) \\), you choose a single row \\( i \\) and apply the expansion formula to that row.</li>\n    <li>Cofactor expansion can be done on columns too.</li>\n</ul>\n</p>",
    "front": "<b>Cofactor Expansion Theorem</b>\n<p>Let \\( A \\) be an \\( n \\times n \\) matrix. How can we compute \\( \\det(A) \\) using cofactor expansion along the \\( i \\)th row?</p>",
    "id": 1741723147170,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741724090802
      }
    ],
    "back": "<u><b>Answer</b></u><br>\n<p>\nFor an \\( n \\times n \\) matrix \\( A = [a_{ij}] \\), the determinant is computed using <b>cofactor expansion along the \\( i \\)th row</b>, where \\( i \\) is a fixed row chosen from \\( \\{1,2, \\dots, n\\} \\): <br>\n$$\n\\det(A) = \\sum_{j=1}^{n} (-1)^{i+j} a_{ij} \\det(M_{ij})\n$$\n<br>\nwhere \\( \\det(M_{ij}) \\) is the determinant of the \\((n-1) \\times (n-1)\\) minor obtained by removing row \\( i \\) and column \\( j \\). <br>\n<ul>\n    <li>\\( A \\) is invertible if and only if \\( \\det(A) \\neq 0 \\); if \\( \\det(A) = 0 \\), then \\( A \\) has no inverse.</li>\n    <li>The determinant helps determine volume scaling in higher-dimensional spaces; if \\( \\det(A) = 0 \\), the transformation collapses space into a lower-dimensional subspace.</li>\n</ul>\n</p>",
    "front": "<b>Determinant of an \\( n \\times n \\) Matrix</b>\n<p>Let \\( A \\in \\mathbb{R}^{n \\times n} \\) for \\( n > 3 \\), then \\( \\det(A) = ? \\).</p>",
    "id": 1741721757550,
    "tags": [
      "18.065",
      "midterm1",
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741724700376
      }
    ],
    "back": "<u><b>Answer</b></u><br>  \n<p>  \nThe <b>adjugate matrix</b> of an \\( n \\times n \\) matrix \\( A \\), denoted as \\( \\text{adj}(A) \\), is the transpose of the cofactor matrix of \\( A \\): <br>  \n$$  \n\\text{adj}(A) = C^T  \n$$  \n<br>  \nwhere \\( C \\) is the <b>cofactor matrix</b> of \\( A \\), whose entries are the cofactors \\( C_{ij} \\). <br>  \n<br>  \n<ul>  \n    <li>The adjugate is used to compute the inverse of \\( A \\) when \\( A \\) is invertible:  <br>\n    $$  \n    A^{-1} = \\frac{1}{\\det(A)} \\text{adj}(A), \\quad \\text{if } \\det(A) \\neq 0.  \n    $$  \n    </li>  \n    <li>The adjugate appears in applications such as solving linear systems using Cramer's Rule.</li>  \n</ul>  \n</p>  ",
    "front": "<b>Adjugate Matrix</b>  \n<p>Let \\( A \\) be an \\( n \\times n \\) matrix. What is the adjugate of \\( A \\), denoted as \\( \\text{adj}(A) \\)?</p>  ",
    "id": 1741724144493,
    "tags": [
      "18.065",
      "midterm1",
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Answer</b></u><br>\n<p>\nIf \\( A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\), then its inverse is given by: <br>\n$$\nA^{-1} = \\frac{1}{a d - b c} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n$$\n<br>\nprovided that \\( \\det(A) = a d - b c \\neq 0 \\).\n</p>",
    "front": "<b>Inverse of a $2 \\times 2$ Matrix</b>\n<p>Let \\( A \\in \\mathbb{R}^{2 \\times 2} \\), then \\( A^{-1} = ? \\)</p>",
    "id": 1741721080206,
    "tags": [
      "18.065",
      "midterm1",
      "16.32",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Answer</b></u><br>\n<p>\nIf \\( A = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} \\), <br>\nthen its inverse is given by: <br>\n$$\nA^{-1} = \\frac{1}{\\det(A)} \\text{Adj}(A)\n$$\n<br>\nwhere \\( \\det(A) \\) is the determinant of \\( A \\), and \\( \\text{Adj}(A) \\) is the adjugate (transpose of the cofactor matrix). <br>\n\\( A^{-1} \\) exists if and only if \\( \\det(A) \\neq 0 \\).\n</p>",
    "front": "<b>Inverse of a $3 \\times 3$ Matrix</b>\n<p>Let \\( A \\in \\mathbb{R}^{3 \\times 3} \\), then \\( A^{-1} = ? \\)</p>",
    "id": 1741721296491,
    "tags": [
      "18.065",
      "midterm1",
      "16.32",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Answer</b></u><br>  \n<p>  \nThe inverse of an \\( n \\times n \\) matrix \\( A \\), denoted as \\( A^{-1} \\), exists iff \\( \\det(A) \\neq 0 \\). It is given by: <br>  \n$$  \nA^{-1} = \\frac{1}{\\det(A)} \\text{adj}(A)  \n$$  \n<br>  \nwhere \\( \\text{adj}(A) \\) is the <b>adjugate matrix</b> of \\( A \\) (the transpose of the cofactor matrix). <br>  \n<br>  \n<ul>  \n    <li>\\( A \\) is invertible iff \\( \\det(A) \\neq 0 \\); otherwise, \\( A^{-1} \\) does not exist.</li>\n</ul>  \n</p>  ",
    "front": "<b>Inverse of an \\( n \\times n \\) Matrix</b>  \n<p>Let \\( A \\) be an \\( n \\times n \\) matrix. How do we compute \\( A^{-1} \\) when it exists?</p>  ",
    "id": 1741724759280,
    "tags": [
      "18.065",
      "midterm1",
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nA square matrix $A$ is <b>invertible</b> (or nonsingular) if there exists a matrix $B$ such that:\n$$\nAB = BA = I\n$$\nwhere $I$ is the identity matrix.\n</p>\n<br>\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Ensure that $A$ is square.</li>\n  <li>Check if the determinant $\\det(A)$ is nonzero. If $\\det(A) \\neq 0$, then $A$ is invertible.</li>\n  <li>If invertible, compute the inverse $A^{-1}$ using methods like Gaussian elimination or the adjugate formula.</li>\n</ol>",
    "front": "<b>Invertible Matrix</b> \n<ul>\n  <li><b>Define:</b> What is an invertible matrix?</li>\n  <li><b>Practical Use:</b> Why is invertibility important?</li>\n  <li><b>Procedure:</b> How do we determine if a matrix is invertible?</li>\n</ul>",
    "id": 1740887674549,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 2
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nA square matrix $A$ is called <b>singular</b> if it is not invertible. This is equivalent to:\n$$\n\\det(A) = 0\n$$\n</p>\n<br>\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Ensure that $A$ is square.</li>\n  <li>Calculate the determinant $\\det(A)$. If $\\det(A) = 0$, then $A$ is singular.</li>\n  <li>Interpretation: A singular matrix has linearly dependent columns (or rows) and does not possess an inverse.</li>\n</ol>",
    "front": "<b>Singular Matrix</b> \n<ul>\n  <li><b>Define:</b> What is a singular matrix?</li>\n  <li><b>Practical Use:</b> What does it imply when a matrix is singular?</li>\n  <li><b>Procedure:</b> How do we determine if a matrix is singular?</li>\n</ul>",
    "id": 1740887684118,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nA square matrix $A$ is <b>nonsingular</b> if it is invertible, meaning there exists a matrix $B$ such that:\n$$\nAB = BA = I\n$$\nEquivalently, $A$ is nonsingular if:\n$$\n\\det(A) \\neq 0\n$$\n</p>\n<br>\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Confirm that $A$ is a square matrix.</li>\n  <li>Compute $\\det(A)$. If $\\det(A) \\neq 0$, then $A$ is nonsingular.</li>\n  <li>If nonsingular, an inverse $A^{-1}$ exists and can be computed.</li>\n</ol>",
    "front": "<b>Nonsingular Matrix</b> \n<ul>\n  <li><b>Define:</b> What is a nonsingular matrix?</li>\n  <li><b>Practical Use:</b> Why is nonsingularity important?</li>\n  <li><b>Procedure:</b> How do we verify if a matrix is nonsingular?</li>\n</ul>",
    "id": 1741074715967,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741514390091
      }
    ],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nA set of vectors $\\{v_1, v_2, \\dots, v_n\\}$ is called an <b>orthonormal set</b> if the vectors are both <b>orthogonal</b> (mutually perpendicular) and <b>normalized</b> (each has unit length). This means:\n$$\nv_i^T v_j =\n\\begin{cases}\n1, & \\text{if } i = j \\quad (\\text{each vector has unit length}) \\\\\n0, & \\text{if } i \\neq j \\quad (\\text{vectors are orthogonal})\n\\end{cases}\n$$\n</p>\n\n<br>\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Compute the dot product of each pair of vectors. If all pairs are orthogonal (dot product = 0 for distinct vectors), continue.</li>\n  <li>Check that each vector has unit length: $||v_i|| = 1$ for all $i$.</li>\n  <li>If both conditions hold, the set is orthonormal.</li>\n</ol>\n\n<br>\n<u><b>Practical Use</b></u> <br>\n<p>\nOrthonormal sets are widely used in:\n<ul>\n  <li>Orthogonal transformations (e.g., rotations, reflections)</li>\n  <li>QR decomposition in linear algebra</li>\n  <li>Fourier series and signal processing</li>\n  <li>Eigenvector bases in quantum mechanics</li>\n</ul>\n</p>",
    "front": "<b>Orthonormal Set of Vectors</b> \n<ul>\n  <li><b>Define:</b> What is an orthonormal set of vectors?</li>\n  <li><b>Procedure:</b> How do we check if a set of vectors is orthonormal?</li>\n  <li><b>Practical Use:</b> Where are orthonormal sets used?</li>\n</ul>",
    "id": 1741513526904,
    "tags": [
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": false,
        "reason": "Orthonormality",
        "timestamp": 1741832531124
      },
      {
        "correct": true,
        "timestamp": 1741835217104
      },
      {
        "correct": true,
        "timestamp": 1741835385960
      }
    ],
    "back": "<u><b>Definition</b></u> <br>\n<p>\nA square matrix $Q$ is called <b>orthogonal</b> if its column vectors (and row vectors) form an orthonormal set. This is equivalent to: <br>\n$$\nQ^T Q = QQ^T = I\n$$\n<br>\nwhere $I$ is the identity matrix. <br><br>\n\nThis directly implies that the inverse of $Q$ is its transpose, i.e., <br>\n$$\nQ^{-1} = Q^T.\n$$\n</p>\n<br>\n\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Check that $Q$ is a square matrix.</li>\n  <li>Compute $Q^T Q$. If $Q^T Q = I$, then $Q$ is orthogonal.</li>\n  <li>Alternatively, verify that the columns of $Q$ are orthonormal (i.e., each column has unit length and is orthogonal to the others).</li>\n</ol>",
    "front": "<b>Orthogonal Matrix</b> \n<ul>\n  <li><b>Define:</b> What is an orthogonal matrix?</li>\n  <li><b>Procedure:</b> How do we check if a matrix is orthogonal?</li>\n</ul>",
    "id": 1740887687921,
    "tags": [
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": false,
        "timestamp": 1741720149217
      },
      {
        "correct": true,
        "timestamp": 1741720153010
      },
      {
        "correct": true,
        "timestamp": 1741720155394
      },
      {
        "correct": true,
        "timestamp": 1741720156552
      },
      {
        "correct": false,
        "timestamp": 1741835329328
      },
      {
        "correct": true,
        "timestamp": 1741835505832
      },
      {
        "correct": false,
        "timestamp": 1741835508456
      },
      {
        "correct": true,
        "timestamp": 1741835513802
      }
    ],
    "back": "<u><b>Definition</b></u><br>\n<p>\nA <b>Householder matrix</b> is a symmetric, orthogonal matrix used for transforming vectors and matrices in numerical linear algebra. It is defined as: <br>\n$$\nH = I - 2vv^T, \\quad H \\in \\mathbb{R}^{n \\times n}, \\quad  v \\in \\mathbb{R}^n \\setminus\\{0\\}\n$$\n<br>\nwhere:\n<ul>\n    <li><b>$I$</b> is the $n \\times n$ identity matrix.</li>\n    <li><b>$v$</b> is often chosen as a unit vector (i.e., \\( ||v|| = 1 \\)).</li>\n    <li><b>$vv^T$</b> forms an $n \\times n$ rank-one projection matrix.</li>\n</ul>\n</p>\n<br>\n<p>\nMore generally, if \\( v \\) is not necessarily a unit vector, the Householder matrix is given by: <br>\n$$\nH = I - 2\\frac{vv^T}{v^T v}, \\quad v^T v \\neq 0\n$$\n<br>\nwhich ensures the transformation remains valid for any nonzero vector \\( v \\).\n</p>\n<br>\n\n<u><b>Properties</b></u><br>\n<ul>\n    <li><b>Orthogonal:</b> \\( H^T H = I \\), meaning \\( H^{-1} = H \\) (it is its own inverse).</li>\n    <li><b>Symmetric:</b> \\( H^T = H \\).</li>\n    <li><b>Reflection Transformation:</b> It reflects a vector about a hyperplane perpendicular to \\( v \\).</li>\n</ul>\n<br>\n\n<u><b>Practical Use</b></u> <br>\n<p>\nWe use Householder matrices for numerical linear algebra, especially for QR factorization and for reducing matrices to tridiagonal or Hessenberg forms.\n</p>\n<br>\n<p>\n<b>TODO: add pset + lecture 02/11 02/13 stuff stuff</b>\n</p>\n<br>\n\n<u><b>Procedure</b></u> <br>\n<ol>\n  <li>Select a nonzero vector $v \\in \\mathbb{R}^n$.</li>\n  <li>Compute the Householder matrix using:\n      $$\n      H = I - 2\\frac{vv^T}{v^T v}\n      $$\n  </li>\n  <li>Use $H$ to reflect a given vector or to zero out subdiagonal elements during matrix factorization.</li>\n</ol>",
    "front": "<b>Householder Matrix</b> \n<ul>\n  <li><b>Define:</b> What is a Householder matrix?</li>\n  <li><b>Properties:</b> What are its key properties?</li>\n  <li><b>Practical Use:</b> What are Householder matrices used for?</li>\n  <li><b>Procedure:</b> How is a Householder matrix constructed?</li>\n</ul>",
    "id": 1741075039903,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Let $H = I - 2uu^T \\in \\mathbb{R}^{n \\times n}$ be a Householder matrix and let $X \\in \\mathbb{R}^{n \\times n}$. Given the special structure of $H$, can you propose a method for computing $HX$ whose computational complexity is $O(n^2)$?",
    "id": 1741714530534,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741819743765
      },
      {
        "correct": false,
        "timestamp": 1741819745453
      },
      {
        "correct": true,
        "timestamp": 1741819746154
      },
      {
        "correct": true,
        "timestamp": 1741819747708
      },
      {
        "correct": true,
        "timestamp": 1741819748882
      },
      {
        "correct": false,
        "timestamp": 1741819751657
      },
      {
        "correct": true,
        "timestamp": 1741819753055
      },
      {
        "correct": false,
        "reason": "r_ij formula",
        "timestamp": 1741829357347
      }
    ],
    "back": "<u><b>Definition</b></u><br>\n<p>\nThe <b>Gram-Schmidt Orthogonalization</b> process is a method for converting a set of linearly independent vectors into an orthonormal basis for a subspace. Given a set of vectors \n$$\n\\{a_1, a_2, \\dots, a_n\\}\n$$ \nin an inner product space, the process constructs an orthonormal set \n$$\n\\{q_1, q_2, \\dots, q_n\\}\n$$ \nsuch that each new vector is orthogonal to the previous ones.\n</p>\n<br>\n\n<u><b>Procedure</b></u><br>\n<ol>\n  <li>Start with a set of linearly independent vectors \\( \\{ a_1, a_2, ..., a_n \\} \\).</li>\n  <li>Define the first orthonormal vector as: <br>\n  $$ \n  q_1 = \\frac{a_1}{\\|a_1\\|}\n  $$</li>\n  <li>For each subsequent vector \\( a_i \\), subtract its projection onto all previous orthonormal vectors: <br>\n  $$\n  u_i = a_i - \\sum_{j=1}^{i-1} (a_i^T q_j) q_j\n  $$</li>\n  <li>Normalize \\( u_i \\) to get an orthonormal vector: <br>\n  $$ \n  q_i = \\frac{u_i}{\\|u_i\\|}\n  $$</li>\n  <li>Compute the upper triangular matrix elements: <br>\n  $$\n  r_{i,j} = a_j^T q_i, \\quad \\text{for } i \\leq j\n  $$\n  $$\n  r_{i,j} = 0, \\quad \\text{for } i > j\n  $$</li>\n  <li>Repeat until all vectors are processed.</li>\n</ol>\n<br>\n\n<u><b>Algorithm Pseudocode</b></u><br>\n<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/gram-schmidt_pseudocode.png\"></div>\n<br>\n\n<u><b>Practical Use</b></u><br>\n<p>\nGram-Schmidt Orthogonalization is widely used in:\n<ul>\n    <li>Generating orthonormal bases in linear algebra.</li>\n    <li>QR factorization in numerical linear algebra.</li>\n    <li>Signal processing and machine learning applications.</li>\n</ul>\n</p>",
    "front": "<b>Gram-Schmidt Orthogonalization</b>\n<ul>  \n    <li><b>Define:</b> What is Gram-Schmidt Orthogonalization?</li>\n    <li><b>Procedure:</b> How do you perform Gram-Schmidt Orthogonalization?</li>\n    <li><b>Practical Use:</b> Where/when is Gram-Schmidt Orthogonalization used?</li>\n</ul>",
    "id": 1741733840654,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Definition</b></u><br>\n<p>\nThe <b>Householder Reflection</b> method is an alternative orthogonalization process that constructs an orthonormal basis using reflections instead of projections. Given a set of vectors  \n$$\n\\{a_1, a_2, \\dots, a_n\\}\n$$  \nin an inner product space, this process applies a sequence of orthogonal reflections to transform the original set into an orthonormal basis  \n$$\n\\{q_1, q_2, \\dots, q_n\\}\n$$  \nwhile ensuring numerical stability.\n</p>\n<br>\n\n<u><b>Procedure</b></u><br>\n<ol>\n  <li>Start with a set of linearly independent vectors \\( A = [a_1, a_2, ..., a_n] \\).</li>\n  <li>For each column \\( a_k \\), construct a reflection that zeroes out the elements below the diagonal.</li>\n  <li>Define the Householder vector \\( v_k \\) for each step as: <br>\n  $$  \n  v_k = a_k + \\text{sign}(a_{k, k}) \\|a_k\\| e_k  \n  $$  \n  where \\( e_k \\) is the \\( k \\)-th standard basis vector.</li>\n  <li>Normalize \\( v_k \\): <br>\n  $$  \n  v_k = \\frac{v_k}{\\|v_k\\|}  \n  $$</li>\n  <li>Construct the Householder reflection matrix: <br>\n  $$  \n  H_k = I - 2 v_k v_k^T  \n  $$</li>\n  <li>Apply the transformation iteratively to zero out subdiagonal entries: <br>\n  $$  \n  A' = H_k A  \n  $$  \n  updating \\( A \\) at each step.</li>\n  <li>After \\( n \\) steps, the resulting matrix contains the orthonormal vectors as its columns.</li>\n</ol>\n<br>\n\n<u><b>Practical Use</b></u><br>\n<p>\nHouseholder reflections are widely used in:\n<ul>\n    <li>QR decomposition in numerical linear algebra.</li>\n    <li>Least squares problem solving.</li>\n    <li>Eigenvalue computations and matrix factorizations.</li>\n</ul>\n</p>",
    "front": "<b>Householder Reflection-Based Orthogonalization</b>\n<ul>  \n    <li><b>Define:</b> What is Householder Reflection-Based Orthogonalization?</li>\n    <li><b>Procedure:</b> How do you perform Householder Reflection-Based Orthogonalization?</li>\n    <li><b>Practical Use:</b> Where/when is Householder Reflection-Based Orthogonalization used?</li>\n</ul>",
    "id": 1741823154481,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Application of QR decomposition to least squares",
    "id": 1741831109041,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741514620699
      },
      {
        "correct": false,
        "timestamp": 1741515337941
      },
      {
        "correct": true,
        "timestamp": 1741515338393
      },
      {
        "correct": true,
        "timestamp": 1741515343329
      }
    ],
    "back": "<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/five_factorizations.jpeg\"></div>\n\n<ol>\n  <li>Column-Row Factorization (CR): \\( A = CR \\)</li>\n  <li>Lower-Upper Factorization (LU): \\( A = LU \\) or \\( PA = LU \\)</li>\n  <li>Orthogonal-Triangular Factorization (QR): \\( A = QR \\)</li>\n  <li>Eigenvalue Decomposition (Spectral Decomposition): \n    <ul>\n      <li>General: \\( A = X \\Lambda X^{-1} \\)</li>\n      <li>Symmetric: \\( A = Q \\Lambda Q^T \\)</li>\n    </ul>\n  </li>\n  <li>Singular Value Decomposition (SVD): \\( A = U \\Sigma V^T \\)</li>\n</ol>",
    "front": "<b>The Five Factorizations of a Matrix</b>",
    "id": 1740884244261,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<b><u>Definition</u></b> <br>  \nA set of vectors \\( \\{ v_1, v_2, \\dots, v_n \\} \\) in \\( \\mathbb{R}^n \\) (or \\( \\mathbb{C}^n \\)) satisfies the <b>orthonormality condition</b> if:  \n<ol>  \n    <li>Each vector is <b>unit length</b> (normalized):  \n       \\[\n       \\| v_i \\| = 1, \\quad \\forall i.\n       \\]  \n    </li>  \n    <li>The vectors are <b>mutually orthogonal</b>:  \n       \\[\n       v_i^T v_j = 0, \\quad \\text{for } i \\neq j.\n       \\]  \n       This means that the dot product (or inner product) between distinct vectors is zero, and each vector has a norm of 1.  \n    </li>  \n</ol>  \n<br>  \n\n<b><u>Properties</u></b> <br>  \n<ul>  \n    <li>If a matrix \\( Q \\) has orthonormal columns, then \\( Q^T Q = I \\), making it an orthogonal (or unitary) matrix.</li>  \n    <li>Orthonormal vectors simplify computations in linear algebra, especially in QR factorization and least squares problems.</li>  \n    <li>For an orthonormal basis of \\( \\mathbb{R}^n \\), any vector \\( x \\) can be uniquely written as a linear combination of the basis vectors.</li>  \n    <li>In function spaces, orthonormal functions play a key role in Fourier analysis.</li>  \n</ul>  ",
    "front": "<b>Orthonormality Condition</b>  \n<ul>  \n    <li><b>Define:</b> What is the orthonormality condition for a set of vectors?</li>  \n    <li><b>Properties:</b> What are the key characteristics of an orthonormal set of vectors?</li>  \n</ul>  ",
    "id": 1741168096548,
    "tags": [
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741847755782
      }
    ],
    "back": "<b><u>Definition</u></b> <br>  \nQR factorization is a decomposition of a matrix \\( A \\in \\mathbb{R}^{m \\times n} \\) into the product of an orthogonal matrix \\( Q \\) and an upper triangular matrix \\( R \\):  \n\\[\nA = QR,\n\\]\n<br>\nwhere:  <br>\n- \\( Q \\in \\mathbb{R}^{m \\times m} \\) is an orthogonal matrix (\\( Q^T Q = I \\)).\n<ul>\n    <li>\n    \\( R \\in \\mathbb{R}^{m \\times n} \\) is an upper triangular matrix.  \n     </li>\n</ul>\n<br>\n\n<b><u>Applications</u></b> <br>  \n<ul>  \n    <li>Solving linear systems efficiently.</li>  \n    <li>Computing eigenvalues and eigenvectors (QR algorithm).</li>  \n    <li>Least squares approximation in regression problems.</li>  \n    <li>Numerical stability in iterative methods.</li>  \n</ul>  \n<br>  \n\n<b><u>Procedure</u></b> <br>  \n<ol>  \n    <li>Start with a matrix \\( A \\in \\mathbb{R}^{m \\times n} \\).</li>  \n    <li>Use the <b>Gram-Schmidt process</b> or <b>Householder reflections</b> to construct an orthogonal matrix \\( Q \\).</li>  \n    <li>Compute \\( R = Q^T A \\), which results in an upper triangular matrix.</li>  \n    <li>The decomposition satisfies \\( A = QR \\).</li>  \n</ol>",
    "front": "<b>QR Factorization</b>  \n<ul>  \n    <li><b>Define:</b> What is QR factorization?</li>  \n    <li><b>Applications:</b> Where is QR factorization used?</li>  \n    <li><b>Procedure:</b> How to compute QR factorization?</li>  \n</ul>  ",
    "id": 1740890854848,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<b><u>QR Factorization Result</u></b> <br>\nFor the matrix: <br>\n\\[\nA =\n\\begin{bmatrix}\n2 & 2 \\\\\n0 & 3\n\\end{bmatrix},\n\\] <br>\nwe can choose: <br>\n\\[\nQ =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix},\n\\quad\nR =\n\\begin{bmatrix}\n2 & 2 \\\\\n0 & 3\n\\end{bmatrix}.\n\\] <br><br>\nThus, \\( A = QR \\).",
    "front": "<b>Factor matrix \\( A \\) into \\( QR \\)</b> <br>  \n\\[\nA =\n\\begin{bmatrix}\n2 & 2 \\\\\n0 & 3\n\\end{bmatrix}\n\\]",
    "id": 1741185849308,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<b><u>QR Factorization Result</u></b> <br>\nFor the matrix: <br>\n\\[\nA =\n\\begin{bmatrix}\n2 & 3 & 1 \\\\\n0 & 4 & 5 \\\\\n0 & 0 & 6\n\\end{bmatrix},\n\\] <br>\nwe can choose: <br>\n\\[\nQ =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix},\n\\quad\nR =\n\\begin{bmatrix}\n2 & 3 & 1 \\\\\n0 & 4 & 5 \\\\\n0 & 0 & 6\n\\end{bmatrix}.\n\\] <br><br>\nThus, \\( A = QR \\).",
    "front": "<b>Factor matrix \\( A \\) into \\( QR \\)</b> <br>  \n\\[\nA =\n\\begin{bmatrix}\n2 & 3 & 1 \\\\\n0 & 4 & 5 \\\\\n0 & 0 & 6\n\\end{bmatrix}\n\\]",
    "id": 1741185873373,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<b><u>Definition</u></b> <br>  \nCR factorization is a matrix decomposition technique where a given matrix \\( A \\) is factored into the product of two matrices:  <br>\n$$  \nA = CR\n$$  \n<br>\nwhere \\( C \\) is a matrix with orthonormal columns (often computed via Gram-Schmidt or QR-like processes), and \\( R \\) is an upper triangular matrix.\n<br><br>  \n\n<b><u>Applications</u></b> <br>  \n<ul>  \n  <li>Used in numerical linear algebra for solving least squares problems.</li>  \n  <li>Helps in reducing computational complexity in iterative methods.</li>  \n  <li>Applied in signal processing and machine learning for matrix approximations.</li>  \n</ul>  \n<br>  \n\n<b><u>Procedure</u></b> <br>  \n<ol>  \n  <li>Start with a given matrix \\( A \\) (typically an \\( m \\times n \\) matrix).</li>  \n  <li>Compute the matrix \\( C \\) whose columns form an orthonormal basis for the column space of \\( A \\).</li>  \n  <li>Compute the matrix \\( R \\) as the product \\( C^T A \\), which results in an upper triangular matrix.</li>  \n  <li>The decomposition satisfies \\( A = CR \\), where \\( C^T C = I \\) (orthonormality condition).</li>  \n</ol>  ",
    "front": "<b>CR Factorization</b>  \n<ul>  \n  <li><b>Define:</b> What is CR factorization?</li>  \n  <li><b>Applications:</b> Where is CR factorization used?</li>  \n  <li><b>Procedure:</b> How is CR factorization performed?</li>  \n</ul>  ",
    "id": 1741167809635,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<b><u>CR Factorization Result</u></b> <br>\nFor the matrix: <br>\n\\[\nA =\n\\begin{bmatrix}\n1 & 2 \\\\\n0 & 3\n\\end{bmatrix},\n\\] <br>\nwe can choose: <br>\n\\[\nC =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} \\quad \\text{(the identity matrix, which is orthonormal)},\n\\]\nand\n\\[\nR =\n\\begin{bmatrix}\n1 & 2 \\\\\n0 & 3\n\\end{bmatrix} \\quad \\text{(an upper triangular matrix with integer elements)}.\n\\] <br><br>\nThus, \\(A = CR\\).",
    "front": "<b>Factor matrix \\( A \\) into \\( CR \\)</b> <br>  \n\\[\nA =\n\\begin{bmatrix}\n1 & 2 \\\\\n0 & 3\n\\end{bmatrix}\n\\]",
    "id": 1741168186959,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "<b>Permutation Matrix</b>",
    "id": 1741165893792,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741835683974
      }
    ],
    "back": "<b><u>Definition</u></b> <br>  \nLU decomposition is the factorization of a square matrix \\( A \\) into the product of a lower triangular matrix \\( L \\) and an upper triangular matrix \\( U \\), such that:  <br>\n\\[\nA = LU.\n\\] \n\n<br><br>  \n\n<b><u>Example</u></b> <br>  \nFor a \\( 3 \\times 3 \\) matrix:  <br>\n\\[\nA =\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\\]\n<br>\nLU decomposition gives:  <br>\n\\[\nL =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\nl_{21} & 1 & 0 \\\\\nl_{31} & l_{32} & 1\n\\end{bmatrix},\n\\quad\nU =\n\\begin{bmatrix}\nu_{11} & u_{12} & u_{13} \\\\\n0 & u_{22} & u_{23} \\\\\n0 & 0 & u_{33}\n\\end{bmatrix}\n\\]  \n\n<br><br>\n\n<b><u>Applications</u></b> <br>  \n<ol>  \n  <li>Solving linear systems efficiently using forward and backward substitution.</li>  \n  <li>Computing matrix determinants as \\( \\det(A) = \\det(L) \\det(U) \\).</li>  \n  <li>Matrix inversion by solving multiple systems efficiently.</li>  \n  <li>Numerical methods, such as optimization and differential equations.</li>  \n</ol>  \n\n<br>\n\n<b><u>Procedure</u></b> <br>  \nLU decomposition is performed through Gaussian elimination:  \n<ol>  \n  <li>Convert matrix \\( A \\) into an upper triangular matrix \\( U \\) using row operations.</li>  \n  <li>Keep track of the multipliers used in each step to form the lower triangular matrix \\( L \\).</li>  \n  <li>If partial pivoting is required, an additional permutation matrix \\( P \\) may be introduced, leading to \\( PA = LU \\).</li>  \n</ol>  \n\n<br>  \n<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/Screenshot_2025-03-02_at_1.34.36_AM.png\"></div> ",
    "front": "<b>LU Decomposition</b>  \n<ul>  \n  <li><b>Define:</b> What is LU decomposition?</li>  \n  <li><b>Applications:</b> Where is LU decomposition used?</li>  \n  <li><b>Procedure:</b> How is LU decomposition performed?</li>  \n</ul>  ",
    "id": 1740890882881,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 3
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741835686552
      }
    ],
    "back": "TODO: New back content",
    "front": "LU decomposition and Gauss elimination application to linear systems.",
    "id": 1741831308011,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<b><u>LU Decomposition Result</u></b> <br>\nFor the matrix: <br>\n\\[\nA =\n\\begin{bmatrix}\n4 & 3 \\\\\n6 & 3\n\\end{bmatrix},\n\\] <br>\nthe LU decomposition (without permutation) is: <br>\n\\[\nL =\n\\begin{bmatrix}\n1 & 0 \\\\\n\\frac{3}{2} & 1\n\\end{bmatrix},\n\\quad\nU =\n\\begin{bmatrix}\n4 & 3 \\\\\n0 & -\\frac{3}{2}\n\\end{bmatrix}.\n\\] <br><br>",
    "front": "<b>Factor matrix \\( A \\) into \\( LU \\)</b> <br>  \n\\[\nA =\n\\begin{bmatrix}\n4 & 3 \\\\\n6 & 3\n\\end{bmatrix}\n\\]\n",
    "id": 1741164002995,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741165545220
      }
    ],
    "back": "<b><u>LU Decomposition Result</u></b> <br>\nFor the matrix: <br>\n\\[\nA =\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 10\n\\end{bmatrix},\n\\] <br>\nthe LU decomposition (without permutation) is: <br>\n\\[\nL =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n4 & 1 & 0 \\\\\n7 & 2 & 1\n\\end{bmatrix},\n\\quad\nU =\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & -3 & -6 \\\\\n0 & 0 & 1\n\\end{bmatrix}.\n\\] <br><br>",
    "front": "<b>Factor matrix \\( A \\) into \\( LU \\)</b> <br>  \n\\[\nA =\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 10\n\\end{bmatrix}\n\\]",
    "id": 1741164054782,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741166780847
      }
    ],
    "back": "<b><u>LU Decomposition Result</u></b> <br>\nFor the given matrix, a permutation is required during Gaussian elimination. We first form a permutation matrix P that swaps rows 2 and 3 to avoid a zero pivot in the second elimination step. <br>\n\nPermutation Matrix:  <br>\n\\[\nP =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix}\n\\]\n\n<br>\nTransformed Matrix: <br>\n\\[\nPA =\n\\begin{bmatrix}\n1 & 3 & 2 \\\\\n3 & 8 & 6 \\\\\n2 & 6 & 5\n\\end{bmatrix}\n\\]\n\n<br>\nThe LU decomposition of PA (i.e., PA = LU) is: <br>\n\\[\nL =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n3 & 1 & 0 \\\\\n2 & 0 & 1\n\\end{bmatrix},\n\\quad\nU =\n\\begin{bmatrix}\n1 & 3 & 2 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n \\]",
    "front": "<b>Factor matrix \\( A \\) into \\( LU \\)</b> <br>  \n\\[\nA = \\begin{bmatrix}\n1 & 3 & 2 \\\\\n2 & 6 & 5 \\\\\n3 & 8 & 6\n\\end{bmatrix}\n\\]",
    "id": 1741165926652,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "TODO: card for this concept <br><br>\n\nThe standard property of singular values states that for any invertible matrix A, the singular values of its inverse satisfy:\n\n\\sigma_i(A^{-1}) = \\frac{1}{\\sigma_{n-i+1}(A)}",
    "id": 1741659280390,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741836669150
      },
      {
        "correct": false,
        "reason": "U = something",
        "timestamp": 1741836682442
      },
      {
        "correct": true,
        "timestamp": 1741837175168
      },
      {
        "correct": true,
        "timestamp": 1741837373262
      },
      {
        "correct": true,
        "timestamp": 1741837394510
      }
    ],
    "back": "<u><b>Definition</b></u><br>  \n<p>  \nThe Singular Value Decomposition (SVD) of a matrix \\( A \\in \\mathbb{R}^{m \\times n} \\) is given by:  \n$$  \nA = U \\Sigma V^T,  \n$$  \nwhere:  \n<ul>  \n    <li>\\( U \\) is an \\( m \\times m \\) orthogonal matrix (its columns are the left singular vectors),</li>  \n    <li>\\( \\Sigma \\) is an \\( m \\times n \\) diagonal matrix with nonnegative real numbers on the diagonal (the singular values), and</li>  \n    <li>\\( V \\) is an \\( n \\times n \\) orthogonal matrix (its columns are the right singular vectors).</li>  \n</ul>  \n</p>  \n<br>  \n\n<u><b>Steps to Compute SVD</b></u><br>  \n<ol>  \n  <li>Compute the eigenvalues and eigenvectors of \\( A^T A \\). Order eigenvalues in descending order. Eigenvectors form \\( V \\), and singular values are \\( \\sqrt{\\text{eigenvalues}} \\).</li>  \n  <li>Compute \\( U \\) using eigen decomposition of \\( A A^T \\) or \\( U = A V \\Sigma^{-1} \\) for nonzero singular values.</li>  \n</ol>  ",
    "front": "<b>Singular Value Decomposition (SVD)</b>  \n<ul>  \n    <li><b>Define:</b> What is the SVD of \\( A \\in \\mathbb{R}^{m \\times n} \\)?</li>  \n    <li><b>Procedure:</b> How to compute SVD for \\( A \\in \\mathbb{R}^{m \\times n} \\)?</li>  \n</ul>",
    "id": 1740890913235,
    "tags": [
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "$$\nA = U \\Sigma V^T\n$$\n<br>\nwhere $U \\in \\mathbb{R}^{m \\times r}$ is orthogonal, $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is diagonal with non-negative entries, and $V \\in \\mathbb{R}^{n \\times r}$ is orthogonal.\n<br><br>\n\n$$\nA = \\sum_{i=1}^{r} {u_i \\, \\sigma_i \\, v_{i}^T}\n$$\n<br>\nwhere each product is a rank-one matrix with their sum equaling the original matrix $A$. REMEMBER: an outer-product is performed between the left and right singular vectors!",
    "front": "For matrix $A \\in \\mathbb{R}^{m \\times n}$, write the matrix and vector sum notation of its SVD and provide the matrix dimensions.",
    "id": 1745096977473,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741839080243
      }
    ],
    "back": "<u><b>Steps to Compute PCA</b></u><br>  \n<ol>  \n  <li>Given a dataset with mean-centered data matrix \\( X \\in \\mathbb{R}^{m \\times n} \\), compute its covariance matrix:  \n      $$ C = \\frac{1}{m} X^T X. $$  \n  </li>  \n  <li>Compute the eigenvalues and eigenvectors of \\( C \\). The eigenvectors (principal components) form the matrix \\( V \\), and the corresponding eigenvalues indicate the variance explained by each component.</li>  \n  <li>Choose the top \\( k \\) eigenvectors corresponding to the largest eigenvalues to form \\( V_k \\).</li>  \n  <li>Project the original data onto the new lower-dimensional subspace:  \n      $$ Z = X V_k, $$  \n      where \\( Z \\) represents the data in the reduced \\( k \\)-dimensional space.  \n      <ul>  \n          <li>\\( V_k \\) is \\( n \\times k \\) (the top \\( k \\) principal components).</li>  \n          <li>\\( Z \\) is \\( m \\times k \\) (the transformed data in the lower-dimensional space).</li>  \n      </ul>  \n  </li>  \n</ol>  ",
    "front": "<b>Principal Component Analysis (PCA)</b>  \n<ul>  \n    <li><b>Procedure:</b> How to compute PCA?</li>  \n</ul>",
    "id": 1741831505419,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741839076218
      }
    ],
    "back": "<u><b>Steps to Compute the Pseudoinverse</b></u><br>  \n<ol>  \n  <li>Compute the singular value decomposition (SVD) of \\( A \\):  \n      $$ A = U \\Sigma V^T. $$  \n  </li>  \n  <li>Form \\( \\Sigma^+ \\) by taking the reciprocal of each nonzero singular value in \\( \\Sigma \\), keeping zeros as zeros, and transposing the matrix dimensions.</li>  \n  <li>Compute the pseudoinverse as:  \n      $$ A^+ = V \\Sigma^+ U^T. $$  \n  </li>  \n</ol>  ",
    "front": "<b>Pseudoinverse Using SVD</b>  \n<ul>  \n    <li><b>Procedure:</b> How to compute the pseudoinverse of \\( A \\) using SVD?</li>  \n</ul>",
    "id": 1741831520702,
    "tags": [
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741839901789
      },
      {
        "correct": false,
        "timestamp": 1741839904752
      },
      {
        "correct": true,
        "timestamp": 1741839905542
      },
      {
        "correct": true,
        "timestamp": 1741839910783
      },
      {
        "correct": true,
        "timestamp": 1741839911669
      }
    ],
    "back": "<u><b>Steps to Compute Least Squares Solution</b></u><br>  \n<ol>  \n  <li>Given an overdetermined system \\( A x = b \\), the goal is to minimize the error:  \n      $$ \\| A x - b \\|^2. $$  \n  </li>  \n  <li>Compute the SVD of \\( A \\):  \n      $$ A = U \\Sigma V^T. $$  \n  </li>  \n  <li>Compute the pseudoinverse:  \n      $$ A^+ = V \\Sigma^+ U^T, $$  \n      where \\( \\Sigma^+ \\) is formed by taking the reciprocal of each nonzero singular value in \\( \\Sigma \\) while keeping zeros as zeros.</li>  \n  <li>Obtain the least squares solution:  \n      $$ x = A^+ b = V \\Sigma^+ U^T b. $$  \n  </li>  \n</ol>  \n<p>  \nThis \\( x \\) minimizes \\( \\| A x - b \\|^2 \\), providing the best approximate solution even when \\( A \\) is not invertible.  \n</p>  ",
    "front": "<b>Least Squares Solution Using SVD</b>  \n<ul>  \n    <li><b>Procedure:</b> How to compute the least squares optimal solution using SVD and the pseudoinverse?</li>  \n</ul>",
    "id": 1741831540365,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741837901692
      }
    ],
    "back": "<u><b>Steps to Compute Low-Rank Approximation</b></u><br>  \n<ol>  \n  <li>Compute the SVD of \\( A \\in \\mathbb{R}^{m \\times n} \\):  \n      $$ A = U \\Sigma V^T $$  \n  </li>  \n  <li>Choose a target rank \\( k \\) (where \\( k < \\min(m, n) \\)).</li>  \n  <li>Truncate \\( U, \\Sigma, V \\) by keeping only the first \\( k \\) singular values and corresponding singular vectors:  \n      <ul>  \n          <li><b>Keep the Top \\( k \\) Singular Values:</b> Form \\( \\Sigma_k \\) by retaining only the first \\( k \\) singular values from \\( \\Sigma \\), discarding the rest.</li>  \n          <li><b>Keep the Corresponding Singular Vectors:</b> Take the first \\( k \\) columns of \\( U \\) (forming \\( U_k \\)) and the first \\( k \\) columns of \\( V \\). For \\( V^T \\), this corresponds to taking the first \\( k \\) rows.</li>  \n      </ul>  \n      The resulting approximation is given by:  \n      $$ A_k = U_k \\Sigma_k V_k^T $$  \n      This \\( A_k \\) is the best rank-\\( k \\) approximation of \\( A \\) in terms of both the Frobenius norm and the spectral norm.  \n  </li>  \n</ol>  \n<br>  \n<u><b>Dimensions of \\( A_k \\)</b></u><br>  \n<ul>  \n    <li>\\( U_k \\) is \\( m \\times k \\) (the first \\( k \\) columns of \\( U \\)).</li>  \n    <li>\\( \\Sigma_k \\) is \\( k \\times k \\) (a diagonal matrix containing the top \\( k \\) singular values).</li>  \n    <li>\\( V_k^T \\) is \\( k \\times n \\) (the first \\( k \\) rows of \\( V^T \\), equivalently, \\( V_k \\) is \\( n \\times k \\)).</li>  \n</ul>  ",
    "front": "<b>Low-Rank Approximation Using SVD</b>  \n<ul>  \n    <li><b>Procedure:</b> How to compute a low-rank approximation of a matrix using SVD?</li>  \n</ul>",
    "id": 1741831453604,
    "tags": [
      "18.065",
      "midterm1",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Prove that for any matrix $A \\in R^{m\u00d7n}$, that $A^{T}A$ and $A$ have\nthe same null space.",
    "id": 1740890964079,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Definition</b></u><br>\n<p>\nTwo square matrices $\\mathbf{A}$ and $\\mathbf{B}$ are similar if there exists a nonsingular matrix $\\mathbf{X}$ such that:\n$$\n\\mathbf{B} = \\mathbf{X}^{-1} \\mathbf{A} \\mathbf{X}\n$$\nwhere $\\mathbf{X}$ is an invertible matrix. \n<br><br>\nThis transformation preserves many key properties of the matrix, including:\n<ul>\n  <li>Eigenvalues</li>\n  <li>Characteristic polynomial</li>\n  <li>Determinant</li>\n  <li>Trace</li>\n  <li>Rank</li>\n  <li>Minimal polynomial</li>\n  <li>Jordan canonical form</li>\n</ul>\n</p>\n<br>\n<u><b>Procedure</b></u><br>\n<ol>\n  <li>Find a candidate nonsingular matrix $\\mathbf{X}$.</li>\n  <li>Verify that the transformation $\\mathbf{B} = \\mathbf{X}^{-1} \\mathbf{A} \\mathbf{X}$ holds.</li>\n  <li>Check for key properties such as identical characteristic polynomials.</li>\n</ol>\n<br>\n<u><b>Practical Use</b></u><br>\n<p>\nSimilarity transformations are used in various areas of linear algebra, including:\n<ul>\n  <li>Computing canonical forms (e.g., Jordan form).</li>\n  <li>Reducing matrices to simpler forms for eigenvalue analysis.</li>\n  <li>Determining if matrices represent the same linear transformation in different bases.</li>\n</ul>\n</p>",
    "front": "<b>Similarity Transformation</b>\n<ul>  \n    <li><b>Define:</b> What is a Similarity Transformation? What key properties does it preserve?</li>\n    <li><b>Procedure:</b> How do you determine if two matrices are similar?</li>\n    <li><b>Practical Use:</b> Where/when is Similarity Transformation used?</li>\n</ul>",
    "id": 1741653306737,
    "tags": [
      "18.065",
      "midterm1",
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<b><u>Definition</u></b> <br>  \nA matrix \\( A \\in \\mathbb{R}^{n \\times n} \\) is called a <b>symmetric matrix</b> if it satisfies: \n\\[\nA^T = A.\n\\]\nThis means that the matrix is equal to its transpose. <br><br>  \n\n<b><u>Properties</u></b> <br>  \n<ul>\n    <li>Symmetric matrices always have real eigenvalues.</li>\n    <li>The eigenvectors of a symmetric matrix corresponding to distinct eigenvalues are orthogonal.</li>\n    <li>In the real field, every symmetric matrix is diagonalizable.</li>  \n    <li>Off-diagonal elements satisfy \\( a_{ij} = a_{ji} \\), meaning the matrix is symmetric across the main diagonal.</li>\n</ul>  ",
    "front": "<b>Symmetric Matrix</b>  \n<ul>  \n    <li><b>Define:</b> What is a symmetric matrix?</li>  \n    <li><b>Properties:</b> What are the key characteristics of a symmetric matrix?</li>  \n</ul>  ",
    "id": 1740890063445,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Propose a function $x = linear\\_solver(U, b)$, where $U \\in R^{n \\times n}$ is an upper triangular matrix, $b \\in R^n$ is a vector, and $x \\in R^n$ is the solution to $Ux = b$. <br><br>\n\nThen apply your procedure to",
    "id": 1740891657888,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Deflation via Householder Transformation",
    "id": 1741078759558,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741072483001
      }
    ],
    "back": "<b><u>Definition</u></b> <br>\n<p>\nTwo square matrices $A$ and $B$ that are related by <br>\n$$\nB = X^{-1}AX\n$$\n<br>\nwhere $X$ is a square nonsingular matrix are said to be similar.\n</p>\n<br>\n<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/IMG_3E1D74CC9C45-1.jpeg\"></div> <br>\n\n<b><u>Properties</u></b> <br>\n<ul>\n  <li>\n    If $(\\lambda, v) \\text{ is an eigenpair of } B, \\text{ then } (\\lambda, Xv) \\text{ is an eigenpair of } A$.  \n    <ul>\n      <li><u>Proof</u> <br>\n        If \\( Bv = \\lambda v \\), then  \n        <br>\n        $$\n        A(Xv) = X B X^{-1} X v = X B v = \\lambda (Xv)\n        $$\n      </li>\n    </ul>\n  </li>\n</ul>\n\n<br><br>\n<ul>\n  <li>Identical characteristic polynomials</li>\n  <li>Equal determinants, traces, and ranks</li>\n  <li>The same minimal polynomial and Jordan canonical form</li>\n  <li>Consistent diagonalizability properties</li>\n</ul>",
    "front": "<b>Similar matrix</b> <br><br>\nDefine <br><br>\nProperties",
    "id": 1740892885417,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<b><u>Definition</u></b> <br>\n<p>\nDiagonalization is the process of finding a diagonal matrix $D$ such that a square matrix $A$ is similar to $D$, meaning:  <br>\n$$\nA = X D X^{-1}\n$$\n</p>\n\nwhere:\n<ul>\n  <li>$D$ is a diagonal matrix whose entries are the eigenvalues of $A$.</li>\n  <li>$X$ is the matrix whose columns are the eigenvectors of $A$.</li>\n  <li>$A$ is diagonalizable if it has $n$ linearly independent eigenvectors.</li>\n</ul>\n\n<br>\n\n<b><u>Practical Use</u></b> <br>\n<ul>\n  <li><b>Computing Matrix Powers Efficiently:</b> If we need $A^k$, we can compute it as:\n    $$\n    A^k = X D^k X^{-1}\n    $$\n    where raising $D$ to a power is simple since it is diagonal.\n  </li>\n</ul>\n\n<br>\n\n<b><u>Procedure</u></b> <br>\n<ol>\n  <li><b>Find Eigenvalues:</b> Solve the characteristic equation:\n    $$\n    \\det(A - \\lambda I) = 0\n    $$\n    to find the eigenvalues $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$.\n  </li>\n  <li><b>Find Eigenvectors:</b> For each eigenvalue $\\lambda_i$, solve:\n    $$\n    (A - \\lambda_i I)v_i = 0\n    $$\n    to find the corresponding eigenvector $v_i$.\n  </li>\n  <li><b>Construct Matrices:</b>\n    <ul>\n      <li>$X$ is the matrix whose columns are the eigenvectors $v_1, v_2, \\dots, v_n$.</li>\n      <li>$D$ is the diagonal matrix with eigenvalues $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$ on the diagonal.</li>\n    </ul>\n  </li>\n  <li><b>Compute $X^{-1}$:</b> Since $X$ is invertible (if $A$ is diagonalizable), compute its inverse $X^{-1}$.</li>\n  <li><b>Verify the Similarity Relation:</b> Ensure that:\n    $$\n    A = X D X^{-1}\n    $$\n  </li>\n</ol>",
    "front": "<b>Diagonalization</b> \n<ul>\n  <li><b>Define:</b> What is diagonalization?</li>\n  <li><b>Practical Use:</b> Why is diagonalization useful?</li>\n  <li><b>Procedure:</b> How do we diagonalize a matrix?</li>\n</ul>",
    "id": 1741066673709,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741842867076
      }
    ],
    "back": "<b><u>Definition</u></b> <br>\nThe power method is an iterative algorithm used to approximate the dominant eigenvalue (the eigenvalue with the greatest absolute value) and its corresponding eigenvector of a matrix. <br><br>\n\n<b><u>Procedure</u></b> <br>\n1. Choose an initial vector \\( x_0 \\) in which every element is nonzero. A good choice is a vector of all 1s normalized to unit length (to ensure it is in the span of the dominant eigenvector). <br>\n2. For \\( k = 0, 1, 2, \\dots \\), compute \\( x_{k+1} = A x_k \\). <br>\n3. Normalize \\( x_{k+1} \\) to avoid numerical overflow. <br>\n4. Repeat until convergence; \\( x_k \\) approaches the eigenvector associated with the dominant eigenvalue. <br>\n5. Estimate the dominant eigenvalue by \\( \\lambda \\approx \\frac{x_k^T A x_k}{x_k^T x_k} \\).\n<br><br>\n\n<b><u>When does it NOT converge?</u></b> <br>\n1. Non-unique dominant eigenvalue. <br>\n2. Zero project. <br>",
    "front": "<b>Power Method</b>\n<ul>\n    <li><b>Define:</b> What is the power method?</li>\n    <li><b>Procedure:</b> How does the power method algorithm approximate the dominant eigenvalue and its corresponding eigenvector?</li>\n    <li>When does the power method NOT converge?</li>\n</ul>",
    "id": 1741166998328,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1741842887064
      }
    ],
    "back": "<p>\nDeflation in the recursive power method is a technique used to remove the influence of the dominant eigenpair after computing the largest eigenvalue \\( \\lambda_1 \\) and its corresponding eigenvector \\( x_1 \\). The matrix is updated as:\n$$\nA_1 = A - \\lambda_1 x_1 x_1^T\n$$\nThis subtraction eliminates the rank-one component associated with \\( (\\lambda_1, x_1) \\), allowing the power method to extract the next dominant eigenpair from the modified matrix \\( A_1 \\).  \nDeflation is commonly used in numerical linear algebra to compute multiple eigenvalues iteratively.\n</p>",
    "front": "<b>Deflation in Recursive Power Method</b>\n<ul>  \n    <li>What is deflation in the recursive power method, and how is it used?</li>\n</ul>",
    "id": 1741842491794,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Eigenvalue and eigenvectors, power method, spectral theorem, positive (semi-)de\u2000nite matrix --> application in optimization",
    "id": 1741831385966,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Singular Vector",
    "id": 1741418172612,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Algorithm</b></u><br>  \n<ol>  \n    <li>Compute the residual:  \n    $$  \n    r_k = b - A x_k  \n    $$  \n    </li>  \n    <li>Compute the step size:  \n    $$  \n    \\alpha_k = \\frac{r_k^T r_k}{r_k^T A r_k}  \n    $$  \n    </li>  \n    <li>Update the solution:  \n    $$  \n    x_{k+1} = x_k + \\alpha_k r_k  \n    $$  \n    </li>  \n</ol>  ",
    "front": "<b>Steepest Descent Method</b>  \n<ul>  \n    <li><b>Algorithm:</b> What are the steps for solving \\(Ax = b\\) using the Steepest Descent Method?</li>  \n</ul>  ",
    "id": 1741273002745,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Algorithm</b></u><br>  \n<ol>  \n    <li>Initialize residual and search direction:  \n    $$  \n    r_0 = b - A x_0, \\quad p_0 = r_0  \n    $$  \n    </li>  \n    <li>Compute the step size:  \n    $$  \n    \\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k}  \n    $$  \n    </li>  \n    <li>Update the solution:  \n    $$  \n    x_{k+1} = x_k + \\alpha_k p_k  \n    $$  \n    </li>  \n    <li>Update the residual:  \n    $$  \n    r_{k+1} = r_k - \\alpha_k A p_k  \n    $$  \n    </li>  \n    <li>Compute the coefficient:  \n    $$  \n    \\beta_k = \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}  \n    $$  \n    </li>  \n    <li>Update the search direction:  \n    $$  \n    p_{k+1} = r_{k+1} + \\beta_k p_k  \n    $$  \n    </li>  \n</ol>  ",
    "front": "<b>Conjugate Gradient Method</b>  \n<ul>  \n    <li><b>Algorithm:</b> What are the steps for solving \\(Ax = b\\) using the Conjugate Gradient Method?</li>  \n</ul>  ",
    "id": 1741273019654,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Definition</b></u><br>  \n<p>  \nThe \\(k\\)-th Krylov subspace generated by \\(A\\) and an initial vector \\(r_0\\) is:  \n$$  \n\\mathcal{K}_k(A, r_0) = \\text{span}\\{ r_0, A r_0, A^2 r_0, \\dots, A^{k-1} r_0 \\}  \n$$  \n</p>  \n<br>  \n<u><b>Role in Iterative Methods</b></u><br>  \n<p>  \nThe Krylov subspace provides a lower-dimensional space onto which the original problem is projected. Many iterative methods, such as the Conjugate Gradient Method, use this subspace to accelerate convergence by constructing approximate solutions within it.  \n</p>  ",
    "front": "<b>Krylov Subspace</b>  \n<ul>  \n    <li><b>Definition:</b> What is the Krylov subspace?</li>  \n    <li><b>Role in Iterative Methods:</b> How is the Krylov subspace used in numerical algorithms?</li>  \n</ul>  ",
    "id": 1741831591667,
    "tags": [
      "18.065",
      "midterm1"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1745096247951
      }
    ],
    "back": "$$\n(A^T A)_{i j} = \\sum_{k=1}^{m}{A_{k i} A_{k j}}\n$$\n<br>\nwhich can be interpreted as the product of rows.\n<br><br>\n$$\n(A A^T)_{i j} = \\sum_{k=1}^{n}{A_{i k} A_{j k}}\n$$\n<br> \nwhich can be interpreted as the product of columns.",
    "front": "For $A \\in \\mathbb{R}^{m \\times n}$, write $A^T A$ and $A A^T$ in summation notation.",
    "id": 1745094991542,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": false,
        "reason": "part 3",
        "timestamp": 1745281510103
      },
      {
        "correct": true,
        "timestamp": 1745281590310
      },
      {
        "correct": true,
        "timestamp": 1745282084656
      }
    ],
    "back": "If $A$ is a (strictly) <b>positive matrix</b>, i.e, $A_{i j} > 0, \\; \\forall i, j$\n<ol>\n<li>\n$\\rho(A)$ is an eigenvalue of $A$ with multiplicity 1.\n</li>\n\n<li>\nAll other eigenvalues of A satisfies $| \\lambda | < \\rho(A)$\n</li>\n\n<li>\nThere is a (strictly) positive eigenvector $x$ corresponding to $\\rho(A)$ s.t. $x_i\n >0, \\; \\forall i$</li>\n</ol>",
    "front": "Perron-Frobenius theorem",
    "id": 1745093988203,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1745281595247
      },
      {
        "correct": true,
        "timestamp": 1745282086786
      }
    ],
    "back": "denoted $\\rho(A)$, from Perron-Frobenius.",
    "front": "spectral radius",
    "id": 1745099607146,
    "tags": [
      "16.32",
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "pset 3 using matrix vector summation notation to show both sum of positive products.",
    "front": "<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/Screenshot_from_2025-04-19_17-03-45.png\"></div>",
    "id": 1745096599588,
    "tags": [
      "18.065",
      "midterm2",
      "pp"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "pset 3\n<ul>\n\n<li>\nBest rank-1 approximation is truncated SVD <br>\n$$\nA_1 = u_1 \\, \\sigma_1 \\, v_1^T\n$$\n</li>\n\n<li>\nThe first singular vector $u_1$ of $A$ is an eigenvector of $A A^T$ --> the right singular vector $v_1$ of $A$ is an eigenvector of $A^T A$\n    <ul>\n    <li>\n    TODO: interesting thing to prove, check if on previous exam b/c might need it?\n    </li>\n    </ul>\n</li>\n\n<li>\n$A A^T$ and $A^T A$ produce PD matrices.\n</li>\n\n<li>\nTherefore, $u_1$ and $v_1$ can be chosen to be strictly positive\n</li>\n\n<li>\n $u_1$ and $v_1$ outer-product is strictly positive matrix b/c product of positive vectors.\n</li>\n\n<li>\n$\\sigma_1$ positive so the combined $A_1 = \\sigma_1 \\, u_1 \\, v_1^T$ is also a strictly positive matrix.\n</li>\n\n</ul>",
    "front": "<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/Screenshot_from_2025-04-19_17-05-36.png\"></div>",
    "id": 1745096706505,
    "tags": [
      "18.065",
      "midterm2",
      "pp"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Definition</b></u><br>  \n<p>  \nA <b>Probability Transition Matrix</b> is a square matrix that represents the probabilities of transitioning from one state to another in a Markov process. Each entry in the matrix describes the probability of moving from state \\(i\\) to state \\(j\\). <br>\n$$  \nP =  \n\\begin{bmatrix}  \np_{11} & p_{12} & \\cdots & p_{1n} \\\\  \np_{21} & p_{22} & \\cdots & p_{2n} \\\\  \n\\vdots & \\vdots & \\ddots & \\vdots \\\\  \np_{n1} & p_{n2} & \\cdots & p_{nn}  \n\\end{bmatrix}  \n$$ \n<br>\nwhere \\( p_{ij} \\) is the probability of transitioning from state \\( i \\) to state \\( j \\).  \n</p>  \n<br>\n\n<u><b>Key Properties</b></u><br>  \n<ul>  \n  <li>\n  <b>\\( p_{ij} \\) is the probability of transitioning from state \\( i \\) to state \\( j \\)</b>\n  </li>\n  <li><b>Each row sums to 1</b> <br>\n  $$  \n  \\sum_{j=1}^{n} p_{ij} = 1, \\quad \\forall i  \n  $$  \n  </li>  \n  <li>All elements are non-negative: \\( 0 \\leq p_{ij} \\leq 1 \\).</li>  \n  <li>For an absorbing Markov chain, at least one row has a 1 on the diagonal and 0 elsewhere.</li>  \n</ul>  \n<br>",
    "front": "<b>Probability Transition Matrix</b>  \n<ul>  \n    <li><b>Define:</b> What is a Probability Transition Matrix?</li>  \n    <li><b>Procedure:</b> How do you construct and use a Probability Transition Matrix?</li>  \n    <li><b>Practical Use:</b> Where/when is a Probability Transition Matrix used?</li>  \n</ul>  ",
    "id": 1741703975173,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/Screenshot_from_2025-04-19_18-10-34.png\"></div>",
    "front": "come back to this: <br>\n<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/Screenshot_from_2025-04-19_18-10-15.png\"></div>",
    "id": 1745100606306,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": false,
        "reason": "col-outer-row sum notation",
        "timestamp": 1745281976521
      },
      {
        "correct": true,
        "timestamp": 1745282211315
      }
    ],
    "back": "$$\n(AB)_{i j} = \\sum_{k=1}^{n}{A_{i k} B_{k j}}\n$$\n<br><br>\n\nEquivalently you can instead build the entire matrix AB as a sum of rank-1 matrices: \n<br>\n<ul>\n  <li>\n  $A_{: k} \\in \\mathbb{R}^{m}$ is the $k$th column of $A$\n  </li>\n  <li>\n  $B_{k :} \\in \\mathbb{R}^{p}$ is the $k$th row of $B$\n  </li>\n</ul> \n$$\n(AB) = \\sum_{k=1}^{n}{A_{: k} B_{k :}}\n$$\n",
    "front": "Given matrices $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$, explain the matrix row-dot-column vs. column-outer-product-row view of the product $A B$.",
    "id": 1745102551331,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1745284569182
      }
    ],
    "back": "<u><b>Definition</b></u><br>\n<p>\nThe <b>expected value</b> of a random variable \\( X \\) is:\n$$\n\\mathbb{E}[X] = \\sum_i P_i \\cdot X_i\n$$\nThe <b>variance</b> of \\( X \\) is:\n$$\n\\text{Var}(X) = \\mathbb{E}[X^2] - \\left(\\mathbb{E}[X]\\right)^2\n$$\n</p>\n<br>\n<u><b>Procedure</b></u><br>\n<ol>\n  <li>List all possible values \\( X_i \\) and their probabilities \\( P_i \\).</li>\n  <li>Compute \\( \\mathbb{E}[X] = \\sum_i P_i \\cdot X_i \\).</li>\n  <li>Compute \\( \\mathbb{E}[X^2] = \\sum_i P_i \\cdot X_i^2 \\).</li>\n  <li>Calculate variance: \\( \\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\).</li>\n</ol>",
    "front": "expected value and variance formulas for a random variable $X$\n<ul>  \n    <li><b>Define:</b> What are the formulas for the expected value and variance of a random variable X?</li>\n    <li><b>Procedure:</b> How do you calculate expected value and variance from a probability distribution?</li>\n</ul>",
    "id": 1745284366519,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": false,
        "reason": "variance computation",
        "timestamp": 1745283198279
      },
      {
        "correct": false,
        "reason": "expected val of inside sum = sum of all outcomes * their probability",
        "timestamp": 1745283640504
      },
      {
        "correct": true,
        "timestamp": 1745283646854
      },
      {
        "correct": true,
        "timestamp": 1745284574189
      }
    ],
    "back": "Key Idea: approximate the sum of these rank-1 matrices by sampling $c << n$ terms.\n<br><br>\n\nKey Formula: <br>\n$$\n\\text{Approx}(A B) = \\frac{1}{c} \\sum_{s=1}^{c}{ \\frac{1}{p_{j_s}} A_{: j_s} B_{: j_s}^T }\n$$\n<br><br>\n\nAlgorithm:\n<ol>\n\n<li>\nStart with \"column outer-product row view\" of the matrix product $A B$ <br>\n$$\nAB = \\sum_{k=1}^{n}{A_{: k} B^T_{: k}}\n$$\n<br>\nTherefore, each outer-product is an $m \\times p$ rank-1 matrix.\n</li>\n\n<li>\nChoose a sampling distribution for $p = (p_1, \\dots, p_n)$, i.e., distribution across column indices of $A$ and row indices of $B$.\n</li>\n\n<li>\nBuild a Monte-Carlo estimator, i.e., randomly sample and average over $c$, $j_s$ indices <br>\n$$\n\\text{Approx}(A B) = \\frac{1}{c} \\sum_{s=1}^{c}{ \\frac{1}{p_{j_s}} A_{: j_s} B_{: j_s}^T }\n$$\n<br>\nEach sample $j_s$ is a random index in ${1, \\dots, n}$ with $Pr(j_t = i) = p_i$\n\n<ul>\n    <li>\n    This is an <b>unbiased</b> estimate of $AB$ b/c when you take the expectation of a random\u2010sampling estimator, you always weight each possible outcome by its probability of occurring. <br>\n    $$\n    \\mathbb{E}[\\text{Approx}(A B)] = \\mathbb{E}[\\frac{1}{c} \\sum_{s=1}^{c}{ \\frac{1}{p_{j_s}} A_{: j_s} B_{j_s :}^T }]\n    $$\n<br>\n    $$\n    = \\frac{1}{c} \\sum_{s=1}^{c}{ \\mathbb{E}[ \\frac{1}{p_{j_s}} A_{: j_s} B_{j_s :}^T ] } = \\frac{1}{c} \\, c \\; \\mathbb{E}[ \\frac{1}{p_{j_s}} A_{: j_s} B_{j_s :}^T ]\n    $$\n<br>\n    $$\n    \\mathbb{E}\\!\\Bigl[\\frac{1}{p_{j_s}}\\,A_{:j_s}\\,B_{j_s:}^{T}\\Bigr]\n    = \\sum_{i=1}^{n}p_i\\;\\frac{1}{p_i}\\,A_{:i}\\,B_{i:}^{T}\n    = \\sum_{i=1}^{n}A_{:i}\\,B_{i:}^{T}\n    = AB.\n    $$\n\n    </li>\n</ul>\n</li>\n\n<li>\nAverage to get an unbiased estimator. --> TODO: show that it is unbiased\n</li>\n\n<li>\nCompute the variance <br>\n$$\nP_i = \\frac{ \\| A_{: i} \\|_2 \\| B_{i :} \\|_2 }{ \\sum_{j=1}^n \\| A_{: j} \\|_2 \\| B_{j :} \\|_2 }\n$$\n</li>\n\n</ol>",
    "front": "For $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$, explain the <b>randomized matrix multiplication</b> approach to approximating $A B$<br>",
    "id": 1745100662703,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "pset 3 p7 <br>\nTODO: New back content",
    "front": "Steepest descent method and least squares",
    "id": 1745202853509,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "$\\forall x, y \\in C, \\; x \\theta + (1 -\\theta) y \\in C, \\; \\forall \\theta \\in [0, 1]$ \n<p>\nwhich can be interpreted as a line between any two points in a convex set is also in the convex set. Or that the weighted average between the two points is also in the set.\n</p>",
    "front": "convex set (definition)",
    "id": 1745168149209,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1745285492590
      }
    ],
    "back": "<u><b>Definition</b></u><br>\n<ul>\n  <li>\n  A function \\( f: \\mathcal{X} \\to \\mathbb{R} \\) is convex if 1) its domain \\( \\mathcal{X} \\) is convex and 2) \n  $$\n  f(\\theta x + (1 - \\theta)y) \\leq \\theta f(x) + (1 - \\theta)f(y), \\; \\forall x, y \\in \\mathcal{X},\\ \\theta \\in [0,1]\n  $$\n  </li>\n  <li>\n  If $f$ is twice continuously differentiable then $f$ is convex iff its $H \\succeq 0, \\; \\forall x \\in \\mathcal{X}$ and $\\equiv$ for a single variable function $f$ is convex iff $f''(x) \\geq 0, \\; \\forall x \\in \\mathcal{X}$\n  </li>\n  <li>\n  If $f$ is continuously differentiable, then <br>\n$f$ is convex $\\iff$ $f(y) \\geq f(x) + \\nabla f(x)^{T} (y - x), \\; \\forall x, y $\n  <ul>\n    <li>\n    Interpretation: tangent line is always below the graph.\n    </li>\n  </ul>\n  </li>\n</ul>\n\n<br>\n<u><b>Procedure</b></u><br>\n<p>\nCheck if $f$ is convex by checking it meets any of the above criteria.\n</p>",
    "front": "convex function (define, procedure to check)",
    "id": 1745175790362,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1745285891907
      }
    ],
    "back": "<b><u>Define</u></b>\n<ul>\n  <li>\n  $f$ is $\\mu$-strongly convex if $f - \\frac{\\mu}{2} \\| x \\|_{2}^{2}$ is convex with $\\mu > 0$.\n  <ul>\n    <li>\n    Interpretation: If the function is convex after removing some convex part and its still convex, this gives us a notion of how strongly convex the function is.\n    </li>\n  </ul>\n  </li>\n  <li>\n  $f$ is $\\mu$-strongly convex $\\iff$ $\\nabla^2 f(x) \\geq \\mu I$\n  <ul>\n    <li>\n    Interpretation: $H - \\mu I \\succeq 0$ (still PSD), then $f$ is $\\mu$-strongly convex.\n    </li>\n  </ul>\n  </li>\n  <li>\n  $$\n  f(y) \\geq f(x) + \\nabla f(x)^T(y - x) + \\frac{\\mu}{2} \\| y - x \\|^2\n  $$\n  </li>\n</ul>\n<br><br>\n\n",
    "front": "strong convexity",
    "id": 1745195439041,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Definition</b></u><br>\n<p>\n<b>\u03bc convexity</b> refers to <b>strong convexity</b> of a twice-differentiable function, where the strong convexity constant \\( \\mu \\) is the infimum of the smallest eigenvalues of the function\u2019s Hessian:\n$$\n\\mu = \\inf_{x \\in \\mathbb{R}^n} \\lambda_1(x)\n$$\nwhere \\( \\lambda_1(x) \\) is the smallest eigenvalue of the Hessian \\( \\nabla^2 f(x) \\).\n</p>\n<br>\n<u><b>Procedure</b></u><br>\n<ol>\n  <li>Compute the Hessian matrix: \\( H(x) = \\nabla^2 f(x) \\).</li>\n  <li>Find its smallest eigenvalue \\( \\lambda_1(x) \\) at each point.</li>\n  <li>Take the infimum over all \\( x \\in \\mathbb{R}^n \\): \n  $$ \\mu = \\inf_{x \\in \\mathbb{R}^n} \\lambda_1(x) $$</li>\n</ol>\n<br>\n<u><b>Practical Use</b></u><br>\n<p>\nUsed in optimization to determine convergence guarantees; for fixed Hessians (e.g., quadratic functions), \u03bc is just the smallest eigenvalue.\n</p>",
    "front": "<b>\u03bc Convexity and How to Find the \u03bc Value</b>",
    "id": 1745184076812,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<p>\nbecause for convex optimization problems, every stationary / critical point must be a global minimum.\n</p>\n<p>\nFurthermore, if $f$ is strongly convex and global min exists, then the global min must be unique.\n</p>",
    "front": "why do we like convexity in optimization problems?",
    "id": 1745195802473,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1745286096158
      },
      {
        "correct": true,
        "timestamp": 1745286368021
      }
    ],
    "back": "<p>\nGoal: given $x_k$ and $d_k$, find $\\alpha_k$ (cheaply) s.t. there is a \"large\" decay in $f$, i.e., we want $f(x_{k+1}) << f(x_k)$\n</p>\n$$\nx_{k+1} = x_k + \\alpha_k d_k\n$$\n<br>\nwhere $\\alpha_k$ is the step size and $d_k$ is the descent direction.\n<br><br>\n<b>Exact Line Search</b><br>\n$$\n\\alpha_k = \\arg\\min_{\\alpha > 0} f(x_k + \\alpha d_k)\n$$",
    "front": "line search (problem definition, exact line search)",
    "id": 1745198251493,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Taylor expansion",
    "id": 1745203649066,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Algorithm</b></u>\n<p>\n$$\nx_{k+1} = x_{k} - \\alpha_k \\nabla f(x_k)\n$$\n<br>\nsteepest descent direction is $d_k = - \\nabla f(x_k)$\n</p>\n<br>\n\n<u><b>Rough Derivation</b></u> <br>\nGoal: given a function $f$ and a current point $x_k$, we want to choose a direction $d_k$ and step size $a_k$ such that $x_{k+1} = x_k + \\alpha d_k$ minimizes the function $f(x_{k+1}) = f(x_k + \\alpha_k d_k)$\n<br>\n<ol>\n  <li>\n  First order Taylor expansion of $f$ around $x_k$ is $f(x_k) = f(x_k) + \\alpha_k \\nabla f(x_k)^T d_k + \\mathcal{O}(\\alpha_k^2)$ <br>\nThe truncated linearly approximation is $f(x_k + \\alpha_k d_k) \\approx f(x_k) + \\alpha \\nabla f(x_k)^T d_k$\n  </li>\n  <li>\n  To minimize $f(x_k + \\alpha_k d_k)$ we choose the direction $d_k$ that makes $\\nabla f(x_k)^T d_k$ most negative. <br>\nThis is achieved by choosing $d_k = - \\nabla f(x_k)$\n  </li>\n  <li>\n  Then we have to choose $\\alpha_k$ step size (small fixed small constant or via line search to minimize $f(x_k + \\alpha_k d_k)$ along our chosen $d_k$ steepest direction)\n  </li>\n</ol>",
    "front": "gradient descent (algo, rough derivation / intuition)",
    "id": 1745203042262,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1745289260565
      }
    ],
    "back": "pset 4.7 <br>\nTODO: New back content",
    "front": "compute gradient $\\nabla f(x)$ analytically by hand <br>\nTODO: pp",
    "id": 1745205384592,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "check that $\\underset{k \\rightarrow \\infty}{\\text{lim}} x_k = \\text{global min}$",
    "front": "how to check that $x^k$ converges to the global minimizer?",
    "id": 1745289333581,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<b>L-smooth</b> means that a function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is <b>Lipschitz continuous</b> with Lipschitz constant $\\mathbf{L}$: <br>\n$$\n\\| \\nabla f(x) - \\nabla f(y) \\| \\leq L \\| x - y \\|, \\quad \\forall x, y \\in \\mathbb{R}^n\n$$\n<br><br>\nInterpretation:\n<ul>\n  <li>\n  Quantifies the maximum rate at which the gradient of $f$ can change.\n  </li>\n  <li>\n  In geometric terms, it bounds the curvature of the function or \"steepness\" of the function.\n  </li>\n</ul> ",
    "front": "L-smooth (definition)",
    "id": 1745205659383,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1745290280291
      }
    ],
    "back": "$$\n\\underset{k \\rightarrow \\infty}{\\text{lim}} \\frac{\\| x_{k+1} - x^* \\|}{\\| x_k - x^* \\|} = 1\n$$",
    "front": "sublinear convergence",
    "id": 1745207460307,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1745290280892
      }
    ],
    "back": "$$\n\\frac{\\| x_{k+1} - x^* \\|}{\\| x_k - x^* \\|} < a, \\quad a \\in (0, 1)\n$$\n<br>\nfor sufficiently large $k$ (we use this language b/c might not strictly hold in the beginning of algorithm)",
    "front": "linear convergence",
    "id": 1745207703164,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1745290281344
      }
    ],
    "back": "$$\n\\underset{k \\rightarrow \\infty}{\\text{lim}} \\frac{\\| x_{k+1} - x^* \\|}{\\| x_k - x^* \\|} = 0\n$$",
    "front": "superlinear convergence",
    "id": 1745207997148,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1745290281743
      }
    ],
    "back": "$$\n\\frac{\\| x_{k+1} - x^* \\|}{\\| x_k - x^* \\|^2} \\leq a, \\quad a > 0\n$$\n<br>\nfor sufficiently large $k$ \n(we use this language b/c might now hold early in algorithm)",
    "front": "quadratic convergence",
    "id": 1745208110509,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<p>\n<u><b>Algorithm</b></u> <br>\n$$\nx_{k+1} = x_k - \\alpha_k \\nabla^2 f(x_k)^{-1} \\nabla f(x_k)\n$$\n</p>\n<br>\n\n<p>\n<u><b>Advantages</b></u> <br>\n<ul>\n  <li>\n  Locally quadratic convergence when $x_k$ sufficiently close to $x^*$\n  </li>\n</ul>\n</p>\n<br>\n\n<p>\n<u><b>Disadvantages</b></u> <br>\n<ul>\n  <li>\n  Expensive to compute and invert Hessian at each step.\n  </li>\n</ul>\n</p>\n<br>\n\n<p>\n<u><b>Rough Derivation</b></u> <br>\nGoal: given $f$ and $x_k$, choose $d$ to min $f(x_k + d)$\n<ol>\n  <li>\n  2nd order Taylor expansion of $f$ around $x_k$ is $f(x_k + d) \\approx f(x_k) + \\nabla f(x_k)^T d + \\frac{1}{2} d^T \\nabla^2 f(x_k) d$\n  </li>\n  <li>\n  Extract components involving $d$ to <br>\n$$\nQ(d) = \\nabla f(x_k)^T d + \\frac{1}{2} d^T \\nabla^2 f(x_k) d\n$$\n  </li>\n  <li>\n  Minimize $Q(d)$ w.r.t. $d$ by setting $\\nabla Q(d) = 0$ and solving for $d$ <br>\n$$\n\\nabla Q(d) = \\nabla f(x_k) + \\nabla^2 f(x_k) d = 0 \\quad \\rightarrow \\quad d =  \\nabla^2 f(x_k)^{-1} \\nabla f(x_k)\n$$\n  </li>\n</ol>\n</p>",
    "front": "Newton's method (algo, advantage, disadvantage, rough derivation)",
    "id": 1745208228179,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "$$\nx_{k+1} = x_k - \\alpha_k H_k \\nabla f(x_k)\n$$\n<br><br>\n<b>Key Idea</b>: efficiently approximate Newton's method without computing or inverting the full Hessian $\\nabla^2 f(x)$.\n<br><br>\nTODO: come back to details of how to calculate $H_k$",
    "front": "Quasi-Newton method (algo, key idea)",
    "id": 1745208234997,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "constrained optimization (canonical formulation)",
    "id": 1745208251183,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1745291837670
      }
    ],
    "back": "<b>KKT Conditions</b><br>\n<p>\nIf \\( x^* \\) is a local minimum and certain regularity conditions (e.g., constraint qualifications like LICQ) hold, then there exist multipliers \\( \\lambda^* \\in \\mathbb{R}^m \\), \\( \\nu^* \\in \\mathbb{R}^p \\) such that:\n</p>\n\n<ol>\n  <li><b>Stationarity:</b><br>\n  $$\n  \\nabla f(x^*) + \\sum_{i=1}^{m} \\lambda_i^* \\nabla g_i(x^*) + \\sum_{j=1}^{p} \\nu_j^* \\nabla h_j(x^*) = 0\n  $$\n  </li>\n\n  <li><b>Primal Feasibility:</b><br>\n  $$\n  g_i(x^*) \\leq 0, \\quad i = 1, \\dots, m \\\\\n  h_j(x^*) = 0, \\quad j = 1, \\dots, p\n  $$\n  </li>\n\n  <li><b>Dual Feasibility:</b><br>\n  $$\n  \\lambda_i^* \\geq 0, \\quad i = 1, \\dots, m\n  $$\n  </li>\n\n  <li><b>Complementary Slackness:</b><br>\n  $$\n  \\lambda_i^* g_i(x^*) = 0, \\quad i = 1, \\dots, m\n  $$\n  </li>\n</ol>\n",
    "front": "KKT conditions",
    "id": 1745208387617,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<p>\n  Encodes both the objective and constraints in the form <br>\n  $$\n  \\mathcal{L}(x, \\lambda, \\nu) = f(x) + \\sum_i \\lambda_i g_i(x) + \\sum_j \\nu_j h_j(x)\n  $$\n  <br>\n  where $\\lambda$ and $\\nu$ are Lagrange multipliers (dual variables) for the inequality $g(x) \\leq 0$ and equality constraints $h(x) = 0$, respectively<br>\n</p>",
    "front": "Lagrangian function",
    "id": 1745213019013,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<p><strong>Primal:</strong></p>\n<p>\n  $$\n  \\min_{x} f(x) \\quad \\text{subject to} \\quad c_i(x) = 0, \\, c_j(x) \\leq 0\n  $$\n</p>\n<p><strong>Dual:</strong></p>\n<p>\n  $$\n  \\max_{\\lambda, \\nu \\geq 0} g(\\lambda, \\nu) \\quad \\text{where} \\quad g(\\lambda, \\nu) = \\inf_x \\mathcal{L}(x, \\lambda, \\nu)\n  $$\n</p>",
    "front": "primal and dual problems",
    "id": 1745211769278,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<p><strong>Weak Duality:</strong></p>\n<p>\n  $$\n  g(\\lambda, \\nu) \\leq f(x)\n  $$\n</p>\n<p>for any dual feasible $(\\lambda, \\nu \\geq 0)$ and primal feasible $x$.</p>\n<p>This implies:</p>\n<p>\n  $$\n  \\max_{\\lambda, \\nu \\geq 0} g(\\lambda, \\nu) \\leq \\min_{x} f(x)\n  $$\n</p>\n<p>i.e., <b>the optimal value of the dual problem is always less than or equal to the optimal value of the primal problem.</b></p>\n",
    "front": "weak duality",
    "id": 1745208404901,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<p><strong>Strong Duality:</strong></p>\n<p>\n  $$\n  \\max_{\\lambda, \\nu \\geq 0} g(\\lambda, \\nu) = \\min_{x} f(x)\n  $$\n</p>\n<p>i.e., the optimal value of the dual problem equals the optimal value of the primal problem.</p>\n<p>This holds under certain conditions, such as convexity of the primal problem.</p>\n",
    "front": "strong duality",
    "id": 1745213343509,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "linear objective function and linear constraints. <br>\nif LP is finite then its dual problem is also finite with $q^* = p^*$ <br>\nTODO: add more from lecture slides",
    "front": "linear programming (define)",
    "id": 1745195327463,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "The quadratic penalty method is an approach for solving constrained optimization problems of the form:\n$$\n\\underset{x}{\\text{min}} f(x) \\quad \\text{subject to} \\quad c_i (x) = 0, \\quad \\forall i \\in E\n$$\n<br>\nwhere instead of solving the constrained problem directly, the method transforms into a sequence of unconstrained problems by adding a penalty term to the objective function:\n$$\n\\mathcal{P}(x, \\sigma) = f(x) + \\frac{\\sigma}{2} \\sum_{i \\in E} c_i^2(x)\n$$\n<br>\nwhere $\\sigma > 0$ is a penalty parameter that penalizes violations of the constraints. \n\n<br><br>\nhelps solve constrained optimization by converting it to a sequence of easier unconstrained problems. The penalty parameter $\\sigma$ ensures that constraint violations are penalized heavily, and under certain conditions, convergence to the true solution is guaranteed. However, large penalty values make the numerical problem harder, which is a key trade-off to understand.\n\n",
    "front": "quadratic penalty method",
    "id": 1745210873101,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<p>\n<b><u>Key Idea</u></b> <br>\nMarry KKT classical Lagrangian with the quadratic penalty method.\n</p>\n\n<p>\n<b><u>Algorithm</u></b> <br>\n<ol>\n  <li>\n    Initialize \\(x^0,\\;\\lambda^0,\\;\\sigma_0>0,\\;\\rho>1,\\;\\alpha>0,\\;k_{\\max},\\;k_{\\max,\\mathrm{inner}},\\;\\varepsilon,\\;\\eta\\).\n  </li>\n  <li>\n    For \\(k=0,1,2,\\dots\\) until \\(k=k_{\\max}\\) or \\(\\|c(x^k)\\|<\\varepsilon\\):\n    <ol>\n      <li>\n        <strong>Primal update:</strong><br/>\n        \\[\n          x^{k+1}\n          \\;=\\;\n          \\arg\\min_{x}\\;L_{\\sigma_k}(x,\\lambda^k)\n          \\;=\\;\n          f(x)\\;+\\;(\\lambda^k)^{T}c(x)\\;+\\;\\tfrac{\\sigma_k}{2}\\,\\|c(x)\\|^2.\n        \\]\n        Solve via gradient descent:\n        <ol>\n          <li>Set \\(x^{k+1,0}=x^k\\).</li>\n          <li>\n            For \\(j=0,1,\\dots\\) until \\(j=k_{\\max,\\mathrm{inner}}\\) or \n            \\(\\|\\nabla_x L_{\\sigma_k}(x^{k+1,j},\\lambda^k)\\|<\\eta\\), update\n            \\[\n              x^{k+1,j+1}\n              =\n              x^{k+1,j}\n              \\;-\\;\\alpha\\,\\nabla_x L_{\\sigma_k}(x^{k+1,j},\\lambda^k).\n            \\]\n          </li>\n          <li>Set \\(x^{k+1}=x^{k+1,j}\\).</li>\n        </ol>\n      </li>\n      <li>\n        <strong>Dual update:</strong><br/>\n        \\[\n          \\lambda^{k+1}\n          =\n          \\lambda^k\n          \\;+\\;\n          \\sigma_k\\,c\\bigl(x^{k+1}\\bigr).\n        \\]\n      </li>\n      <li>\n        <strong>Penalty update:</strong><br/>\n        \\[\n          \\sigma_{k+1}\n          =\n          \\rho\\,\\sigma_k.\n        \\]\n      </li>\n      <li>\n      Stop when $\\|c(x^{k+1})\\|<\\varepsilon$ or $k+1=k_{\\max}$\n      </li>\n    </ol>\n  </li>\n  <li>\n    Return \\(\\bigl(x^{k+1},\\lambda^{k+1}\\bigr)\\).\n  </li>\n</ol>\n</p>",
    "front": "augmented Lagrangian method",
    "id": 1745210795486,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "subgradient",
    "id": 1745210807704,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "subdifferential",
    "id": 1745210814953,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "subgradient method",
    "id": 1745210830281,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1745291095853
      }
    ],
    "back": "<b>Proximal Operator</b><br>\n<p>\nThe proximal operator of a function \\( h \\) at a point \\( x \\in \\mathbb{R}^n \\) is defined as:\n</p>\n$$\n\\text{prox}_h(x) = \\arg\\min_u \\left( h(u) + \\frac{1}{2} \\|u - x\\|_2^2 \\right)\n$$\n<hr>\n<b>What This Means</b><br>\n<p>\nThe proximal operator returns a point \\( u \\) that is a compromise between minimizing \\( h(u) \\) and staying close to \\( x \\). The second term\n$$\n\\frac{1}{2} \\|u - x\\|_2^2\n$$\nacts as a penalty for straying too far from \\( x \\).\n</p>",
    "front": "proximal operator",
    "id": 1745210840369,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "proximal gradient descent",
    "id": 1745205454631,
    "tags": [
      "18.065",
      "midterm2"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "The Taylor series for a vector-valued function \\( F(x) \\) expands it around \\( x \\): <br><br>\n$$\nF(x + \\Delta x) \\approx F(x) + g(x)^T \\Delta x + \\frac{1}{2} \\Delta x^T H(x) \\Delta x + \\dots\n$$\n<br><br>\n\nwhere: <br>\n\n- \\( g(x) \\) is the gradient of \\( F \\): <br><br>\n$$\ng = \\begin{bmatrix} \\frac{\\partial F}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial F}{\\partial x_n} \\end{bmatrix}.\n$$\n\n<br><br>\n- \\( H(x) \\) is the Hessian matrix of second derivatives: <br><br>\n$$\nH =\n\\begin{bmatrix}\n\\frac{\\partial^2 F}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 F}{\\partial x_1 \\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 F}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 F}{\\partial x_n^2}\n\\end{bmatrix}.\n$$\n<br><br>\nTruncating the series gives a quadratic approximation of \\( F(x) \\) around \\( x \\).",
    "front": "Taylor series for a vector-valued function \\( F(x) \\)",
    "id": 1742426830953,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "Positive definiteness of a real $n \\times n$ symmetric matrix $A$ is defined as <br><br>\n$$\nx^T A x > 0, \\quad \\forall x \\neq 0, \\; x \\in \\mathbb{R}^n\n$$",
    "front": "Positive definite (PD) <br><br>\nDefine<br><br>\nHow to test for it",
    "id": 1742426831006,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "meeting some necessary conditions does not mean the thing is true but its required that the condition be true when the thing is true. <br><br>\n\nsufficient condition when meant means the thing of interest is true.",
    "front": "Necessary vs. sufficient conditions",
    "id": 1742426830989,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "A positive definite (PD) Hessian guarantees a local minimum, while a positive semi-definite (PSD) Hessian is only necessary\u2014it does not confirm a minimum on its own.<br><br>\n\n\\[\n\\begin{array}{|c|c|c|c|}\n\\hline\n\\textbf{Condition} & \\textbf{Hessian Type} & \\textbf{What It Means} & \\textbf{Conclusion} \\\\\n\\hline\n\\text{Necessary} & \\text{Positive Semi-Definite (PSD)} & \\text{No negative curvature, but could be flat in some directions} & \\text{Local min possible, but not guaranteed} \\\\\n\\hline\n\\text{Sufficient} & \\text{Positive Definite (PD)} & \\text{Strictly upward curvature in all directions} & \\text{Guarantees a local min} \\\\\n\\hline\n\\end{array}\n\\]",
    "front": "Hessian necessary and sufficient conditions for a local minimum.",
    "id": 1742426830791,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "correct": true,
        "timestamp": 1742421096992
      }
    ],
    "back": "$$\ng(x^*) = 0\n$$\n<br><br>\ngradient is zero.",
    "front": "Stationary point necessary condition.",
    "id": 1742426830564,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "If $f: \\mathbb{R}^n \\to \\mathbb{R}$ is a differentiable function, the gradient of $f(x)$ is defined as:\n<br><br>\n\n$$\n\\nabla f(x) = \n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\\n\\frac{\\partial f}{\\partial x_2} \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial x_n}\n\\end{bmatrix}.\n$$\n\n<br><br>\nEach component $\\frac{\\partial f}{\\partial x_i}$ represents the partial derivative of $f$ with respect to $x_i$.\n<br><br>\n\nThe gradient is a vector that points in the direction of the steepest increase of $f(x)$.",
    "front": "<b>Gradient of a Differentiable Vector Function</b> \n<ul>\n  <li><b>Define:</b> What is the gradient of a differentiable vector function?</li>\n  <li><b>Procedure:</b> How is the gradient of a vector function computed and interpreted geometrically?</li>\n</ul>",
    "id": 1742426830193,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "A functional is a mapping from a space of functions to the real numbers. <br><br>\n\nMathematically, a functional $J$ takes a function $y(x)$ as input and returns a real number:\n<br><br>\n\n$$\nJ[y] = \\int_{a}^{b} F(x, y, y') \\, dx.\n$$\n\n<br><br>\nFunctionals are commonly used in calculus of variations and physics to express quantities such as energy or action.",
    "front": "What is a <b>functional</b>?",
    "id": 1742426830471,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "The first variation of a functional is the first-order term in the Taylor expansion of the increment, providing a linear approximation of the change in the functional. <br><br>\n\nThe first variation of a functional $J[x(t)]$ with respect to a perturbation $\\delta x(t)$ is defined as:\n<br><br>\n\n$$\n\\delta J(x(t), \\delta x(t)) = \\left. \\frac{d}{d\\epsilon} J(x(t) + \\epsilon \\delta x(t)) \\right|_{\\epsilon=0}.\n$$\n<br><br>\n\n<b>TODO: fix beyond here to make more sense</b>\n<br><br>\n\nFor a functional $J[y]$, we express its perturbation as:\n<br><br>\n\n$$\nJ[y + \\epsilon \\delta y] = \\int_a^b F(x, y + \\epsilon \\delta y, y' + \\epsilon \\delta y') \\, dx.\n$$\n<br><br>\n\nExpanding $F( \\cdot )$ in a Taylor series around $\\epsilon = 0$ gives:\n<br><br>\n$$\nF(x, y + \\epsilon \\delta y, y' + \\epsilon \\delta y') = F(x, y, y') + \\epsilon \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) + O(\\epsilon^2).\n$$\n<br>\n\nSubstituting this into the integral:\n<br><br>\n$$\nJ[y + \\epsilon \\delta y] = J[y] + \\epsilon \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx + O(\\epsilon^2).\n$$\n<br><br>\n\nTaking the limit as $\\epsilon \\to 0$, we obtain the first variation:\n<br><br>\n$$\n\\delta J = \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx.\n$$\n<br>",
    "front": "<b>Increment in Calculus of Variations</b> \n<ul>\n  <li><b>Define:</b> What is the increment of a functional?</li>\n  <li><b>Procedure:</b> How is the increment of a functional expressed and used when deriving the first variation?</li>\n</ul>",
    "id": 1742426830412,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "The first variation of functional is a linear approximation of the increment, i.e., first-order Taylor expansion of the increment.<br><br>\n\nThe **first variation** of a functional $J[x(t)]$ with respect to a perturbation $\\delta x(t)$ is defined as:\n<br><br>\n\n$$\n\\delta J(x(t), \\delta x(t)) = \\left. \\frac{d}{d\\epsilon} J(x(t) + \\epsilon \\delta x(t)) \\right|_{\\epsilon=0}.\n$$\n<br><br>\n\nFor a functional $J[y]$ and a small perturbation $\\delta y$, consider:\n<br><br>\n\n$$\nJ[y + \\epsilon \\delta y] = \\int_a^b F(x, y + \\epsilon \\delta y, y' + \\epsilon \\delta y') \\, dx.\n$$\n<br><br>\n\nExpanding the integrand $F( \\cdot )$ in a Taylor series around $\\epsilon = 0$, we obtain:\n<br><br>\n$$\nF(\\cdot) = F(x, y, y') + \\epsilon \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) + O(\\epsilon^2).\n$$\n<br><br>\n\nSubstituting this expansion back into the integral gives:\n<br><br>\n$$\nJ[y + \\epsilon \\delta y] = \\int_a^b \\Bigg[ F(x, y, y') + \\epsilon \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) + O(\\epsilon^2) \\Bigg] dx.\n$$\n<br>\n\nThis can be rewritten as:\n<br><br>\n$$\nJ[y + \\epsilon \\delta y] = J[y] + \\epsilon \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx + O(\\epsilon^2).\n$$\n<br><br>\n\nThe first variation $\\delta J$ is defined as the first-order term:\n<br><br>\n$$\n\\delta J = \\lim_{\\epsilon \\to 0} \\frac{J[y + \\epsilon \\delta y] - J[y]}{\\epsilon} = \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx.\n$$\n<br>",
    "front": "<b>First Variation</b> \n<ul>\n  <li><b>Define:</b> What is the first variation in the context of calculus of variations?</li>\n  <li><b>Procedure:</b> How is the first variation computed and used to determine extremals of functionals?</li>\n</ul>",
    "id": 1742426830356,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "The Euler-Lagrange equation states that for a function $g(x, \\dot{x}, t)$:\n<br><br>\n\n$$\n\\frac{d}{dt} \\left( \\frac{\\partial g}{\\partial \\dot{x}} \\right) - \\frac{\\partial g}{\\partial x} = 0\n\n\\quad \\quad \\quad \\quad \\text{or} \\quad \\quad \\quad \\quad\n\n\\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{x}} \\right) - \\frac{\\partial L}{\\partial x} = 0\n$$\n\n<br><br>\nRewriting this:\n\n<br><br>\n$$\n\\frac{d}{dt} (g_{\\dot{x}}) - g_x = 0\n\n\\quad \\quad \\quad \\quad \\text{or} \\quad \\quad \\quad \\quad\n\n\\frac{d}{dt} (L_{\\dot{x}}) - L_x = 0\n$$\n\n<br><br>\nwhich is the fundamental condition for extremizing a functional in the calculus of variations.",
    "front": "<b>Euler-Lagrange Equation</b> \n<ul>\n  <li><b>Define:</b> What is the Euler-Lagrange equation?</li>\n  <li><b>Procedure:</b> How is the Euler-Lagrange equation derived and applied in optimization problems?</li>\n</ul>",
    "id": 1742426830577,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Definition</b></u><br>\n<p>\nThe <b>costate</b> in optimal control theory, often denoted as \\( p(t) \\), represents the gradient of the optimal cost (value function) with respect to the state \\( x \\). The costate \\( p(t) \\) is essentially the gradient of the value function with respect to the state:\n$$\np(t) = \\nabla_x V(x,t).\n$$\nThis means \\( p(t) \\) tells you how much an incremental change in the state at time \\( t \\) will impact the total future cost. It\u2019s like a \"price tag\" on having an extra unit of the state.\n</p>\n<br>\n<u><b>Procedure</b></u><br>\n<ol>\n  <li>Define the Hamiltonian function:\n  $$ H = L(x, u, t) + p^T f(x, u, t) $$ \n  where \\( L \\) is the running cost and \\( f \\) describes system dynamics.</li>\n  <li>Derive the costate equation using the adjoint system:\n  $$ \\dot{p}(t) = -\\frac{\\partial H}{\\partial x} $$\n  with final condition \\( p(T) = \\frac{\\partial \\Phi}{\\partial x} \\big|_{t=T} \\).</li>\n  <li>Solve the costate equation alongside the state dynamics for optimal control.</li>\n</ol>\n<br>\n<u><b>Practical Use</b></u><br>\n<p>\nThe costate is crucial in solving optimal control problems, such as in aerospace trajectory optimization, economics (dynamic programming), and robotics. It helps determine optimal feedback laws and assess the sensitivity of cost functions to state variations.\n</p>",
    "front": "<b>Costate</b>\n<ul>  \n    <li><b>Define:</b> What is the costate in optimal control theory?</li>\n    <li><b>Procedure:</b> How do you compute the costate variable?</li>\n    <li><b>Practical Use:</b> Where/when is the costate used?</li>\n</ul>",
    "id": 1742447263218,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Definition</b></u><br>  \n<p>  \nThe <b>Hamiltonian</b> is a function used in optimal control theory to describe the total cost, combining the running cost and the effect of system dynamics.  \n</p>  \n<br>\n\n<p><b>Continuous-Time Setting:</b></p> \n$$  \nH(x, u, p, t) = L(x, u, t) + p^T f(x, u, t),  \n$$  \n<br>\n<p>  \nFor a system with dynamics  \n$$  \n\\dot{x}(t) = f(x(t), u(t), t)  \n$$  \nand a running cost \\( L(x(t), u(t), t) \\), the Hamiltonian consists of the immediate cost \\( L(x, u, t) \\) and the term \\( p^T f(x, u, t) \\), which captures how the current control \\( u \\) influences future costs through system dynamics. The vector \\( p(t) \\) is known as the costate (or adjoint) vector.  \n</p>  \n<br>\n\n<p><b>Discrete-Time Setting:</b></p> \n$$  \nH(x_k, u_k, p_{k+1}, k) = g(x_k, u_k, k) + p_{k+1}^T f(x_k, u_k, k),  \n$$  \n<br>\n<p>  \nFor a discrete-time system  \n$$  \nx_{k+1} = f(x_k, u_k, k)  \n$$  \nwith cost function \\( g(x_k, u_k, k) \\), the Hamiltonian similarly consists of the immediate cost \\( g(x_k, u_k, k) \\) and the term \\( p_{k+1}^T f(x_k, u_k, k) \\), which links the effect of the control decision to future costs. The vector \\( p_{k+1} \\) plays the role of the costate in the discrete-time setting.  \n</p>  ",
    "front": "<b>Hamiltonian</b>  \n<ul>  \n    <li><b>Define:</b> What is the Hamiltonian in continuous and discrete systems?</li>  \n</ul>",
    "id": 1742446224283,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Definition</b></u><br>\n<p>\nThe Hamilton\u2013Jacobi\u2013Bellman (HJB) equation is given by:\n$$\n-\\frac{\\partial V}{\\partial t}(x,t) = \\min_{u} \\left\\{ L(x,u,t) + \\nabla_x V(x,t)^T f(x,u,t) \\right\\},\n$$\nwhere: <br>\n<ul>\n    <li><b>\\( V(x,t) \\)</b>: The value function, representing the minimum cost-to-go from state \\( x \\) at time \\( t \\).</li>\n    <li><b>\\( \\frac{\\partial V}{\\partial t}(x,t) \\)</b>: The partial derivative of the value function with respect to time, indicating how the optimal cost-to-go changes over time.</li>\n    <li><b>\\( L(x,u,t) \\)</b>: The running (instantaneous) cost function, which penalizes tracking error and control effort at each time step.</li>\n    <li><b>\\( f(x,u,t) \\)</b>: The system dynamics, describing how the state \\( x \\) evolves over time under the control input \\( u \\).</li>\n    <li><b>\\( \\nabla_x V(x,t) \\)</b>: The gradient of the value function with respect to the state, measuring the sensitivity of the cost-to-go with respect to changes in \\( x \\).</li>\n    <li><b>\\( \\nabla_x V(x,t)^T f(x,u,t) \\)</b>: The rate of change of the optimal cost-to-go due to the system dynamics.</li>\n    <li><b>\\( \\min_{u} \\)</b>: The control \\( u \\) is chosen to minimize the right-hand side, ensuring the best possible control action at every instant.</li>\n</ul>\n<br>\nThe <b>HJB equation</b> gives a necessary condition for optimality by characterizing the value function \\( V(x,t) \\), which represents the minimum cost-to-go starting from state \\( x \\) at time \\( t \\). <br><br>\n\nIt is derived from Bellman\u2019s Principle of Optimality, which states that any optimal strategy must remain optimal at all future times.\n</p>\n<br>\n\n<u><b>Procedure</b></u><br>\n<p>\nFor a general continuous-time control problem with state dynamics:\n</p>\n$$\n\\dot{x}(t) = f(x(t), u(t), t)\n$$\n<p>\nand cost functional:\n</p>\n$$\nJ = \\int_{t}^{t_f} L(x(s), u(s), s) \\, ds + \\phi(x(t_f)),\n$$\n<p>\nthe value function \\( V(x,t) \\) satisfies the HJB equation:\n</p>\n$$\n-\\frac{\\partial V}{\\partial t}(x,t) = \\min_{u} \\left\\{ L(x,u,t) + \\nabla_x V(x,t)^T f(x,u,t) \\right\\},\n$$\n<p>\nwith the terminal condition:\n</p>\n$$\nV(x,t_f) = \\phi(x(t_f)).\n$$\n<p>\nTo solve the HJB equation:\n</p>\n<ol>\n  <li>Define the state dynamics and cost functional.</li>\n  <li>Formulate the HJB equation using the value function.</li>\n  <li>Find the control \\( u^*(t) \\) that minimizes the right-hand side.</li>\n  <li>Integrate to obtain the optimal cost-to-go function.</li>\n</ol>\n<br>",
    "front": "<b>Hamilton\u2013Jacobi\u2013Bellman (HJB) Equation</b>\n<ul>  \n    <li><b>Define:</b> What is the Hamilton\u2013Jacobi\u2013Bellman (HJB) equation? What are each of its components? What does it provide us?</li>\n    <li><b>Procedure:</b> How is the HJB equation derived and solved?</li>\n</ul>",
    "id": 1742451704356,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<u><b>Answer</b></u><br>  \n<p>  \nThe Hamilton-Jacobi-Bellman (HJB) equation is given by:  <br>\n$$\n-\\frac{\\partial V}{\\partial t}(x,t) = \\min_{u} \\left\\{ L(x,u,t) + \\nabla_x V(x,t)^T f(x,u,t) \\right\\}.\n$$  \n<br><br>\nThe Hamiltonian is defined as:  <br>\n$$  \nH(x, u, p, t) = L(x, u, t) + p^T f(x, u, t),  \n$$  \n<br>\nwhere \\( p = \\nabla_x V(x,t) \\) is the costate variable.  \n<br><br>\nSubstituting the Hamiltonian into the HJB equation, we get:  \n$$\n-\\frac{\\partial V}{\\partial t}(x,t) = \\min_{u} H(x, u, \\nabla_x V(x,t), t).\n$$  \n<br><br>\nThus, the Hamiltonian provides a compact way to express the terms inside the minimization in the HJB equation, but the HJB equation itself explicitly enforces optimality by minimizing over \\( u \\).\n</p>",
    "front": "<b>What is the relationship between the Hamiltonian and the HJB equation?</b>",
    "id": 1742448635240,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "$$\nH(x, u, \\lambda, t) = f(x, u, t) \\cdot \\lambda + L(x, u, t).\n$$\n<br><br>\nwhere: <br>\n- x  is the state variable. <br>\n- u  is the control variable. <br>\n- \\lambda  is the costate (adjoint) variable (analogous to momentum in classical mechanics). <br>\n- L(x, u, t)  is the running cost.<br>\n- f(x, u, t)  describes the system dynamics.\n\n<br><br><br><br>\n$$\nH = \\dot{x} \\frac{\\partial L}{\\partial \\dot{x}} - L \n  \\quad \\text{or} \\quad \nH = \\dot{x} \\frac{\\partial g}{\\partial \\dot{x}} - g\n$$\n<br><br>\nRewriting as:<br><br>\n$$\nH = \\dot{x} L_{\\dot{x}} - L \n  \\quad \\text{or} \\quad \nH = \\dot{x} g_{\\dot{x}} - g\n$$\n\n<br><br>\n\nNote that:<br><br>\n$$\ng_t + \\frac{dH}{dt} = 0.\n$$\n\n<br><br>\nThe crucial property of the Hamiltonian is that:<br>\n- If  $L$  does not explicitly depend on time (i.e.,  $L_t = 0$ ), then  $H$  is conserved.<br>\n- This leads directly to the Beltrami identity, which states that:\n$$\n\\dot{x} g_{\\dot{x}} - g = \\text{constant}.\n$$",
    "front": "Hamiltonian",
    "id": 1742426830311,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "$$\n\\min_{x \\in \\mathbb{R}^{n}}{f(x)}\n$$\n<br><br>\nsubject to: <br>\n$$\ng_{i}(x) \\leq 0, \\quad i \\in \\mathcal{I}, \\quad \\mathcal{I} = 1, \\dots, m \\quad \\text{(inequality constraints)}\n$$\n<br>\n$$\nh_{i}(x) = 0, \\quad j \\in \\mathcal{E}, \\quad \\mathcal{E} = 1, \\dots, p \\quad \\; \\text{(equality constraints)}\n$$\n<br><br>\nwhere:\n<ul>\n  <li>$$f(x)$$ is the objective function to be minimized.</li>\n  <li>$$g_i(x) \\leq 0$$ represents inequality constraints that must be satisfied.</li>\n  <li>$$h_j(x) = 0$$ represents equality constraints that must be exactly satisfied.</li>\n  <li>$$x \\in \\mathbb{R}^n$$ is the decision variable (a vector in \\( n \\)-dimensional space).</li>\n</ul>",
    "front": "Standard form of a constrained optimization problem.",
    "id": 1742426830638,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<ol>\n\n  <li>Stationarity <br>\n    $$\n    \\nabla f(x^*) + \\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*) + \\sum_{j=1}^{p} \\mu_j \\nabla h_j(x^*) = 0\n    $$\n  </li>\n  <br>\n\n  <li>Primal feasibility <br>\n    $$\n    g_i(x^*) \\leq 0, \\quad \\forall i \\in \\{1, \\dots, m\\}\n    $$\n    <br>\n    $$\n    h_j(x^*) = 0, \\quad \\forall j \\in \\{1, \\dots, p\\}\n    $$\n  </li>\n  <br>\n\n  <li>Dual feasibility <br>\n    $$\n    \\lambda_i \\geq 0, \\quad \\forall i \\in \\{1, \\dots, m\\}\n    $$\n  </li>\n  <br>\n\n  <li>Complementary slackness <br>\n    $$\n    \\lambda_i g_i(x^*) = 0, \\quad \\forall i \\in \\{1, \\dots, m\\}\n    $$\n  </li>\n  <br>\n\n</ol>",
    "front": "KKT Conditions.",
    "id": 1742426830356,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "When the gradients of the active constraints are linearly dependent\u2014that is, when the Linear Independence Constraint Qualification (LICQ) fails. In this case, the constraint gradients do not span enough directions to cancel out the gradient of the objective, meaning the stationarity condition cannot be met.",
    "front": "Under what circumstances can we fail to choose Lagrange multipliers that satisfy the KKT conditions without contradiction?",
    "id": 1742426830447,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "At an optimal point $x^*$, the idea is that no small, feasible perturbation $d$ should be able to decrease the objective function. In other words, for every feasible direction $d$, we must have <br><br>\n\n$$\n\\nabla f(x^{*})^T d \\geq 0.\n$$\n\n<br><br>\nNow, when a constraint $g_i(x) \\leq 0$ is active (i.e. $g_i(x^*) = 0)$, any small movement d that keeps us feasible must satisfy <br><br>\n$$\n\\nabla g_i(x^{*})^T d \\leq 0.\n$$\n<br><br>\nThis means the feasible directions are limited by the gradients of the active constraints. For there to be no feasible descent direction\u2014that is, for the objective not to decrease along any $d$\u2014the entire gradient $\\nabla f(x^*)$ must be \u201cneutralized\u201d by the constraint forces. This requirement is captured by the stationarity condition: <br><br>\n$$\n\\nabla f(x^*) + \\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*) = 0.\n$$\n<br><br>\nHere, the multipliers $\\lambda_i$ adjust the contribution of each constraint\u2019s gradient so that they together cancel out $\\nabla f(x^*)$. This ensures that any movement allowed by the constraints does not provide a descent direction, preserving the optimality of $x^*$.\n<br>",
    "front": "Why must the stationarity condition hold at an optimal solution in constrained optimization?",
    "id": 1742426830309,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "At an optimal point $x^*$, the gradients of the active constraints, $\\nabla g_i(x^*)$ for all $i$ such that $g_i(x^*) = 0$, define the normal (or \u201cblocking\u201d) directions at $x^*$. They essentially tell us which directions are forbidden for movement if we want to remain feasible. <br><br>\n\nFor $x^*$ to be optimal, the gradient $\\nabla f(x^*)$ must not point in any direction that would allow us to decrease $f(x)$ while staying within the feasible region. This is only possible if $\\nabla f(x^*)$ lies entirely in the span of the constraint gradients. In mathematical terms, this means that there exist multipliers $\\lambda_i$ such that <br><br>\n\n$$\n\\nabla f(x^*) = -\\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*).\n$$\n\n<br><br>\nIf any component of $\\nabla f(x^*)$ were to lie outside of the span of the $\\nabla g_i(x^*)$, then there would be a feasible direction in which $f(x)$ could decrease, contradicting the optimality of $x^*$. <br><br>\n\nThus, the condition guarantees that all possible descent directions are \u201cblocked\u201d by the active constraints, ensuring that $x^*$ is indeed optimal.",
    "front": "What is the geometric intuition behind the KKT stationarity condition?",
    "id": 1742426830120,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "The LICQ condition states that at a feasible point $x^*$, the gradients of all active constraints must be linearly independent. <br><br>\n\nMathematically, if the set of active constraints at $x^*$ is given by:\n<br><br>\n$$\n\\mathcal{A} = \\{ i \\mid g_i(x^*) = 0 \\},\n$$\n<br><br>\nthen the gradients of these constraints,\n<br><br>\n$$\n\\{ \\nabla g_i(x^*) \\}_{i \\in \\mathcal{A}},\n$$\n<br><br>\nmust be linearly independent. This means that if there exists a set of scalars $\\lambda_i$ such that:\n<br><br>\n$$\n\\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*) = 0,\n$$\n<br><br>\nthen it must follow that $\\lambda_i = 0$ for all $i \\in \\mathcal{A}$. <br><br>\n\nIf LICQ holds, then the active constraint gradients span a well-defined space, ensuring that the KKT conditions hold without contradiction. <br><br>\n\nIf LICQ fails, the stationarity condition may not be satisfied, meaning no valid Lagrange multipliers exist.",
    "front": "Linear Independence Constraint Qualification (LICQ) condition.",
    "id": 1742426830195,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Interpolation points.",
    "id": 1742426830514,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO",
    "front": "Collocation points",
    "id": 1742426830689,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "General way to express most optimal control problems: <br><br>\n\n\\[\n\\begin{array}{ll}\n\\textbf{Minimize} & J = \\phi\\Bigl(x(t_0),\\, x(t_f),\\, t_0,\\, t_f,\\, q,\\, s\\Bigr) \\\\\n\\\\\n\\textbf{with respect to} & x(t),\\; u(t),\\; t_0,\\; t_f,\\; s,\\; q \\\\\n\\\\\n\\textbf{subject to} & \\dot{x}(t) = f\\Bigl(x(t),\\, u(t),\\, t,\\, s\\Bigr), \\quad t \\in [t_0, t_f], \\\\\n\\\\\n& q = \\int_{t_0}^{t_f} g\\Bigl(x(t),\\, u(t),\\, t,\\, s\\Bigr) dt, \\\\\n\\\\\n& c\\Bigl(x(t),\\, u(t),\\, t,\\, s\\Bigr) \\le 0, \\quad t \\in [t_0, t_f], \\\\\n\\\\\n& \\psi\\Bigl(x(t_0),\\, x(t_f),\\, t_0,\\, t_f,\\, s\\Bigr) = 0, \\\\\n\\\\\n& \\text{and appropriate bounds on } x(t),\\, u(t),\\, s,\\, t_0,\\, t_f.\n\\end{array}\n\\]",
    "front": "Bolza optimization problem formulation.",
    "id": 1742426830812,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content\n\n<br><br>\nThe following functions are transcendental:<br>\n<img src=\"/assets/Screenshot_2025-02-28_at_1.31.02_AM.png\">",
    "front": "Transcendental equation",
    "id": 1742426830959,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content <br><br>\n\nWhat is it and what is the equation for it?",
    "front": "Catenary curve.",
    "id": 1742426830518,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content <br><br>\n\nWhat is it and what is the equation for it?",
    "front": "Catenoid.",
    "id": 1742426830905,
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "Tracking error weighting matrix used in the cost functional.",
    "front": "$R_{rr}$ matrix",
    "id": 1742432258789,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "Control input weighting matrix used in the cost functional.",
    "front": "$R_{uu}(t)$ matrix",
    "id": 1742432289977,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<p>\nSystem controllable if <b>controllability matrix</b> has full rank\n</p>\n\n<p>\n<b><u>Key Condition</u></b> <br>\n$$\nrank(\\mathcal{C}) = n\n$$\n<br>\n$$\n\\mathcal{C} = [B \\; AB \\; A^{2}B \\; \\dots \\; A^{n-1}B ]\n$$\n</p>",
    "front": "controllability (of continuous time linear systems)",
    "id": 1745434247355,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<p>\nSystem <b>stabilizeable</b> if all unstable modes of the system can be controlled, even if the system isn't fully controllable, i.e., all eigenvalues of $A$ with non-negative real parts must be controllable.\n</p>\n\n<p>\n<b><u>Key Condition</u></b> <br>\nAll eigenvalues of $A$ with non-negative real parts must be controllable.\n</p>",
    "front": "stabilizability (of continuous time linear systems)",
    "id": 1745434265859,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<p>\nSystem <b>observable</b> if state can be determined from output measurements over a finite time.\n</p>\n\n<p>\n<b><u>Key Condition</u></b> <br>\n$$\n\\text{rank}(\\mathcal{O}) = n\n$$\n<br>\n$$\n\\mathcal{O} = \n\\begin{bmatrix}\nC \\\\\nCA \\\\\nCA^2 \\\\\n\\vdots \\\\\nCA^{n-1}\n\\end{bmatrix}\n$$\n</p>",
    "front": "observability",
    "id": 1745434271278,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<p>\nSystem <b>detectable</b> if all unobservable modes are stable.\n</p>\n<p>\n<b><u>Key Condition</u></b> <br>\nAny eigenvalue of $A$ with non-negative real part must be observable.\n</p>\n",
    "front": "detectability",
    "id": 1745434276504,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "TODO: quiz 1 concepts",
    "id": 1745434125580,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Definition</b></u><br>\n<p>\nA <b>Value Function</b> represents the minimum \"cost-to-go\" from a given state \\( x \\) at time \\( t \\), defining the optimal cost for reaching a goal while following an optimal control policy.\n</p>\n<br>\n\n<u><b>Usage</b></u><br>\n<p>\nIn Optimal Control Theory, the Value Function is used to determine the optimal control policy by solving the HJB equation. It quantifies the best possible performance given any initial state, helping in applications such as trajectory optimization, robotics, and economic decision-making.\n</p>\n<br>\n\n<u><b>Procedure</b></u><br>\n<ol>\n  <li>Define the cost functional for the optimal control problem.</li>\n  <li>Set up the system dynamics using differential or difference equations.</li>\n  <li>Apply the Hamilton-Jacobi-Bellman (HJB) equation to characterize the optimal value function.</li>\n  <li>Use numerical methods or analytical solutions to compute the value function iteratively.</li>\n</ol>\n<br>\n",
    "front": "<b>Value Function</b>\n<ul>  \n    <li><b>Define:</b> What is a Value Function?</li>\n    <li><b>Usage:</b> How is a Value Function used in Optimal Control Theory?</li>\n    <li><b>Procedure:</b> How do you compute or use a Value Function?</li>\n</ul>",
    "id": 1742431893518,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Answer</b></u><br>\n<p>\nWe assume that the value function takes a <b>quadratic form</b>:\n$$\nV(x,t) = x^T P(t) x + 2\\eta(t)^T x + \\rho(t),\n$$\n<br>\nwhere:\n<ul>\n    <li>\\( P(t) \\) is a symmetric matrix evolving over time.</li>\n    <li>\\( \\eta(t) \\) is a time-dependent vector.</li>\n    <li>\\( \\rho(t) \\) is a scalar function of time.</li>\n</ul>\n</p>\n<br>\n\n<u><b>Reasoning</b></u><br>\n<ol>\n    <li><b>LQ Problems Have Quadratic Costs:</b> The cost function in LQ problems is quadratic in state \\( x \\) and control \\( u \\), making a quadratic value function natural.</li>\n    <li><b>HJB Equation Confirms the Quadratic Structure:</b> Substituting a quadratic form into the Hamilton-Jacobi-Bellman (HJB) equation leads to a Riccati equation for \\( P(t) \\), confirming this assumption.</li>\n    <li><b>Simplifies the Optimal Control Law:</b> The optimal control law follows the form:\n    $$ u^*(t) = u_0(t) - F(t)x(t), $$\n    which naturally results from a quadratic value function.</li>\n</ol>\n<br>\n\n<u><b>Implications</b></u><br>\n<p>\nThis quadratic assumption greatly simplifies solving LQ problems and provides a closed-form solution for the optimal control law.\n</p>",
    "front": "<b>For LQ problems, what do we assume?</b>\n<ul>  \n    <li><b>Reasoning:</b> Why do we assume this form?</li>\n    <li><b>Implications:</b> How does this assumption simplify solving LQ problems?</li>\n</ul>",
    "id": 1742434417823,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "States that the optimal strategy from any state and time must also be optimal for the remaining time.",
    "front": "Principle of Optimality",
    "id": 1742427806534,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content\npset 7, p1.1\n",
    "front": "Change of variables",
    "id": 1745018630552,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Box-Muller Transform (of random variables)",
    "id": 1745019455415,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "exp(log(y)) = y <br>\nexp(a\u22c5log(y)) = y^a <br>\nexp(a\u22c5log(y))=(e^{log(y)})^a = y^a",
    "id": 1745021457016,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "pset 7, p1.1 good example",
    "front": "multivariate change\u2011of\u2011variables formula",
    "id": 1745023123149,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Definition</b></u><br>\n<p>\nThe <b>standard normal density function</b> for a single real-valued variable \\( x \\in \\mathbb{R} \\) is defined as:\n$$\n\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)\n$$\nwhere \\( \\exp(\\cdot) \\) denotes the exponential function.\n</p>\n",
    "front": "<b>Standard Normal Density Function</b>\n<ul>  \n    <li><b>Define:</b> What is the standard normal density function for a single real-valued variable \\( x \\in \\mathbb{R} \\)?</li>\n</ul>\n",
    "id": 1745025157354,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": false,
    "attempts": [],
    "back": "<u><b>Definition</b></u><br>\n<p>\nTwo random variables <b>\\( X_1 \\)</b> and <b>\\( X_2 \\)</b> are said to be <b>independent</b> if and only if their joint density function factors as:\n$$\nf_{X_1, X_2}(x_1, x_2) = f_{X_1}(x_1) \\cdot f_{X_2}(x_2)\n$$\nfor all \\( x_1, x_2 \\in \\mathbb{R} \\).\n</p>\n",
    "front": "<b>Independence of Random Variables</b>\n<ul>  \n    <li><b>Define:</b> What does it mean for two random variables \\( X_1 \\) and \\( X_2 \\) to be independent?</li>\n</ul>\n",
    "id": 1745025222523,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "lecture 17 <br>\nTODO: New back content",
    "front": "discrete kalman filter (equations)",
    "id": 1745433720086,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "lecture 16 <br>\nTODO: New back content",
    "front": "stochastic process",
    "id": 1745434051861,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "lecture 15 <br>\nTODO: New back content",
    "front": "static estimation",
    "id": 1745433981459,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "Control ARE <br>\nFilter ARE<br>\nDuality Map <br>\n\n<br> TODO: New back content",
    "front": "duality of LQR and Kalman Filter",
    "id": 1745437408307,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<p>\nCertainty equivalence says \u201cuse the estimate as truth\u201d without loss of optimality.\n</p>\n\n<p>\nHolds for LQG but definitely not always! <br>\nTODO: New back content\n</p>",
    "front": "Certainty Equivalence Principle",
    "id": 1745437146671,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "orthogonal projection theorem",
    "id": 1745437106524,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<p>\nOrthogonal projection proves the KF update is the MSE\u2010optimal linear estimator.\n</p>\n<p>\nThe Kalman filter arises by viewing <br>\n$$\n\\hat x(t) = \\mathbb{E}[x(t)\\mid \\mathcal Y_t]\n$$\n<br>\nas the orthogonal projection of the state onto the space spanned by past measurements $\\mathcal{Y_t}$.\n<br>\n\nTODO: fix this <br><br>\n\n\t\u2022\tThe innovation\nr(t) = dy(t) - C\\,\\hat x(t)\\,dt\nis orthogonal to all past r(s), s<t.\n\t\u2022\tProjection \u21a6 minimal MSE estimator \u21d2 update law\nd\\hat x = A\\hat x\\,dt + B\\,u\\,dt + L(t)\\,r(t),\n\\quad L(t)=P_f(t)\\,C^\\top\\,V^{-1}.\n</p>",
    "front": "Orthogonal Projection Theorem \u21d2 Kalman Filter",
    "id": 1745437603399,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "<p>\nSeparation lets you design estimator and controller independently\n</p>\n\n<p>\nHolds for LQG but definitely not always! <br>\nTODO: New back content\n</p>",
    "front": "Separation Principle",
    "id": 1745437118740,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: list of symbols and meaning <br>\nTODO: New back content",
    "front": "LQG",
    "id": 1745437088820,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Principle of Least Action",
    "id": 1745595129057,
    "tags": [
      "16.32",
      "research"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content <br>\n\n\t\u2022\t2-norm (energy)\n\\|u(t)\\|2 \\;=\\;\\Bigl(\\!\\!\\int{-\\infty}^{\\infty}u^2(t)\\,dt\\Bigr)^{\\!1/2},\n\t\u2022\t\u221e-norm (peak)\n\\|u(t)\\|_\\infty = \\max_t |u(t)|.",
    "front": "Signal norms",
    "id": 1746038938792,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "$$\n\\| G(s) \\|_{\\infty} = \\underset{\\omega}{\\sup} \\bar{\\sigma}[G(j \\omega)]\n$$\n<br>\n<div style=\"width: 100%; height: 100%;\"><img class=\"embedded-card-image\" src=\"/assets/Screenshot_2025-04-30_at_3.25.28_PM.png\"></div>",
    "front": "$\\mathcal{H}_{\\infty}$ system norm",
    "id": 1746040771901,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content <br>\n\n\t\u2022\tSystem (operator) norms for a stable LTI G(s):\n\t\u2022\tH\u2082 norm\n\\|G\\|2^2\n= \\frac1{2\\pi}\\!\\int{-\\infty}^{\\infty}\\mathrm{trace}[G^H(j\u03c9)G(j\u03c9)]\\,d\u03c9,\nmeasures average energy gain.\n\t\u2022\tH\u221e norm\n\\|G\\|\\infty = \\sup{\u03c9}\\,\\bar\u03c3\\!\\bigl[G(j\u03c9)\\bigr],\nthe peak (worst-case) L\u2082\u2192L\u2082 gain:\n\\|G\\|\\infty = \\sup{u\\neq0}\\|y\\|_2/\\|u\\|_2.",
    "front": "System (operator) norms",
    "id": 1746039478644,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Checking \\( \\|G\\|_\\infty < \\gamma \\) via Hamiltonian Tests\n",
    "id": 1746039483502,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "$\\gamma$-iteration (bisection search)",
    "id": 1746039639489,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [
      {
        "timestamp": 1752636630371,
        "correct": false,
        "reason": "forgot risk-sensitive stochastic LQG"
      },
      {
        "timestamp": 1752636633700,
        "correct": true
      },
      {
        "timestamp": 1752636635266,
        "correct": true
      },
      {
        "timestamp": 1752636636104,
        "correct": true
      },
      {
        "timestamp": 1752636636961,
        "correct": true
      }
    ],
    "back": "$H_{\\infty}$ minimizes the risk-sensitive (stochastic) LQG problem. Instead of minimizing $J = E[G]$, where\n<br>\n$$\nG = \\int{x^T R_{xx} x + u^T R_{uu} u } \\text{dt}\n$$\n<br>\nminimize \n<br>\n$$\nJ = - \\frac{2}{\\theta} \\log( \\text{E} [e^{\\frac{1}{2} \\theta G}])\n$$",
    "front": "$H_{\\infty}$ minimizes the ___.",
    "id": 1746039943596,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "nonlinear least squares",
    "id": 1746041762849,
    "tags": [
      "16.32",
      "final"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "Jaynes, 2003, Probability Theory the Logic of Science <br>\nTODO: New back content",
    "front": "Jaynes desirable properties of plausible inference",
    "id": 1745595545829,
    "tags": [
      "research"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "Jaynes, 2003, Probability Theory the Logic of Science <br>\nTODO: New back content",
    "front": "Maximum Entropy Principle",
    "id": 1745595158376,
    "tags": [
      "research"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "Jaynes, 2003, Probability Theory the Logic of Science <br>\nTODO: New back content",
    "front": "Uniform distribution deriviation from entropy maximum principle",
    "id": 1745595164840,
    "tags": [
      "research"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "Jaynes, 2003, Probability Theory the Logic of Science <br>\nTODO: New back content",
    "front": "Gaussian distribution deriviation from entropy maximum principle",
    "id": 1745595189975,
    "tags": [
      "research"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Clopper-Pearson Confidence Intervals vs. Bayesian Credibility Interval",
    "id": 1745595801398,
    "tags": [
      "research"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Frequentist Confidence Interval vs. Bayesian Credibility Interval",
    "id": 1745595922584,
    "tags": [
      "research"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "Bayes Rule",
    "id": 1745595832084,
    "tags": [
      "research"
    ],
    "understanding": 1
  },
  {
    "absolutelyRequired": true,
    "attempts": [],
    "back": "TODO: New back content",
    "front": "TODO: New front content",
    "id": 1745595845914,
    "tags": [
      "research"
    ],
    "understanding": 1
  }
]