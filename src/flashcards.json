[
  {
    "back": "Taylor series is the infinite series:<br><br>\n$$\nf(x) = \\sum_{n=0}^{\\inf}{\\frac{f^{(n)}(a)}{n!}(x - a)^{n}}\n$$\n<br><br>\nTaylor series \"expansion\" of a function typically implies truncation, meaning we approximate the function $f(x)$ by a finite number of terms. <br><br>\n$$\nf(x) \\approx f(a) + f^{\\prime}(a)(x-a) + \\dots + \\frac{f^{(N)}(a)}{N!} (x-a)^N.\n$$\n<br><br>",
    "front": "Taylor series of $f(x)$ at a point $x = a$ <br><br>\nvs. <br><br>\nTaylor series expansion of $f(x)$ at / around a point $x = a$ <br>",
    "importance": 2,
    "tags": [
      "18.0651",
      "16.32"
    ],
    "understanding": 1
  },
  {
    "back": "If $f: \\mathbb{R}^n \\to \\mathbb{R}$ is a differentiable function, the gradient of $f(x)$ is defined as:\n<br><br>\n\n$$\n\\nabla f(x) = \n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\\n\\frac{\\partial f}{\\partial x_2} \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial x_n}\n\\end{bmatrix}.\n$$\n\n<br><br>\nEach component $\\frac{\\partial f}{\\partial x_i}$ represents the partial derivative of $f$ with respect to $x_i$.\n<br><br>\n\nThe gradient is a vector that points in the direction of the steepest increase of $f(x)$.",
    "front": "Define the gradient of $f(x)$.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "back": "A functional is a mapping from a space of functions to the real numbers. <br><br>\n\nMathematically, a functional $J$ takes a function $y(x)$ as input and returns a real number:\n<br><br>\n\n$$\nJ[y] = \\int_{a}^{b} F(x, y, y') \\, dx.\n$$\n\n<br><br>\nFunctionals are commonly used in calculus of variations and physics to express quantities such as energy or action.",
    "front": "What is a <b>functional</b>?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "back": "The first variation of a functional is the first-order term in the Taylor expansion of the increment, providing a linear approximation of the change in the functional. <br><br>\n\nThe first variation of a functional $J[x(t)]$ with respect to a perturbation $\\delta x(t)$ is defined as:\n<br><br>\n\n$$\n\\delta J(x(t), \\delta x(t)) = \\left. \\frac{d}{d\\epsilon} J(x(t) + \\epsilon \\delta x(t)) \\right|_{\\epsilon=0}.\n$$\n<br><br>\n\n<b>TODO: fix beyond here to make more sense</b>\n<br><br>\n\nFor a functional $J[y]$, we express its perturbation as:\n<br><br>\n\n$$\nJ[y + \\epsilon \\delta y] = \\int_a^b F(x, y + \\epsilon \\delta y, y' + \\epsilon \\delta y') \\, dx.\n$$\n<br><br>\n\nExpanding $F( \\cdot )$ in a Taylor series around $\\epsilon = 0$ gives:\n<br><br>\n$$\nF(x, y + \\epsilon \\delta y, y' + \\epsilon \\delta y') = F(x, y, y') + \\epsilon \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) + O(\\epsilon^2).\n$$\n<br>\n\nSubstituting this into the integral:\n<br><br>\n$$\nJ[y + \\epsilon \\delta y] = J[y] + \\epsilon \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx + O(\\epsilon^2).\n$$\n<br><br>\n\nTaking the limit as $\\epsilon \\to 0$, we obtain the first variation:\n<br><br>\n$$\n\\delta J = \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx.\n$$\n<br>",
    "front": "What is the definition of an <b>increment</b>?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "back": "The first variation of functional is a linear approximation of the increment, i.e., first-order Taylor expansion of the increment.<br><br>\n\nThe **first variation** of a functional $J[x(t)]$ with respect to a perturbation $\\delta x(t)$ is defined as:\n<br><br>\n\n$$\n\\delta J(x(t), \\delta x(t)) = \\left. \\frac{d}{d\\epsilon} J(x(t) + \\epsilon \\delta x(t)) \\right|_{\\epsilon=0}.\n$$\n<br><br>\n\nFor a functional $J[y]$ and a small perturbation $\\delta y$, consider:\n<br><br>\n\n$$\nJ[y + \\epsilon \\delta y] = \\int_a^b F(x, y + \\epsilon \\delta y, y' + \\epsilon \\delta y') \\, dx.\n$$\n<br><br>\n\nExpanding the integrand $F( \\cdot )$ in a Taylor series around $\\epsilon = 0$, we obtain:\n<br><br>\n$$\nF(\\cdot) = F(x, y, y') + \\epsilon \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) + O(\\epsilon^2).\n$$\n<br><br>\n\nSubstituting this expansion back into the integral gives:\n<br><br>\n$$\nJ[y + \\epsilon \\delta y] = \\int_a^b \\Bigg[ F(x, y, y') + \\epsilon \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) + O(\\epsilon^2) \\Bigg] dx.\n$$\n<br>\n\nThis can be rewritten as:\n<br><br>\n$$\nJ[y + \\epsilon \\delta y] = J[y] + \\epsilon \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx + O(\\epsilon^2).\n$$\n<br><br>\n\nThe first variation $\\delta J$ is defined as the first-order term:\n<br><br>\n$$\n\\delta J = \\lim_{\\epsilon \\to 0} \\frac{J[y + \\epsilon \\delta y] - J[y]}{\\epsilon} = \\int_a^b \\left( \\frac{\\partial F}{\\partial y} \\delta y + \\frac{\\partial F}{\\partial y'} \\delta y' \\right) dx.\n$$\n<br>",
    "front": "What is the definition of the <b>first variation</b>?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "back": "TODO: New back content",
    "front": "Define and derive the <b>Euler-Lagrange equation</b>.\n<br><br>",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "back": "$$\n\\min_{x \\in \\mathbb{R}^{n}}{f(x)}\n$$\n<br><br>\nsubject to: <br>\n$$\ng_{i}(x) \\leq 0, \\quad i \\in \\mathcal{I}, \\quad \\mathcal{I} = 1, \\dots, m \\quad \\text{(inequality constraints)}\n$$\n<br>\n$$\nh_{i}(x) = 0, \\quad j \\in \\mathcal{E}, \\quad \\mathcal{E} = 1, \\dots, p \\quad \\; \\text{(equality constraints)}\n$$\n<br><br>\nwhere:\n<ul>\n  <li>$$f(x)$$ is the objective function to be minimized.</li>\n  <li>$$g_i(x) \\leq 0$$ represents inequality constraints that must be satisfied.</li>\n  <li>$$h_j(x) = 0$$ represents equality constraints that must be exactly satisfied.</li>\n  <li>$$x \\in \\mathbb{R}^n$$ is the decision variable (a vector in \\( n \\)-dimensional space).</li>\n</ul>",
    "front": "Standard form of a constrained optimization problem.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "back": "<ol>\n\n  <li>Stationarity <br>\n    $$\n    \\nabla f(x^*) + \\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*) + \\sum_{j=1}^{p} \\mu_j \\nabla h_j(x^*) = 0\n    $$\n  </li>\n  <br>\n\n  <li>Primal feasibility <br>\n    $$\n    g_i(x^*) \\leq 0, \\quad \\forall i \\in \\{1, \\dots, m\\}\n    $$\n    <br>\n    $$\n    h_j(x^*) = 0, \\quad \\forall j \\in \\{1, \\dots, p\\}\n    $$\n  </li>\n  <br>\n\n  <li>Dual feasibility <br>\n    $$\n    \\lambda_i \\geq 0, \\quad \\forall i \\in \\{1, \\dots, m\\}\n    $$\n  </li>\n  <br>\n\n  <li>Complementary slackness <br>\n    $$\n    \\lambda_i g_i(x^*) = 0, \\quad \\forall i \\in \\{1, \\dots, m\\}\n    $$\n  </li>\n  <br>\n\n</ol>",
    "front": "KKT Conditions.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "back": "When the gradients of the active constraints are linearly dependent\u2014that is, when the Linear Independence Constraint Qualification (LICQ) fails. In this case, the constraint gradients do not span enough directions to cancel out the gradient of the objective, meaning the stationarity condition cannot be met.",
    "front": "Under what circumstances can we fail to choose Lagrange multipliers that satisfy the KKT conditions without contradiction?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "back": "At an optimal point $x^*$, the idea is that no small, feasible perturbation $d$ should be able to decrease the objective function. In other words, for every feasible direction $d$, we must have <br><br>\n\n$$\n\\nabla f(x^{*})^T d \\geq 0.\n$$\n\n<br><br>\nNow, when a constraint $g_i(x) \\leq 0$ is active (i.e. $g_i(x^*) = 0)$, any small movement d that keeps us feasible must satisfy <br><br>\n$$\n\\nabla g_i(x^{*})^T d \\leq 0.\n$$\n<br><br>\nThis means the feasible directions are limited by the gradients of the active constraints. For there to be no feasible descent direction\u2014that is, for the objective not to decrease along any $d$\u2014the entire gradient $\\nabla f(x^*)$ must be \u201cneutralized\u201d by the constraint forces. This requirement is captured by the stationarity condition: <br><br>\n$$\n\\nabla f(x^*) + \\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*) = 0.\n$$\n<br><br>\nHere, the multipliers $\\lambda_i$ adjust the contribution of each constraint\u2019s gradient so that they together cancel out $\\nabla f(x^*)$. This ensures that any movement allowed by the constraints does not provide a descent direction, preserving the optimality of $x^*$.\n<br>",
    "front": "Why must the stationarity condition hold at an optimal solution in constrained optimization?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "back": "At an optimal point $x^*$, the gradients of the active constraints, $\\nabla g_i(x^*)$ for all $i$ such that $g_i(x^*) = 0$, define the normal (or \u201cblocking\u201d) directions at $x^*$. They essentially tell us which directions are forbidden for movement if we want to remain feasible. <br><br>\n\nFor $x^*$ to be optimal, the gradient $\\nabla f(x^*)$ must not point in any direction that would allow us to decrease $f(x)$ while staying within the feasible region. This is only possible if $\\nabla f(x^*)$ lies entirely in the span of the constraint gradients. In mathematical terms, this means that there exist multipliers $\\lambda_i$ such that <br><br>\n\n$$\n\\nabla f(x^*) = -\\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*).\n$$\n\n<br><br>\nIf any component of $\\nabla f(x^*)$ were to lie outside of the span of the $\\nabla g_i(x^*)$, then there would be a feasible direction in which $f(x)$ could decrease, contradicting the optimality of $x^*$. <br><br>\n\nThus, the condition guarantees that all possible descent directions are \u201cblocked\u201d by the active constraints, ensuring that $x^*$ is indeed optimal.",
    "front": "What is the geometric intuition behind the KKT stationarity condition?",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  },
  {
    "back": "The LICQ condition states that at a feasible point $x^*$, the gradients of all active constraints must be linearly independent. <br><br>\n\nMathematically, if the set of active constraints at $x^*$ is given by:\n<br><br>\n$$\n\\mathcal{A} = \\{ i \\mid g_i(x^*) = 0 \\},\n$$\n<br><br>\nthen the gradients of these constraints,\n<br><br>\n$$\n\\{ \\nabla g_i(x^*) \\}_{i \\in \\mathcal{A}},\n$$\n<br><br>\nmust be linearly independent. This means that if there exists a set of scalars $\\lambda_i$ such that:\n<br><br>\n$$\n\\sum_{i \\in \\mathcal{A}} \\lambda_i \\nabla g_i(x^*) = 0,\n$$\n<br><br>\nthen it must follow that $\\lambda_i = 0$ for all $i \\in \\mathcal{A}$. <br><br>\n\nIf LICQ holds, then the active constraint gradients span a well-defined space, ensuring that the KKT conditions hold without contradiction. <br><br>\n\nIf LICQ fails, the stationarity condition may not be satisfied, meaning no valid Lagrange multipliers exist.",
    "front": "Linear Independence Constraint Qualification (LICQ) condition.",
    "importance": 2,
    "tags": [
      "16.32"
    ],
    "understanding": 1
  }
]